{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa568292",
   "metadata": {},
   "source": [
    "## Preflight & Bootstrap (added)\n",
    "These cells load cached data and prevent accidental re-downloads on rerun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809e21e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prices_wide.parquet           FOUND    (102.0 MB)\n",
      "meta_yq.parquet               FOUND    (0.1 MB)\n",
      "meta_yq_progress.parquet      FOUND    (0.0 MB)\n",
      "sec_recent.parquet            FOUND    (0.1 MB)\n",
      "sec_cache/                    FOUND    (3357 files)\n",
      "sec_facts/                    FOUND    (3269 files)\n",
      "yq_symbol_cache/              FOUND    (3180 files)\n",
      "split_events.parquet          FOUND    (0.2 MB)\n",
      "shares_events.parquet         FOUND    (1.7 MB)\n",
      "yf_snapshot_cache/            FOUND    (2653 files)\n",
      "yq_float_snapshot.parquet     FOUND    (0.0 MB)\n",
      "analysis_enriched.parquet     FOUND    (80.9 MB)\n",
      "\n",
      "Mode: prices: LOAD | meta: LOAD | sec: LOAD | yq_float: LOAD | cache_only: True\n",
      "\n",
      "Tip: leave REBUILD_PRICES=REBUILD_META=REBUILD_SEC=False so the bootstrap loads from disk only.\n"
     ]
    }
   ],
   "source": [
    "# === PREFLIGHT (expanded cache status + no-scrape flags) ===\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "OUT = Path(\"outputs\")\n",
    "\n",
    "paths = {\n",
    "    \"prices_wide.parquet\"        : OUT / \"prices_wide.parquet\",\n",
    "    \"meta_yq.parquet\"            : OUT / \"meta_yq.parquet\",\n",
    "    \"meta_yq_progress.parquet\"   : OUT / \"meta_yq_progress.parquet\",\n",
    "    \"sec_recent.parquet\"         : OUT / \"sec_recent.parquet\",\n",
    "    \"sec_cache/\"                 : OUT / \"sec_cache\",\n",
    "    \"sec_facts/\"                 : OUT / \"sec_facts\",\n",
    "    \"yq_symbol_cache/\"           : OUT / \"yq_symbol_cache\",\n",
    "    \"split_events.parquet\"       : OUT / \"split_events.parquet\",\n",
    "    \"shares_events.parquet\"      : OUT / \"shares_events.parquet\",\n",
    "    \"yf_snapshot_cache/\"         : OUT / \"yf_snapshot_cache\",\n",
    "    \"yq_float_snapshot.parquet\"  : OUT / \"yq_float_snapshot.parquet\",\n",
    "    \"analysis_enriched.parquet\"  : OUT / \"analysis_enriched.parquet\",\n",
    "}\n",
    "\n",
    "def _size(p: Path):\n",
    "    try:\n",
    "        if p.is_dir():\n",
    "            return f\"{len(list(p.glob('*')))} files\"\n",
    "        return f\"{p.stat().st_size/1_048_576:.1f} MB\"\n",
    "    except Exception:\n",
    "        return \"-\"\n",
    "\n",
    "for name, p in paths.items():\n",
    "    status = \"FOUND\" if p.exists() else \"—\"\n",
    "    print(f\"{name:<28} {status:>6}    ({_size(p)})\")\n",
    "\n",
    "# ===== Run-mode flags (keep these False to avoid re-scraping) =====\n",
    "REBUILD_PRICES        = False\n",
    "REBUILD_META          = False\n",
    "REBUILD_SEC           = False\n",
    "REBUILD_YQ_FLOAT      = False     # do not rebuild Yahoo float snapshot\n",
    "YQ_FLOAT_CACHE_ONLY   = True      # use snapshot/cache only, no network for float backfill\n",
    "\n",
    "print(\"\\nMode:\",\n",
    "      \"prices:\", \"LOAD\" if not REBUILD_PRICES else \"REBUILD\", \"|\",\n",
    "      \"meta:\",   \"LOAD\" if not REBUILD_META   else \"REBUILD\", \"|\",\n",
    "      \"sec:\",    \"LOAD\" if not REBUILD_SEC    else \"REBUILD\", \"|\",\n",
    "      \"yq_float:\", \"LOAD\" if not REBUILD_YQ_FLOAT else \"REBUILD\",\n",
    "      \"| cache_only:\", YQ_FLOAT_CACHE_ONLY)\n",
    "\n",
    "# Optional master kill-switch if you like:\n",
    "NO_NETWORK = True  # leave True for normal reruns; set False only when you explicitly want to fetch new data\n",
    "\n",
    "print(\"\\nTip: leave REBUILD_PRICES=REBUILD_META=REBUILD_SEC=False so the bootstrap loads from disk only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bf669db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- run-safe defaults (no scraping) ----\n",
    "REBUILD_PRICES = False\n",
    "REBUILD_META   = False\n",
    "REBUILD_SEC    = False\n",
    "\n",
    "# yq float snapshot / cache behavior\n",
    "REBUILD_YQ_FLOAT    = False      # don't rebuild snapshot\n",
    "YQ_FLOAT_CACHE_ONLY = True       # use cache/snapshot only; no network\n",
    "NO_NETWORK          = False      # leave False unless you want strict \"airplane mode\"\n",
    "\n",
    "# yahooquery metadata pass\n",
    "RUN_YQ_BACKFILL = False          # skip yahooquery scraping pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70901c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global run toggles (fast defaults) ---\n",
    "FAST_MODE        = True        # prefer cached artifacts, avoid recomputation\n",
    "NO_NETWORK       = True        # hard no network: skip all scraping calls\n",
    "REBUILD_META     = False\n",
    "REBUILD_SEC      = False\n",
    "REBUILD_PRICES   = False\n",
    "\n",
    "# YahooQuery/YF controls\n",
    "RUN_YQ_BACKFILL  = False       # metadata backfill off by default\n",
    "YQ_FLOAT_CACHE_ONLY = True     # read cached float snapshots only\n",
    "\n",
    "OUT = Path(\"outputs\"); OUT.mkdir(exist_ok=True)\n",
    "\n",
    "# Snapshots we’ll reuse to keep re-runs fast\n",
    "P_META_YQ          = OUT / \"meta_yq.parquet\"\n",
    "P_META_YQ_MORE     = OUT / \"meta_yq_more.parquet\"\n",
    "P_YQ_SYM_DIR       = OUT / \"yq_symbol_cache\"\n",
    "P_YQ_FLOAT_SNAP    = OUT / \"yq_float_snapshot.parquet\"\n",
    "P_SEC_FACTS_DIR    = OUT / \"sec_facts\"\n",
    "P_SEC_ENRICHED_PX  = OUT / \"sec_enriched_px.parquet\"  # SEC public-float→shares (as-of joined to price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bc0677e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prices_raw: (751, 30726)\n",
      "fundamentals tickers: 5118\n",
      "analysis_enriched rows: 3845871\n"
     ]
    }
   ],
   "source": [
    "# === BOOTSTRAP / RESUME GUARD ===\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, json\n",
    "\n",
    "# Paths & flags\n",
    "OUT = Path(\"outputs\"); OUT.mkdir(exist_ok=True)\n",
    "P_CHUNKS   = OUT / \"chunks\"\n",
    "P_WIDE_PQ  = OUT / \"prices_wide.parquet\"\n",
    "P_WIDE_PKL = OUT / \"prices_wide.pkl\"\n",
    "P_SEC_CACHE = OUT / \"sec_cache\"\n",
    "P_META_YQ   = OUT / \"meta_yq.parquet\"\n",
    "P_META_PROG = OUT / \"meta_yq_progress.parquet\"\n",
    "P_YQ_SYM    = OUT / \"yq_symbol_cache\"\n",
    "\n",
    "# --- Yahoo float snapshot (consolidated) ---\n",
    "P_YQ_FLOAT_SNAP = OUT / \"yq_float_snapshot.parquet\"\n",
    "\n",
    "# Controls for the Yahoo float backfill step\n",
    "REBUILD_YQ_FLOAT   = False   # False = never re-scrape if a snapshot exists\n",
    "YQ_FLOAT_CACHE_ONLY = True   # True = read per-ticker cache files only; no network\n",
    "\n",
    "REBUILD_PRICES = False\n",
    "REBUILD_META   = False\n",
    "REBUILD_SEC    = False\n",
    "\n",
    "# --- Prices: load-or-resume ---\n",
    "def _load_prices_raw():\n",
    "    if P_WIDE_PQ.exists():  return pd.read_parquet(P_WIDE_PQ)\n",
    "    if P_WIDE_PKL.exists(): return pd.read_pickle(P_WIDE_PKL)\n",
    "    parts = sorted(P_CHUNKS.glob(\"prices_wide_*.parquet\"))\n",
    "    if parts: return pd.concat([pd.read_parquet(p) for p in parts], axis=1).sort_index()\n",
    "    return None\n",
    "\n",
    "prices_raw = None if REBUILD_PRICES else _load_prices_raw()\n",
    "\n",
    "# --- SEC: load recent + SIC from cache (prefer saved file if present) ---\n",
    "def _load_sec_recent():\n",
    "    sec_parq = OUT / \"sec_recent.parquet\"\n",
    "    sec_csv  = OUT / \"sec_recent.csv\"\n",
    "    if sec_parq.exists():\n",
    "        return pd.read_parquet(sec_parq).assign(ticker=lambda d: d['ticker'].str.upper())\n",
    "    if sec_csv.exists():\n",
    "        return pd.read_csv(sec_csv).assign(ticker=lambda d: d['ticker'].str.upper())\n",
    "    try:\n",
    "        return sec_recent_df.assign(ticker=lambda d: d['ticker'].str.upper())\n",
    "    except NameError:\n",
    "        return pd.DataFrame(columns=['ticker','cik','recent_form','recent_filing_date'])\n",
    "\n",
    "def _load_sic_from_cache(sec_df):\n",
    "    rows = []\n",
    "    for _, r in sec_df.dropna(subset=['cik']).iterrows():\n",
    "        fp = P_SEC_CACHE / f\"{r['cik']}.json\"\n",
    "        if not fp.exists(): \n",
    "            continue\n",
    "        try:\n",
    "            j = json.loads(fp.read_text())\n",
    "            rows.append({\n",
    "                'ticker': r['ticker'].upper(),\n",
    "                'cik': r['cik'],\n",
    "                'sic': j.get('sic'),\n",
    "                'sic_desc': j.get('sicDescription')\n",
    "            })\n",
    "        except Exception:\n",
    "            pass\n",
    "    return pd.DataFrame(rows).drop_duplicates('ticker')\n",
    "\n",
    "sec_recent_df = _load_sec_recent()\n",
    "sic_df = _load_sic_from_cache(sec_recent_df)\n",
    "\n",
    "def _sic_to_sector(s):\n",
    "    try: s = int(s)\n",
    "    except Exception: return None\n",
    "    if 1 <= s <= 9:    return 'Agriculture'\n",
    "    if 10 <= s <= 14:  return 'Mining'\n",
    "    if 15 <= s <= 17:  return 'Construction'\n",
    "    if 20 <= s <= 39:  return 'Manufacturing'\n",
    "    if 40 <= s <= 49:  return 'Transportation & Utilities'\n",
    "    if 50 <= s <= 51:  return 'Wholesale'\n",
    "    if 52 <= s <= 59:  return 'Retail'\n",
    "    if 60 <= s <= 67:  return 'Finance'\n",
    "    if 70 <= s <= 89:  return 'Services'\n",
    "    if 91 <= s <= 99:  return 'Public Administration'\n",
    "    return None\n",
    "\n",
    "if not sic_df.empty:\n",
    "    sic_df['sector_sic'] = sic_df['sic'].apply(_sic_to_sector)\n",
    "\n",
    "# --- Yahoo meta: load consolidated (parquet + progress + per-symbol cache) ---\n",
    "def _load_meta_yq():\n",
    "    frames = []\n",
    "    if P_META_YQ.exists():   frames.append(pd.read_parquet(P_META_YQ))\n",
    "    if P_META_PROG.exists(): frames.append(pd.read_parquet(P_META_PROG))\n",
    "    if P_YQ_SYM.exists():\n",
    "        rows = []\n",
    "        for fp in P_YQ_SYM.glob(\"*.json\"):\n",
    "            try:\n",
    "                j = json.loads(fp.read_text())\n",
    "                rows.append({\n",
    "                    'ticker': str(j.get('symbol','')).upper(),\n",
    "                    'market_cap': j.get('market_cap'),\n",
    "                    'sector': j.get('sector'),\n",
    "                    'industry': j.get('industry'),\n",
    "                })\n",
    "            except Exception:\n",
    "                pass\n",
    "        if rows: frames.append(pd.DataFrame(rows))\n",
    "    if not frames:\n",
    "        return pd.DataFrame(columns=['ticker','market_cap','sector','industry'])\n",
    "    out = (pd.concat(frames, ignore_index=True)\n",
    "             .dropna(subset=['ticker'])\n",
    "             .assign(ticker=lambda d: d['ticker'].str.upper())\n",
    "             .drop_duplicates('ticker', keep='first'))\n",
    "    return out\n",
    "\n",
    "meta_yq = None if REBUILD_META else _load_meta_yq()\n",
    "if meta_yq is None:\n",
    "    meta_yq = pd.DataFrame(columns=['ticker','market_cap','sector','industry'])\n",
    "\n",
    "# --- Fundamentals (idempotent): prefer Yahoo, backfill with SEC SIC sector ---\n",
    "fundamentals = (\n",
    "    meta_yq\n",
    "    .merge(sic_df[['ticker','sic','sic_desc','sector_sic']] if not sic_df.empty else pd.DataFrame(columns=['ticker','sic','sic_desc','sector_sic']),\n",
    "           on='ticker', how='left')\n",
    "    .merge(sec_recent_df[['ticker','cik','recent_form','recent_filing_date']] if not sec_recent_df.empty else pd.DataFrame(columns=['ticker','cik','recent_form','recent_filing_date']),\n",
    "           on='ticker', how='left')\n",
    ")\n",
    "if 'sector' not in fundamentals.columns:\n",
    "    fundamentals['sector'] = pd.NA\n",
    "fundamentals['sector'] = fundamentals['sector'].where(fundamentals['sector'].notna(), fundamentals.get('sector_sic'))\n",
    "\n",
    "_keep = ['ticker','market_cap','sector','industry','cik','sic','sic_desc','recent_form','recent_filing_date']\n",
    "for c in _keep:\n",
    "    if c not in fundamentals.columns: fundamentals[c] = pd.NA\n",
    "fundamentals = fundamentals[_keep].drop_duplicates('ticker')\n",
    "\n",
    "# --- Prices → tidy + returns (only if prices_raw already loaded) ---\n",
    "def tidy_from_yf(wide: pd.DataFrame) -> pd.DataFrame:\n",
    "    if wide is None or wide.empty:\n",
    "        return pd.DataFrame(columns=['date','ticker','open','high','low','close','adj close','volume'])\n",
    "    df = wide.copy(); df.index.name = 'date'\n",
    "    tidy = (df.stack(level=1, future_stack=True)\n",
    "              .rename_axis(index=['date','ticker'])\n",
    "              .reset_index()\n",
    "              .sort_values(['date','ticker'])\n",
    "              .reset_index(drop=True))\n",
    "    tidy.columns = [c.lower() if isinstance(c, str) else c for c in tidy.columns]\n",
    "    return tidy\n",
    "\n",
    "prices_tidy = tidy_from_yf(prices_raw)\n",
    "if not prices_tidy.empty:\n",
    "    adj = (prices_tidy[['date','ticker','adj close']].dropna()\n",
    "             .sort_values(['ticker','date']))\n",
    "    adj['ret']    = adj.groupby('ticker')['adj close'].pct_change()\n",
    "    adj['logret'] = np.log(adj['adj close'] / adj.groupby('ticker')['adj close'].shift(1))\n",
    "    returns = adj.dropna(subset=['ret','logret'])\n",
    "    analysis_df = (prices_tidy[['date','ticker','open','high','low','close','adj close','volume']]\n",
    "                   .merge(returns[['date','ticker','ret','logret']], on=['date','ticker'], how='left'))\n",
    "else:\n",
    "    analysis_df = pd.DataFrame(columns=['date','ticker','open','high','low','close','adj close','volume','ret','logret'])\n",
    "\n",
    "# --- Final merge + save (safe to re-run) ---\n",
    "analysis_enriched = (\n",
    "    analysis_df\n",
    "    .merge(fundamentals, on='ticker', how='left')\n",
    "    .sort_values(['ticker','date'])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "analysis_enriched.to_parquet(OUT / \"analysis_enriched.parquet\")\n",
    "analysis_enriched.to_csv(OUT / \"analysis_enriched.csv\", index=False)\n",
    "\n",
    "print(\"prices_raw:\", None if prices_raw is None else prices_raw.shape)\n",
    "print(\"fundamentals tickers:\", fundamentals['ticker'].nunique())\n",
    "print(\"analysis_enriched rows:\", len(analysis_enriched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67f14817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safe defaults for reruns\n",
    "if 'YQ_FLOAT_CACHE_ONLY' not in globals(): YQ_FLOAT_CACHE_ONLY = False\n",
    "if 'NO_NETWORK' not in globals():          NO_NETWORK = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ee18544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active_tickers: 5118\n"
     ]
    }
   ],
   "source": [
    "# Derive active_tickers from prices_raw (upper-cased), for later harvesters\n",
    "if 'prices_raw' in globals() and prices_raw is not None:\n",
    "    if isinstance(prices_raw.columns, pd.MultiIndex) and 'Adj Close' in prices_raw.columns.get_level_values(0):\n",
    "        active_tickers = [t for t in prices_raw['Adj Close'].columns if prices_raw['Adj Close'][t].notna().any()]\n",
    "    else:\n",
    "        active_tickers = list(prices_raw.columns)\n",
    "    active_tickers = [str(t).upper() for t in active_tickers]\n",
    "else:\n",
    "    active_tickers = []\n",
    "print(\"active_tickers:\", len(active_tickers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c83456",
   "metadata": {},
   "source": [
    "## Added: Split flags + Time‑varying shares/free float\n",
    "The next cells were inserted to compute:\n",
    "\n",
    "- `performed_split` and `performed_reverse_split` (per-date flags)\n",
    "- `shares_outstanding` (time‑varying, SEC)\n",
    "- `float_shares` / `free_float` (time‑varying, SEC public float ÷ price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dbc9b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split_flags rows: 19301 | unique tickers: 2505\n"
     ]
    }
   ],
   "source": [
    "# === S1 (final): Split events (cached) + per-date flags WITHOUT merge_asof ===\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, json, time, re, logging\n",
    "import yfinance as yf\n",
    "\n",
    "# silence yfinance info prints\n",
    "logging.getLogger(\"yfinance\").setLevel(logging.ERROR)\n",
    "\n",
    "OUT = Path(\"outputs\")\n",
    "SPLITS_DIR = OUT / \"split_events\"; SPLITS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ALL_SPLITS_PQ = OUT / \"split_events.parquet\"\n",
    "SPLIT_FLAGS_PQ = OUT / \"split_flags.parquet\"\n",
    "\n",
    "# Warrants/rights/units often trigger 'period max' warnings & never split anyway\n",
    "_SKIP_SUFFIX_RE = re.compile(r\".*(?:W|WS|WT|W[A-D]?|U|R)$\")  # tweak if you want to try them\n",
    "\n",
    "def _split_df_from_actions(acts: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Extract a tidy split DF from yfinance get_actions() result.\"\"\"\n",
    "    if acts is None or acts.empty or (\"Stock Splits\" not in acts.columns):\n",
    "        return pd.DataFrame(columns=['date','ratio'])\n",
    "    s = acts[\"Stock Splits\"].dropna()\n",
    "    if s.empty:\n",
    "        return pd.DataFrame(columns=['date','ratio'])\n",
    "    idx = pd.to_datetime(s.index).tz_localize(None).normalize()\n",
    "    return pd.DataFrame({\"date\": idx, \"ratio\": pd.to_numeric(s.values, errors=\"coerce\")}).dropna(subset=[\"ratio\"])\n",
    "\n",
    "def fetch_split_events_one(ticker: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns rows: date, ticker, ratio, performed_split, performed_reverse_split\n",
    "    Caches to outputs/split_events/<TICKER>.parquet for fast reruns.\n",
    "    \"\"\"\n",
    "    t = str(ticker).upper()\n",
    "    fp = SPLITS_DIR / f\"{t}.parquet\"\n",
    "    if fp.exists():\n",
    "        try:\n",
    "            return pd.read_parquet(fp)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # optional skip for non-common-stock suffixes\n",
    "    if _SKIP_SUFFIX_RE.match(t):\n",
    "        df = pd.DataFrame(columns=['date','ticker','ratio','performed_split','performed_reverse_split'])\n",
    "        try: df.to_parquet(fp, index=False)\n",
    "        except Exception: pass\n",
    "        return df\n",
    "\n",
    "    acts = None\n",
    "    tk = yf.Ticker(t)\n",
    "    try:\n",
    "        acts = tk.get_actions(period=\"max\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            acts = tk.get_actions(period=\"5d\")  # allowed even for odd instruments\n",
    "        except Exception:\n",
    "            acts = None\n",
    "\n",
    "    base = _split_df_from_actions(acts)\n",
    "    if base.empty:\n",
    "        df = pd.DataFrame(columns=['date','ticker','ratio','performed_split','performed_reverse_split'])\n",
    "    else:\n",
    "        df = base.copy()\n",
    "        df['ticker'] = t\n",
    "        df['performed_split'] = (df['ratio'] > 1).astype('int8')\n",
    "        df['performed_reverse_split'] = ((df['ratio'] > 0) & (df['ratio'] < 1)).astype('int8')\n",
    "        df = df[['date','ticker','ratio','performed_split','performed_reverse_split']]\n",
    "\n",
    "    # cache\n",
    "    try: df.to_parquet(fp, index=False)\n",
    "    except Exception: pass\n",
    "    time.sleep(0.08)  # be polite\n",
    "    return df\n",
    "\n",
    "def build_split_events(symbols) -> pd.DataFrame:\n",
    "    parts = [fetch_split_events_one(t) for t in symbols]\n",
    "    if not parts:\n",
    "        return pd.DataFrame(columns=['date','ticker','ratio','performed_split','performed_reverse_split'])\n",
    "    out = pd.concat(parts, ignore_index=True)\n",
    "    return out.drop_duplicates(['ticker','date'])\n",
    "\n",
    "# Load/build full split event table once\n",
    "if ALL_SPLITS_PQ.exists():\n",
    "    split_events = pd.read_parquet(ALL_SPLITS_PQ)\n",
    "else:\n",
    "    split_events = build_split_events(active_tickers)\n",
    "    split_events.to_parquet(ALL_SPLITS_PQ, index=False)\n",
    "\n",
    "# ---------- Map to first trading day ON/AFTER event date (no merge_asof) ----------\n",
    "# Prepare trading calendar from your price table\n",
    "trading = (\n",
    "    analysis_df[['date','ticker']]\n",
    "    .drop_duplicates()\n",
    "    .assign(\n",
    "        date=lambda d: pd.to_datetime(d['date']).dt.tz_localize(None).dt.normalize(),\n",
    "        ticker=lambda d: d['ticker'].astype(str),\n",
    "    )\n",
    "    .sort_values(['ticker','date'])\n",
    ")\n",
    "\n",
    "# Prepare events with normalized types\n",
    "events = (\n",
    "    split_events[['date','ticker','performed_split','performed_reverse_split']]\n",
    "    .rename(columns={'date':'event_date'})\n",
    "    .assign(\n",
    "        event_date=lambda d: pd.to_datetime(d['event_date']).dt.tz_localize(None).dt.normalize(),\n",
    "        ticker=lambda d: d['ticker'].astype(str),\n",
    "    )\n",
    "    .dropna(subset=['event_date'])\n",
    "    .sort_values(['ticker','event_date'])\n",
    ")\n",
    "\n",
    "# fast per-ticker trading date arrays\n",
    "dates_by_ticker = (\n",
    "    trading.groupby('ticker')['date']\n",
    "           .apply(lambda s: s.to_numpy())   # numpy datetime64[ns] array\n",
    "           .to_dict()\n",
    ")\n",
    "\n",
    "def next_trading_date(tkr: str, dt) -> pd.Timestamp:\n",
    "    arr = dates_by_ticker.get(tkr)\n",
    "    if arr is None or arr.size == 0:\n",
    "        return pd.NaT\n",
    "    i = arr.searchsorted(dt)  # first trading date >= event_date\n",
    "    return pd.NaT if i >= arr.size else arr[i]\n",
    "\n",
    "# map each event to its effective trading day\n",
    "events = events.copy()\n",
    "events['effective_date'] = [\n",
    "    next_trading_date(t, d) for t, d in zip(events['ticker'].to_numpy(), events['event_date'].to_numpy())\n",
    "]\n",
    "events_eff = events.dropna(subset=['effective_date'])\n",
    "\n",
    "split_flags = (\n",
    "    events_eff\n",
    "    .rename(columns={'effective_date':'date'})\n",
    "    [['ticker','date','performed_split','performed_reverse_split']]\n",
    "    .drop_duplicates(['ticker','date'])\n",
    "    .sort_values(['ticker','date'])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "try: split_flags.to_parquet(SPLIT_FLAGS_PQ, index=False)\n",
    "except Exception: pass\n",
    "\n",
    "print(\"split_flags rows:\", len(split_flags), \"| unique tickers:\", split_flags['ticker'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfc7d869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events_enriched rows: 197010 | tickers: 3762\n"
     ]
    }
   ],
   "source": [
    "# === T1 (full, no-merge_asof): SEC companyfacts -> shares_outstanding & float_shares (time-varying) ===\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, json, time\n",
    "\n",
    "OUT = Path(\"outputs\")\n",
    "SEC_FACTS_DIR = OUT / \"sec_facts\"; SEC_FACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ALL_SHARES_EVENTS_PQ = OUT / \"shares_events.parquet\"\n",
    "\n",
    "# ---- Reuse existing SEC session/headers if available; else create them ----\n",
    "try:\n",
    "    SESSION, SEC_HEADERS\n",
    "except NameError:\n",
    "    import requests\n",
    "    from requests.adapters import HTTPAdapter, Retry\n",
    "    SEC_HEADERS = {\"User-Agent\": \"Ozkan Gelincik <ozkangelincik@gmail.com>\", \"Accept\": \"application/json\"}\n",
    "    def make_session():\n",
    "        sess = requests.Session()\n",
    "        retries = Retry(total=5, connect=3, read=3, backoff_factor=0.5,\n",
    "                        status_forcelist=[429,500,502,503,504], allowed_methods=[\"GET\"])\n",
    "        adapter = HTTPAdapter(max_retries=retries, pool_connections=30, pool_maxsize=30)\n",
    "        sess.mount(\"https://\", adapter)\n",
    "        sess.headers.update(SEC_HEADERS)\n",
    "        return sess\n",
    "    SESSION = make_session()\n",
    "\n",
    "# ---- Cache-first fetch for companyfacts ----\n",
    "def fetch_company_facts(cik: str):\n",
    "    cik = str(cik).zfill(10)\n",
    "    fp = SEC_FACTS_DIR / f\"{cik}.json\"\n",
    "    if fp.exists():\n",
    "        try:\n",
    "            return json.loads(fp.read_text())\n",
    "        except Exception:\n",
    "            pass\n",
    "    url = f\"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json\"\n",
    "    try:\n",
    "        r = SESSION.get(url, timeout=30)\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "        j = r.json()\n",
    "        try: fp.write_text(json.dumps(j))\n",
    "        except Exception: pass\n",
    "        time.sleep(0.08)  # be polite\n",
    "        return j\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _collect_values(facts, tax, name, unit=None):\n",
    "    try:\n",
    "        units = facts['facts'][tax][name]['units']\n",
    "    except Exception:\n",
    "        return []\n",
    "    out = []\n",
    "    for u, arr in (units or {}).items():\n",
    "        if unit and u != unit:\n",
    "            continue\n",
    "        for d in arr or []:\n",
    "            end = d.get('end'); val = d.get('val')\n",
    "            if end and val is not None:\n",
    "                out.append({\"date\": pd.to_datetime(end).normalize(), \"val\": float(val), \"unit\": u, \"form\": d.get(\"form\")})\n",
    "    return out\n",
    "\n",
    "def extract_shares_series(facts_json: dict) -> pd.DataFrame:\n",
    "    keys = [\n",
    "        ('us-gaap',  'CommonStockSharesOutstanding'),\n",
    "        ('dei',      'EntityCommonStockSharesOutstanding'),\n",
    "        ('ifrs-full','NumberOfSharesOutstanding'),\n",
    "        ('ifrs-full','IssuedCapitalNumberOfShares'),\n",
    "    ]\n",
    "    rows = []\n",
    "    for tax, name in keys:\n",
    "        rows += _collect_values(facts_json, tax, name, unit='shares')\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=['date','shares_outstanding'])\n",
    "    df = (pd.DataFrame(rows).sort_values('date').drop_duplicates('date', keep='last')\n",
    "            .rename(columns={'val':'shares_outstanding'})[['date','shares_outstanding']])\n",
    "    return df\n",
    "\n",
    "def extract_public_float_usd(facts_json: dict) -> pd.DataFrame:\n",
    "    rows = _collect_values(facts_json, 'dei', 'EntityPublicFloat', unit='USD')\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=['date','entity_public_float_usd'])\n",
    "    df = (pd.DataFrame(rows).sort_values('date').drop_duplicates('date', keep='last')\n",
    "            .rename(columns={'val':'entity_public_float_usd'})[['date','entity_public_float_usd']])\n",
    "    return df\n",
    "\n",
    "def build_shares_events_for_universe(fundamentals_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    parts = []\n",
    "    for _, r in fundamentals_df.dropna(subset=['cik']).iterrows():\n",
    "        cik = str(r['cik']).zfill(10); tkr = str(r['ticker']).upper()\n",
    "        facts = fetch_company_facts(cik)\n",
    "        if not isinstance(facts, dict):\n",
    "            continue\n",
    "        sh = extract_shares_series(facts)\n",
    "        pf = extract_public_float_usd(facts)\n",
    "        if sh.empty and pf.empty:\n",
    "            continue\n",
    "        ev = sh.merge(pf, on='date', how='outer').sort_values('date')\n",
    "        ev['ticker'] = tkr\n",
    "        parts.append(ev[['date','ticker','shares_outstanding','entity_public_float_usd']])\n",
    "    if not parts:\n",
    "        return pd.DataFrame(columns=['date','ticker','shares_outstanding','entity_public_float_usd'])\n",
    "    return (pd.concat(parts, ignore_index=True)\n",
    "              .sort_values(['ticker','date'])\n",
    "              .drop_duplicates(['ticker','date'], keep='last'))\n",
    "\n",
    "# ---- Load or build the SEC-derived events ----\n",
    "if ALL_SHARES_EVENTS_PQ.exists():\n",
    "    shares_events = pd.read_parquet(ALL_SHARES_EVENTS_PQ)\n",
    "else:\n",
    "    shares_events = build_shares_events_for_universe(fundamentals)\n",
    "    shares_events.to_parquet(ALL_SHARES_EVENTS_PQ, index=False)\n",
    "\n",
    "# ---- Normalize & de-dup both sides (dates tz-naive midnight; tickers as str) ----\n",
    "shares_events = (\n",
    "    shares_events\n",
    "    .assign(\n",
    "        date=lambda d: pd.to_datetime(d['date']).dt.tz_localize(None).dt.normalize(),\n",
    "        ticker=lambda d: d['ticker'].astype(str)\n",
    "    )\n",
    "    .dropna(subset=['date','ticker'])\n",
    "    .drop_duplicates(['ticker','date'], keep='last')\n",
    "    .sort_values(['ticker','date'])\n",
    ")\n",
    "\n",
    "price_ev = (\n",
    "    analysis_df[['date','ticker','adj close']]\n",
    "    .rename(columns={'adj close':'adj_close'})\n",
    "    .assign(\n",
    "        date=lambda d: pd.to_datetime(d['date']).dt.tz_localize(None).dt.normalize(),\n",
    "        ticker=lambda d: d['ticker'].astype(str)\n",
    "    )\n",
    "    .dropna(subset=['date','ticker','adj_close'])\n",
    "    .drop_duplicates(['ticker','date'], keep='last')\n",
    "    .sort_values(['ticker','date'])\n",
    ")\n",
    "\n",
    "# ---- Compute price on/as-of filing date per ticker (no merge_asof, no searchsorted) ----\n",
    "# For each ticker: union the price index and filing dates, forward-fill, and then pick exact filing dates.\n",
    "events_enriched_list = []\n",
    "g_prices = dict(tuple(price_ev.groupby('ticker')))  # ticker -> df(date, adj_close)\n",
    "\n",
    "for tkr, g in shares_events.groupby('ticker', sort=False):\n",
    "    # filing dates for this ticker\n",
    "    filing = g[['date']].drop_duplicates().set_index('date')\n",
    "    # price series for this ticker (may be empty)\n",
    "    p = g_prices.get(tkr)\n",
    "    if p is None or p.empty:\n",
    "        # no prices: join will yield NaN adj_close\n",
    "        g2 = g.copy()\n",
    "        g2['adj_close'] = np.nan\n",
    "        events_enriched_list.append(g2)\n",
    "        continue\n",
    "    p = p.set_index('date')['adj_close'].sort_index()\n",
    "    # union index and forward-fill\n",
    "    union_idx = p.index.union(filing.index).sort_values()\n",
    "    p_ffill = p.reindex(union_idx).ffill()\n",
    "    # take values at filing dates\n",
    "    adj_on_filing = p_ffill.reindex(filing.index)\n",
    "    g2 = g.copy()\n",
    "    g2['adj_close'] = adj_on_filing.values\n",
    "    events_enriched_list.append(g2)\n",
    "\n",
    "events_enriched = pd.concat(events_enriched_list, ignore_index=True) if events_enriched_list else shares_events.copy()\n",
    "events_enriched['float_shares'] = np.where(\n",
    "    events_enriched.get('entity_public_float_usd').notna() & events_enriched['adj_close'].notna(),\n",
    "    events_enriched['entity_public_float_usd'] / events_enriched['adj_close'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "events_enriched = events_enriched[['date','ticker','shares_outstanding','float_shares']].sort_values(['ticker','date'])\n",
    "print(\"events_enriched rows:\", len(events_enriched), \"| tickers:\", events_enriched['ticker'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2526cc5e",
   "metadata": {},
   "source": [
    "# All‑US Listings EDA with `yfinance`\n",
    "\n",
    "This notebook pulls **NASDAQ + NYSE** tickers, cleans the list, downloads prices with **`yfinance`**, and gives you a clean starting point for EDA.\n",
    "\n",
    "### What you'll get\n",
    "- Robust ticker collection via `yahoo_fin` (with a fallback to NasdaqTrader FTP files)\n",
    "- Exclusions for ETFs, warrants, preferreds, rights, and odd tickers\n",
    "- Batched downloads with `yf.download()` for speed, retry/backoff logic\n",
    "- A tidy daily returns dataset ready for analysis\n",
    "- Optional market‑cap join via `yahoo_fin` (when available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2e81d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prices_wide.parquet          FOUND    (102.0 MB)\n",
      "meta_yq.parquet              FOUND    (0.1 MB)\n",
      "meta_yq_progress.parquet     FOUND    (0.0 MB)\n",
      "sec_recent.parquet           FOUND    (0.1 MB)\n",
      "sec_cache/                   FOUND    (3357 files)\n",
      "yq_symbol_cache/             FOUND    (3180 files)\n",
      "\n",
      "Tip: leave REBUILD_PRICES=REBUILD_META=REBUILD_SEC=False so the bootstrap loads from disk only.\n"
     ]
    }
   ],
   "source": [
    "# === PREFLIGHT (what will be used on rerun) ===\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "OUT = Path(\"outputs\")\n",
    "paths = {\n",
    "    \"prices_wide.parquet\": OUT / \"prices_wide.parquet\",\n",
    "    \"meta_yq.parquet\":     OUT / \"meta_yq.parquet\",\n",
    "    \"meta_yq_progress.parquet\": OUT / \"meta_yq_progress.parquet\",\n",
    "    \"sec_recent.parquet\":  OUT / \"sec_recent.parquet\",\n",
    "    \"sec_cache/\":          OUT / \"sec_cache\",\n",
    "    \"yq_symbol_cache/\":    OUT / \"yq_symbol_cache\",\n",
    "}\n",
    "\n",
    "def _size(p: Path):\n",
    "    try:\n",
    "        return f\"{p.stat().st_size/1_048_576:.1f} MB\"\n",
    "    except Exception:\n",
    "        return \"-\"\n",
    "\n",
    "for name, p in paths.items():\n",
    "    if p.suffix:  # file\n",
    "        print(f\"{name:<28} {'FOUND' if p.exists() else 'missing':<8} {('('+_size(p)+')') if p.exists() else ''}\")\n",
    "    else:         # directory\n",
    "        exists = p.exists()\n",
    "        n = sum(1 for _ in p.glob('*')) if exists else 0\n",
    "        print(f\"{name:<28} {'FOUND' if exists else 'missing':<8} {f'({n} files)' if exists else ''}\")\n",
    "\n",
    "print(\"\\nTip: leave REBUILD_PRICES=REBUILD_META=REBUILD_SEC=False so the bootstrap loads from disk only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d2bcc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: prices_raw already loaded from cache (Bootstrap).\n"
     ]
    }
   ],
   "source": [
    "import pandas as _pd\n",
    "if 'prices_raw' not in globals() or prices_raw is None or (isinstance(prices_raw, _pd.DataFrame) and prices_raw.empty):\n",
    "    # === BOOTSTRAP / RESUME GUARD ===\n",
    "    from pathlib import Path\n",
    "    import pandas as pd, numpy as np, json\n",
    "\n",
    "    # Paths & flags\n",
    "    OUT = Path(\"outputs\"); OUT.mkdir(exist_ok=True)\n",
    "    P_CHUNKS   = OUT / \"chunks\"\n",
    "    P_WIDE_PQ  = OUT / \"prices_wide.parquet\"\n",
    "    P_WIDE_PKL = OUT / \"prices_wide.pkl\"\n",
    "    P_SEC_CACHE = OUT / \"sec_cache\"\n",
    "    P_META_YQ   = OUT / \"meta_yq.parquet\"\n",
    "    P_META_PROG = OUT / \"meta_yq_progress.parquet\"\n",
    "    P_YQ_SYM    = OUT / \"yq_symbol_cache\"\n",
    "\n",
    "    REBUILD_PRICES = False\n",
    "    REBUILD_META   = False\n",
    "    REBUILD_SEC    = False\n",
    "\n",
    "    # --- Prices: load-or-resume ---\n",
    "    def _load_prices_raw():\n",
    "        if P_WIDE_PQ.exists():  return pd.read_parquet(P_WIDE_PQ)\n",
    "        if P_WIDE_PKL.exists(): return pd.read_pickle(P_WIDE_PKL)\n",
    "        parts = sorted(P_CHUNKS.glob(\"prices_wide_*.parquet\"))\n",
    "        if parts: return pd.concat([pd.read_parquet(p) for p in parts], axis=1).sort_index()\n",
    "        return None\n",
    "\n",
    "    prices_raw = None if REBUILD_PRICES else _load_prices_raw()\n",
    "\n",
    "    # --- SEC: load recent + SIC from cache (if available) ---\n",
    "    def _load_sec_recent():\n",
    "        # If you saved sec_recent_df previously, load it here; otherwise start empty.\n",
    "        try:\n",
    "            return sec_recent_df.assign(ticker=lambda d: d['ticker'].str.upper())\n",
    "        except NameError:\n",
    "            return pd.DataFrame(columns=['ticker','cik','recent_form','recent_filing_date'])\n",
    "\n",
    "    def _load_sic_from_cache(sec_df):\n",
    "        rows = []\n",
    "        for _, r in sec_df.dropna(subset=['cik']).iterrows():\n",
    "            fp = P_SEC_CACHE / f\"{r['cik']}.json\"\n",
    "            if not fp.exists(): \n",
    "                continue\n",
    "            try:\n",
    "                j = json.loads(fp.read_text())\n",
    "                rows.append({\n",
    "                    'ticker': r['ticker'].upper(),\n",
    "                    'cik': r['cik'],\n",
    "                    'sic': j.get('sic'),\n",
    "                    'sic_desc': j.get('sicDescription')\n",
    "                })\n",
    "            except Exception:\n",
    "                pass\n",
    "        return pd.DataFrame(rows).drop_duplicates('ticker')\n",
    "\n",
    "    sec_recent_df = _load_sec_recent()\n",
    "    sic_df = _load_sic_from_cache(sec_recent_df)\n",
    "\n",
    "    def _sic_to_sector(s):\n",
    "        try: s = int(s)\n",
    "        except Exception: return None\n",
    "        if 1 <= s <= 9:    return 'Agriculture'\n",
    "        if 10 <= s <= 14:  return 'Mining'\n",
    "        if 15 <= s <= 17:  return 'Construction'\n",
    "        if 20 <= s <= 39:  return 'Manufacturing'\n",
    "        if 40 <= s <= 49:  return 'Transportation & Utilities'\n",
    "        if 50 <= s <= 51:  return 'Wholesale'\n",
    "        if 52 <= s <= 59:  return 'Retail'\n",
    "        if 60 <= s <= 67:  return 'Finance'\n",
    "        if 70 <= s <= 89:  return 'Services'\n",
    "        if 91 <= s <= 99:  return 'Public Administration'\n",
    "        return None\n",
    "\n",
    "    if not sic_df.empty:\n",
    "        sic_df['sector_sic'] = sic_df['sic'].apply(_sic_to_sector)\n",
    "\n",
    "    # --- Yahoo meta: load consolidated (parquet + progress + per-symbol cache) ---\n",
    "    def _load_meta_yq():\n",
    "        frames = []\n",
    "        if P_META_YQ.exists():   frames.append(pd.read_parquet(P_META_YQ))\n",
    "        if P_META_PROG.exists(): frames.append(pd.read_parquet(P_META_PROG))\n",
    "        if P_YQ_SYM.exists():\n",
    "            rows = []\n",
    "            for fp in P_YQ_SYM.glob(\"*.json\"):\n",
    "                try:\n",
    "                    j = json.loads(fp.read_text())\n",
    "                    rows.append({\n",
    "                        'ticker': str(j.get('symbol','')).upper(),\n",
    "                        'market_cap': j.get('market_cap'),\n",
    "                        'sector': j.get('sector'),\n",
    "                        'industry': j.get('industry'),\n",
    "                    })\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if rows: frames.append(pd.DataFrame(rows))\n",
    "        if not frames:\n",
    "            return pd.DataFrame(columns=['ticker','market_cap','sector','industry'])\n",
    "        out = (pd.concat(frames, ignore_index=True)\n",
    "                 .dropna(subset=['ticker'])\n",
    "                 .assign(ticker=lambda d: d['ticker'].str.upper())\n",
    "                 .drop_duplicates('ticker', keep='first'))\n",
    "        return out\n",
    "\n",
    "    meta_yq = None if REBUILD_META else _load_meta_yq()\n",
    "    if meta_yq is None:\n",
    "        meta_yq = pd.DataFrame(columns=['ticker','market_cap','sector','industry'])\n",
    "\n",
    "    # --- Fundamentals (idempotent): prefer Yahoo, backfill with SEC SIC sector ---\n",
    "    fundamentals = (\n",
    "        meta_yq\n",
    "        .merge(sic_df[['ticker','sic','sic_desc','sector_sic']] if not sic_df.empty else pd.DataFrame(columns=['ticker','sic','sic_desc','sector_sic']),\n",
    "               on='ticker', how='left')\n",
    "        .merge(sec_recent_df[['ticker','cik','recent_form','recent_filing_date']] if not sec_recent_df.empty else pd.DataFrame(columns=['ticker','cik','recent_form','recent_filing_date']),\n",
    "               on='ticker', how='left')\n",
    "    )\n",
    "    if 'sector' not in fundamentals.columns:\n",
    "        fundamentals['sector'] = pd.NA\n",
    "    fundamentals['sector'] = fundamentals['sector'].where(fundamentals['sector'].notna(), fundamentals.get('sector_sic'))\n",
    "\n",
    "    _keep = ['ticker','market_cap','sector','industry','cik','sic','sic_desc','recent_form','recent_filing_date']\n",
    "    for c in _keep:\n",
    "        if c not in fundamentals.columns: fundamentals[c] = pd.NA\n",
    "    fundamentals = fundamentals[_keep].drop_duplicates('ticker')\n",
    "\n",
    "    # --- Prices → tidy + returns (only if prices_raw already loaded) ---\n",
    "    def tidy_from_yf(wide: pd.DataFrame) -> pd.DataFrame:\n",
    "        if wide is None or wide.empty:\n",
    "            return pd.DataFrame(columns=['date','ticker','open','high','low','close','adj close','volume'])\n",
    "        df = wide.copy(); df.index.name = 'date'\n",
    "        tidy = (df.stack(level=1, future_stack=True)\n",
    "                  .rename_axis(index=['date','ticker'])\n",
    "                  .reset_index()\n",
    "                  .sort_values(['date','ticker'])\n",
    "                  .reset_index(drop=True))\n",
    "        tidy.columns = [c.lower() if isinstance(c, str) else c for c in tidy.columns]\n",
    "        return tidy\n",
    "\n",
    "    prices_tidy = tidy_from_yf(prices_raw)\n",
    "    if not prices_tidy.empty:\n",
    "        adj = (prices_tidy[['date','ticker','adj close']].dropna()\n",
    "                 .sort_values(['ticker','date']))\n",
    "        adj['ret']    = adj.groupby('ticker')['adj close'].pct_change()\n",
    "        adj['logret'] = np.log(adj['adj close'] / adj.groupby('ticker')['adj close'].shift(1))\n",
    "        returns = adj.dropna(subset=['ret','logret'])\n",
    "        analysis_df = (prices_tidy[['date','ticker','open','high','low','close','adj close','volume']]\n",
    "                       .merge(returns[['date','ticker','ret','logret']], on=['date','ticker'], how='left'))\n",
    "    else:\n",
    "        analysis_df = pd.DataFrame(columns=['date','ticker','open','high','low','close','adj close','volume','ret','logret'])\n",
    "\n",
    "    # --- Final merge + save (safe to re-run) ---\n",
    "    analysis_enriched = (\n",
    "        analysis_df\n",
    "        .merge(fundamentals, on='ticker', how='left')\n",
    "        .sort_values(['ticker','date'])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    analysis_enriched.to_parquet(OUT / \"analysis_enriched.parquet\")\n",
    "    analysis_enriched.to_csv(OUT / \"analysis_enriched.csv\", index=False)\n",
    "\n",
    "    print(\"prices_raw:\", None if prices_raw is None else prices_raw.shape)\n",
    "    print(\"fundamentals tickers:\", fundamentals['ticker'].nunique())\n",
    "    print(\"analysis_enriched rows:\", len(analysis_enriched))\n",
    "else:\n",
    "    print('Skipping: prices_raw already loaded from cache (Bootstrap).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82be11f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install yfinance yahoo_fin pandas numpy pyarrow tqdm requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0d1f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install requests_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63366f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Certain functionality \n",
      "             requires requests_html, which is not installed.\n",
      "             \n",
      "             Install using: \n",
      "             pip install requests_html\n",
      "             \n",
      "             After installation, you may have to restart your Python session.\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "# If needed, install dependencies in your environment (uncomment if running locally):\n",
    "# !pip install yfinance yahoo_fin pandas numpy pyarrow tqdm requests\n",
    "\n",
    "import re\n",
    "import time\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm # type: ignore\n",
    "\n",
    "import yfinance as yf # type: ignore\n",
    "try:\n",
    "    from yahoo_fin import stock_info as si # type: ignore\n",
    "    YAHOO_FIN_OK = True\n",
    "except Exception:\n",
    "    YAHOO_FIN_OK = False\n",
    "import requests\n",
    "\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.width', 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f71872",
   "metadata": {},
   "source": [
    "## 1) Get NASDAQ & NYSE tickers\n",
    "\n",
    "Primary method uses `yahoo_fin`. If unavailable, we fall back to NasdaqTrader FTP symbol directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "384f19dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ticker exchange\n",
      "0   AACB   NASDAQ\n",
      "1  AACBR   NASDAQ\n",
      "2  AACBU   NASDAQ\n",
      "3   AACG   NASDAQ\n",
      "4   AACI   NASDAQ\n",
      "Total raw tickers: 5136\n"
     ]
    }
   ],
   "source": [
    "def get_exchange_tickers() -> pd.DataFrame:\n",
    "    \"\"\"Return a DataFrame with columns [ticker, exchange] for NASDAQ and NYSE.\n",
    "    Uses yahoo_fin first; falls back to NasdaqTrader FTP text files.\n",
    "    \"\"\"\n",
    "    tickers = []\n",
    "    if YAHOO_FIN_OK:\n",
    "        try:\n",
    "            nasdaq = si.tickers_nasdaq()\n",
    "            tickers.extend([(t, 'NASDAQ') for t in nasdaq])\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            nyse = si.tickers_nyse()\n",
    "            tickers.extend([(t, 'NYSE') for t in nyse])\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not tickers:\n",
    "        # Fallback: NasdaqTrader\n",
    "        base = \"https://ftp.nasdaqtrader.com/SymbolDirectory/\"\n",
    "        urls = {\n",
    "            'NASDAQ': base + 'nasdaqlisted.txt',\n",
    "            'OTHER': base + 'otherlisted.txt'\n",
    "        }\n",
    "        for ex, url in urls.items():\n",
    "            r = requests.get(url, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            df = pd.read_csv(pd.compat.StringIO(r.text), sep='|')\n",
    "            if ex == 'NASDAQ':\n",
    "                syms = df['Symbol'].tolist()\n",
    "                tickers.extend([(t, 'NASDAQ') for t in syms])\n",
    "            else:\n",
    "                syms = df['ACT Symbol'].tolist()\n",
    "                # OTHER includes NYSE + NYSE MKT + BATS etc., we’ll label as NYSE-ish\n",
    "                tickers.extend([(t, 'NYSE') for t in syms])\n",
    "    out = pd.DataFrame(tickers, columns=['ticker', 'exchange']).drop_duplicates('ticker')\n",
    "    return out\n",
    "\n",
    "tickers_df = get_exchange_tickers()\n",
    "print(tickers_df.head())\n",
    "print(f\"Total raw tickers: {len(tickers_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3511a1",
   "metadata": {},
   "source": [
    "## 2) Clean the tickers\n",
    "\n",
    "Remove ETFs, warrants, preferreds, rights, units, and other non‑common equities. Feel free to tweak these rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47a3081c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: 5135 tickers remaining\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>exchange</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AACB</td>\n",
       "      <td>NASDAQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AACBR</td>\n",
       "      <td>NASDAQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AACBU</td>\n",
       "      <td>NASDAQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AACG</td>\n",
       "      <td>NASDAQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AACI</td>\n",
       "      <td>NASDAQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker exchange\n",
       "0   AACB   NASDAQ\n",
       "1  AACBR   NASDAQ\n",
       "2  AACBU   NASDAQ\n",
       "3   AACG   NASDAQ\n",
       "4   AACI   NASDAQ"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXCLUDE_PATTERNS = [\n",
    "    r\"-\\w+$\",         # suffixes like -W, -U, -R, -P etc.\n",
    "    r\"\\^\",            # symbols with carets\n",
    "    r\"\\.\\w+$\",       # BRK.B style dots (optional, keep if you want)\n",
    "]\n",
    "\n",
    "ETF_HINTS = [\n",
    "    ' ETF', ' Trust', ' Fund', ' ETN'\n",
    "]\n",
    "\n",
    "def looks_like_etf(name: str) -> bool:\n",
    "    if not isinstance(name, str):\n",
    "        return False\n",
    "    up = name.upper()\n",
    "    return any(h in up for h in ETF_HINTS)\n",
    "\n",
    "def clean_tickers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # If yahoo_fin exists, we can try fetching company names; otherwise, keep as is\n",
    "    # Here we only filter by symbol patterns to keep it fast and general.\n",
    "    pat = re.compile(\"|\".join(EXCLUDE_PATTERNS), re.IGNORECASE)\n",
    "    df['exclude'] = df['ticker'].astype(str).str.contains(pat)\n",
    "    cleaned = df[~df['exclude']].drop(columns=['exclude'])\n",
    "    cleaned = cleaned[~cleaned['ticker'].str.contains(r\"[^A-Z\\.]\", regex=True, na=False)]\n",
    "    cleaned = cleaned[cleaned['ticker'].str.len() <= 5]  # optional: US equities typically <=5 chars\n",
    "    cleaned = cleaned.drop_duplicates('ticker').reset_index(drop=True)\n",
    "    print(f\"After cleaning: {len(cleaned)} tickers remaining\")\n",
    "    return cleaned\n",
    "\n",
    "tickers_clean = clean_tickers(tickers_df)\n",
    "tickers_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d52582",
   "metadata": {},
   "source": [
    "## 3) Batch download price history\n",
    "\n",
    "- We use `yf.download()` with *lists* of tickers for speed\n",
    "- Chunking avoids URL length limits and rate‑limit errors\n",
    "- Simple retry/backoff to be resilient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "898be9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: prices_raw already loaded from cache (Bootstrap).\n"
     ]
    }
   ],
   "source": [
    "import pandas as _pd\n",
    "if 'prices_raw' not in globals() or prices_raw is None or (isinstance(prices_raw, _pd.DataFrame) and prices_raw.empty):\n",
    "    def chunk(lst: List[str], n: int):\n",
    "        for i in range(0, len(lst), n):\n",
    "            yield lst[i:i+n]\n",
    "\n",
    "    def download_prices(tickers: List[str], period='5y', interval='1d', chunk_size=100, max_retries=3, pause=1.0):\n",
    "        frames = []\n",
    "        for ch in tqdm(list(chunk(tickers, chunk_size))):\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    df = yf.download(ch, period=period, interval=interval, auto_adjust=False, threads=True)\n",
    "                    frames.append(df)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt == max_retries - 1:\n",
    "                        print(f\"Failed on chunk starting {ch[0]}: {e}\")\n",
    "                    else:\n",
    "                        time.sleep(pause * (attempt + 1))\n",
    "        if not frames:\n",
    "            return pd.DataFrame()\n",
    "        out = pd.concat(frames, axis=1)\n",
    "        # Ensure expected multi-index columns (PriceField, Ticker)\n",
    "        if not isinstance(out.columns, pd.MultiIndex):\n",
    "            # Single ticker case will produce flat columns; promote to MultiIndex\n",
    "            out.columns = pd.MultiIndex.from_product([out.columns, [tickers[0]]])\n",
    "        return out.sort_index()\n",
    "\n",
    "    # Use a manageable subset for first pass (e.g., first 300 tickers)\n",
    "    subset = tickers_clean['ticker'].tolist()[:300]\n",
    "    prices_raw = download_prices(subset, period='3y', interval='1d', chunk_size=100)\n",
    "    prices_raw.tail()\n",
    "else:\n",
    "    print('Skipping: prices_raw already loaded from cache (Bootstrap).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8298d111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: prices_raw already loaded from cache (Bootstrap).\n"
     ]
    }
   ],
   "source": [
    "import pandas as _pd\n",
    "if 'prices_raw' not in globals() or prices_raw is None or (isinstance(prices_raw, _pd.DataFrame) and prices_raw.empty):\n",
    "    tickers_all = tickers_clean['ticker'].tolist()\n",
    "\n",
    "    prices_raw = download_prices(\n",
    "        tickers_all,\n",
    "        period='3y',          # or '5y' / 'max'\n",
    "        interval='1d',\n",
    "        chunk_size=100,       # 50–150 is a good range\n",
    "        max_retries=3,\n",
    "        pause=1.5             # increase if you see throttling\n",
    "    )\n",
    "else:\n",
    "    print('Skipping: prices_raw already loaded from cache (Bootstrap).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1397b739",
   "metadata": {},
   "source": [
    "## 4) Tidy format & returns\n",
    "\n",
    "We pivot the wide OHLCV MultiIndex to a tidy DataFrame: one row per date‑ticker, with columns for price fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a8de18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_from_yf(wide: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Return a tidy frame with columns: date, ticker, [Adj Close, Close, High, Low, Open, Volume].\"\"\"\n",
    "    if wide.empty:\n",
    "        return pd.DataFrame(columns=['date','ticker','adj close','close','high','low','open','volume'])\n",
    "\n",
    "    df = wide.copy()\n",
    "\n",
    "    # Make sure the row index has a stable name we control\n",
    "    df.index.name = 'date'\n",
    "\n",
    "    # Stack the ticker level; keep price fields as columns\n",
    "    tidy = (\n",
    "        df\n",
    "        .stack(level=1, future_stack=True)\n",
    "        .rename_axis(index=['date', 'ticker'])   # <-- avoids the fragile 'level_0' rename\n",
    "        .reset_index()\n",
    "        .sort_values(['date', 'ticker'])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # normalize column names to lower case\n",
    "    tidy.columns = [c.lower() if isinstance(c, str) else c for c in tidy.columns]\n",
    "    return tidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49e68ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index name: Date\n",
      "column names: ['Price', 'Ticker']\n"
     ]
    }
   ],
   "source": [
    "print(\"index name:\", prices_raw.index.name)            # should be None or 'Date'\n",
    "print(\"column names:\", prices_raw.columns.names)       # expected ['Attributes','Symbols'] or similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4c636cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>adj close</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>AACB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>AACBR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>AACBU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>AACG</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>AACI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date ticker  adj close  close  high  low  open  volume\n",
       "0 2022-10-03   AACB        NaN    NaN   NaN  NaN   NaN     NaN\n",
       "1 2022-10-03  AACBR        NaN    NaN   NaN  NaN   NaN     NaN\n",
       "2 2022-10-03  AACBU        NaN    NaN   NaN  NaN   NaN     NaN\n",
       "3 2022-10-03   AACG       1.75   1.75  1.75  1.7   1.7  1600.0\n",
       "4 2022-10-03   AACI        NaN    NaN   NaN  NaN   NaN     NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>field</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>AACB</td>\n",
       "      <td>adj close</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>AACBR</td>\n",
       "      <td>adj close</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>AACBU</td>\n",
       "      <td>adj close</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>AACG</td>\n",
       "      <td>adj close</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>AACI</td>\n",
       "      <td>adj close</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date ticker      field  value\n",
       "0 2022-10-03   AACB  adj close    NaN\n",
       "1 2022-10-03  AACBR  adj close    NaN\n",
       "2 2022-10-03  AACBU  adj close    NaN\n",
       "3 2022-10-03   AACG  adj close   1.75\n",
       "4 2022-10-03   AACI  adj close    NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build tidy from the wide MultiIndex frame\n",
    "prices_tidy = tidy_from_yf(prices_raw)\n",
    "display(prices_tidy.head())\n",
    "\n",
    "# (Optional) ultra-tidy “long” version with one value column\n",
    "prices_long = prices_tidy.melt(\n",
    "    id_vars=['date', 'ticker'],\n",
    "    var_name='field',\n",
    "    value_name='value'\n",
    ")\n",
    "display(prices_long.head())\n",
    "\n",
    "prices_long = (\n",
    "    prices_tidy.melt(id_vars=['date','ticker'], var_name='field', value_name='value')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77b6650f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>adj close</th>\n",
       "      <th>ret</th>\n",
       "      <th>logret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3226230</th>\n",
       "      <td>2025-04-08</td>\n",
       "      <td>AACB</td>\n",
       "      <td>9.950</td>\n",
       "      <td>0.007085</td>\n",
       "      <td>0.007060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3231351</th>\n",
       "      <td>2025-04-09</td>\n",
       "      <td>AACB</td>\n",
       "      <td>9.900</td>\n",
       "      <td>-0.005025</td>\n",
       "      <td>-0.005038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3236472</th>\n",
       "      <td>2025-04-10</td>\n",
       "      <td>AACB</td>\n",
       "      <td>9.890</td>\n",
       "      <td>-0.001010</td>\n",
       "      <td>-0.001011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3241593</th>\n",
       "      <td>2025-04-11</td>\n",
       "      <td>AACB</td>\n",
       "      <td>9.885</td>\n",
       "      <td>-0.000506</td>\n",
       "      <td>-0.000506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3246714</th>\n",
       "      <td>2025-04-14</td>\n",
       "      <td>AACB</td>\n",
       "      <td>9.900</td>\n",
       "      <td>0.001517</td>\n",
       "      <td>0.001516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              date ticker  adj close       ret    logret\n",
       "3226230 2025-04-08   AACB      9.950  0.007085  0.007060\n",
       "3231351 2025-04-09   AACB      9.900 -0.005025 -0.005038\n",
       "3236472 2025-04-10   AACB      9.890 -0.001010 -0.001011\n",
       "3241593 2025-04-11   AACB      9.885 -0.000506 -0.000506\n",
       "3246714 2025-04-14   AACB      9.900  0.001517  0.001516"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Daily returns using adj close from the tidy table\n",
    "adj = (\n",
    "    prices_tidy[['date', 'ticker', 'adj close']]\n",
    "    .dropna()\n",
    "    .sort_values(['ticker', 'date'])\n",
    ")\n",
    "\n",
    "adj['ret']    = adj.groupby('ticker')['adj close'].pct_change()\n",
    "adj['logret'] = np.log(adj['adj close'] / adj.groupby('ticker')['adj close'].shift(1))\n",
    "\n",
    "returns = adj.dropna(subset=['ret', 'logret'])\n",
    "display(returns.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "89a84c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index name: Date\n",
      "column names: ['Price', 'Ticker']\n",
      "(3845871, 8) (23075226, 4) (2875035, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"index name:\", prices_raw.index.name)          # should be 'Date' (now handled)\n",
    "print(\"column names:\", prices_raw.columns.names)     # ['Price','Ticker'] is perfect\n",
    "print(prices_tidy.shape, prices_long.shape, returns.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114f01b8",
   "metadata": {},
   "source": [
    "### Daily returns (Adj Close)\n",
    "Compute simple and log returns, dropping thin/empty tickers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67dc7577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yx/5t5dfjxj4pv8tjn2zvbfc2540000gn/T/ipykernel_4700/2680132490.py:8: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  rets = adj.pct_change().rename_axis(index='date')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>0</th>\n",
       "      <th>logret</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>AACG</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.039221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>AADR</td>\n",
       "      <td>0.025720</td>\n",
       "      <td>0.039221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>AAL</td>\n",
       "      <td>0.086409</td>\n",
       "      <td>0.039221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>AAME</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>AAOI</td>\n",
       "      <td>0.069091</td>\n",
       "      <td>0.039221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date Ticker         0    logret\n",
       "0 2022-10-04   AACG  0.040000  0.039221\n",
       "1 2022-10-04   AADR  0.025720  0.039221\n",
       "2 2022-10-04    AAL  0.086409  0.039221\n",
       "3 2022-10-04   AAME  0.000000  0.039221\n",
       "4 2022-10-04   AAOI  0.069091  0.039221"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_returns(wide: pd.DataFrame) -> pd.DataFrame:\n",
    "    if wide.empty:\n",
    "        return pd.DataFrame()\n",
    "    adj = wide['Adj Close'].copy()\n",
    "    # Drop columns with too many NaNs\n",
    "    keep = adj.columns[adj.notna().sum() > 50]\n",
    "    adj = adj[keep]\n",
    "    rets = adj.pct_change().rename_axis(index='date')\n",
    "    logrets = np.log(adj/adj.shift(1))\n",
    "    out = rets.stack().reset_index().rename(columns={'Adj Close':'ret','level_1':'ticker'})\n",
    "    out['logret'] = logrets.stack().reset_index(drop=True)[0]\n",
    "    return out\n",
    "\n",
    "returns = compute_returns(prices_raw)\n",
    "returns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fe6e18",
   "metadata": {},
   "source": [
    "## 5) Coverage report & quick EDA\n",
    "\n",
    "A few quick summaries to sanity‑check the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7573771c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tickers_requested</th>\n",
       "      <th>n_rows_prices_wide</th>\n",
       "      <th>n_fields</th>\n",
       "      <th>n_tickers_downloaded</th>\n",
       "      <th>coverage_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5118</td>\n",
       "      <td>751</td>\n",
       "      <td>6</td>\n",
       "      <td>5121</td>\n",
       "      <td>100.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_tickers_requested  n_rows_prices_wide  n_fields  n_tickers_downloaded  coverage_pct\n",
       "0                 5118                 751         6                  5121        100.06"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# figure out fields & tickers present in prices_raw\n",
    "if isinstance(prices_raw.columns, pd.MultiIndex):\n",
    "    lvl0 = prices_raw.columns.get_level_values(0)\n",
    "    lvl1 = prices_raw.columns.get_level_values(1)\n",
    "    n_fields = int(pd.unique(lvl0).size)\n",
    "    n_tickers_downloaded = int(pd.unique(lvl1).size)\n",
    "else:\n",
    "    n_fields = 1\n",
    "    n_tickers_downloaded = int(prices_raw.shape[1])\n",
    "\n",
    "# how many tickers did we intend to cover?\n",
    "n_requested = (\n",
    "    len(active_tickers) if 'active_tickers' in globals() and active_tickers\n",
    "    else (len(tickers_clean) if 'tickers_clean' in globals() else n_tickers_downloaded)\n",
    ")\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    'n_tickers_requested': int(n_requested),\n",
    "    'n_rows_prices_wide': int(prices_raw.shape[0]),\n",
    "    'n_fields': n_fields,\n",
    "    'n_tickers_downloaded': n_tickers_downloaded,\n",
    "    'coverage_pct': round(100 * n_tickers_downloaded / max(1, n_requested), 2),\n",
    "}])\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f22bce86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tickers_with_fundamentals</th>\n",
       "      <th>pct_meta_vs_prices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5118</td>\n",
       "      <td>0.999414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tickers_with_fundamentals  pct_meta_vs_prices\n",
       "0                       5118            0.999414"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fund_cov = pd.DataFrame([{\n",
    "    'tickers_with_fundamentals': int(fundamentals['ticker'].nunique()) if 'fundamentals' in globals() else 0,\n",
    "    'pct_meta_vs_prices': (\n",
    "        (fundamentals['ticker'].nunique() / n_tickers_downloaded)\n",
    "        if 'fundamentals' in globals() and n_tickers_downloaded else None\n",
    "    ),\n",
    "}])\n",
    "fund_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aaf3a1",
   "metadata": {},
   "source": [
    "### Top/bottom average returns (last 1 year)\n",
    "This is a quick sanity check, not investment advice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc0c4255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logret_1y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FWONA</th>\n",
       "      <td>9.844406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FWONK</th>\n",
       "      <td>9.844406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FWRD</th>\n",
       "      <td>9.844406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FWRG</th>\n",
       "      <td>9.844406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FXNC</th>\n",
       "      <td>9.844406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FYBR</th>\n",
       "      <td>9.844406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FYC</th>\n",
       "      <td>9.844406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FYT</th>\n",
       "      <td>9.844406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FUND</th>\n",
       "      <td>9.844406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZYXI</th>\n",
       "      <td>9.844406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        logret_1y\n",
       "ticker           \n",
       "FWONA    9.844406\n",
       "FWONK    9.844406\n",
       "FWRD     9.844406\n",
       "FWRG     9.844406\n",
       "FXNC     9.844406\n",
       "FYBR     9.844406\n",
       "FYC      9.844406\n",
       "FYT      9.844406\n",
       "FUND     9.844406\n",
       "ZYXI     9.844406"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logret_1y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>WTG</th>\n",
       "      <td>1.961037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KCHV</th>\n",
       "      <td>1.961037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MJID</th>\n",
       "      <td>2.000258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAWR</th>\n",
       "      <td>2.039479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KMRK</th>\n",
       "      <td>2.078699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOCAU</th>\n",
       "      <td>2.078699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUGO</th>\n",
       "      <td>2.078699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MGRT</th>\n",
       "      <td>2.078699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PDDL</th>\n",
       "      <td>2.117920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OTGL</th>\n",
       "      <td>2.117920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        logret_1y\n",
       "ticker           \n",
       "WTG      1.961037\n",
       "KCHV     1.961037\n",
       "MJID     2.000258\n",
       "LAWR     2.039479\n",
       "KMRK     2.078699\n",
       "SOCAU    2.078699\n",
       "AUGO     2.078699\n",
       "MGRT     2.078699\n",
       "PDDL     2.117920\n",
       "OTGL     2.117920"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- make sure returns has the columns we expect ---\n",
    "def ensure_returns_schema(df):\n",
    "    out = df.copy()\n",
    "    # bring index levels out if needed\n",
    "    if 'ticker' not in out.columns and (out.index.names and 'ticker' in [str(n).lower() for n in out.index.names]):\n",
    "        out = out.reset_index()\n",
    "    # normalize/rename common variants\n",
    "    ren = {}\n",
    "    for c in out.columns:\n",
    "        cl = str(c).lower()\n",
    "        if cl in ('symbol','symbols','level_1'): ren[c] = 'ticker'\n",
    "        if cl in ('date','datetime'):            ren[c] = 'date'\n",
    "    if ren:\n",
    "        out = out.rename(columns=ren)\n",
    "    out.columns = [str(c).lower() for c in out.columns]\n",
    "    out['date'] = pd.to_datetime(out['date'])\n",
    "    # if logret missing, compute from adj close\n",
    "    if 'logret' not in out.columns and {'adj close','ticker','date'}.issubset(out.columns):\n",
    "        out = out.sort_values(['ticker','date'])\n",
    "        out['logret'] = np.log(out.groupby('ticker')['adj close'].apply(lambda s: s / s.shift(1))).reset_index(level=0, drop=True)\n",
    "    return out\n",
    "\n",
    "returns_ = ensure_returns_schema(returns)\n",
    "\n",
    "# --- 1-year perf (top/bottom) ---\n",
    "cutoff = returns_['date'].max() - pd.Timedelta(days=365)\n",
    "lastyr = returns_[returns_['date'] >= cutoff]\n",
    "\n",
    "perf = (lastyr\n",
    "        .groupby('ticker', as_index=False)['logret']\n",
    "        .sum()\n",
    "        .sort_values('logret'))\n",
    "\n",
    "top10    = perf.tail(10).set_index('ticker').rename(columns={'logret':'logret_1y'})\n",
    "bottom10 = perf.head(10).set_index('ticker').rename(columns={'logret':'logret_1y'})\n",
    "\n",
    "display(top10)\n",
    "display(bottom10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "82a321b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'Ticker', 0, 'logret'], dtype='object')\n",
      "[None]\n"
     ]
    }
   ],
   "source": [
    "print(returns.columns)       # columns present\n",
    "print(returns.index.names)   # index levels (is 'ticker' hiding here?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb0a3be",
   "metadata": {},
   "source": [
    "## 6) (Optional) Add market caps\n",
    "You can pull market caps for many tickers via `yahoo_fin.get_quote_table` or `si.get_market_cap`. This is slower and may miss some tickers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16e1fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_market_caps(tickers: List[str]) -> pd.DataFrame:\n",
    "    if not YAHOO_FIN_OK:\n",
    "        print(\"yahoo_fin not available; skipping market caps.\")\n",
    "        return pd.DataFrame(columns=['ticker','market_cap'])\n",
    "    caps = []\n",
    "    for t in tqdm(tickers):\n",
    "        try:\n",
    "            cap = si.get_market_cap(t)\n",
    "            caps.append((t, cap))\n",
    "        except Exception:\n",
    "            caps.append((t, np.nan))\n",
    "    return pd.DataFrame(caps, columns=['ticker','market_cap'])\n",
    "\n",
    "# Example (small subset to keep it quick):\n",
    "# caps = get_market_caps(subset[:50])\n",
    "# caps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a098980d",
   "metadata": {},
   "source": [
    "## 7) Save outputs\n",
    "Persist your data for future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c0e176d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved available outputs in /Users/ozkangelincik/Downloads/EDA yfinance in Python/outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/io/parquet.py:190: UserWarning: The DataFrame has column names of mixed type. They will be converted to strings and not roundtrip correctly.\n",
      "  table = self.api.Table.from_pandas(df, **from_pandas_kwargs)\n"
     ]
    }
   ],
   "source": [
    "# utils for saving\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "OUTPUT_DIR = Path(\"outputs\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) tickers\n",
    "tickers_clean.to_csv(OUTPUT_DIR / \"tickers_clean.csv\", index=False)\n",
    "\n",
    "# 2) prices (wide)\n",
    "if \"prices_raw\" in globals() and not prices_raw.empty:\n",
    "    # always succeeds\n",
    "    prices_raw.to_pickle(OUTPUT_DIR / \"prices_wide.pkl\")\n",
    "    # parquet if available\n",
    "    try:\n",
    "        prices_raw.to_parquet(OUTPUT_DIR / \"prices_wide.parquet\")\n",
    "    except Exception as e:\n",
    "        print(\"Skipped prices_wide.parquet:\", e)\n",
    "\n",
    "# 3) prices (tidy)\n",
    "if \"prices_tidy\" in globals() and not prices_tidy.empty:\n",
    "    prices_tidy.to_pickle(OUTPUT_DIR / \"prices_tidy.pkl\")\n",
    "    try:\n",
    "        prices_tidy.to_parquet(OUTPUT_DIR / \"prices_tidy.parquet\")\n",
    "    except Exception as e:\n",
    "        print(\"Skipped prices_tidy.parquet:\", e)\n",
    "\n",
    "# 4) returns\n",
    "if \"returns\" in globals() and not returns.empty:\n",
    "    returns.to_pickle(OUTPUT_DIR / \"returns.pkl\")\n",
    "    try:\n",
    "        returns.to_parquet(OUTPUT_DIR / \"returns.parquet\")\n",
    "    except Exception as e:\n",
    "        print(\"Skipped returns.parquet:\", e)\n",
    "\n",
    "print(\"Saved available outputs in\", OUTPUT_DIR.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0d1082",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- Join market caps/industries and slice performance by size/sector\n",
    "- Compute rolling volatility, drawdowns, and momentum signals\n",
    "- Detect outliers (suspicious zero volumes, stale prices)\n",
    "- Visualize top sectors and factor tilts over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0ba61b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>adj close</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>AACB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>AACBR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>AACBU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>AACG</td>\n",
       "      <td>1.750</td>\n",
       "      <td>1.750</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1.700</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>1600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>AACI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845866</th>\n",
       "      <td>2025-09-30</td>\n",
       "      <td>ZXZZT</td>\n",
       "      <td>12.175</td>\n",
       "      <td>12.175</td>\n",
       "      <td>12.175000</td>\n",
       "      <td>12.110</td>\n",
       "      <td>12.110000</td>\n",
       "      <td>5222.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845867</th>\n",
       "      <td>2025-09-30</td>\n",
       "      <td>ZYBT</td>\n",
       "      <td>2.620</td>\n",
       "      <td>2.620</td>\n",
       "      <td>4.380000</td>\n",
       "      <td>2.133</td>\n",
       "      <td>2.319000</td>\n",
       "      <td>11202500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845868</th>\n",
       "      <td>2025-09-30</td>\n",
       "      <td>ZYME</td>\n",
       "      <td>17.080</td>\n",
       "      <td>17.080</td>\n",
       "      <td>17.389999</td>\n",
       "      <td>16.875</td>\n",
       "      <td>17.190001</td>\n",
       "      <td>653100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845869</th>\n",
       "      <td>2025-09-30</td>\n",
       "      <td>ZYN</td>\n",
       "      <td>19.517</td>\n",
       "      <td>19.517</td>\n",
       "      <td>19.719000</td>\n",
       "      <td>19.517</td>\n",
       "      <td>19.719000</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845870</th>\n",
       "      <td>2025-09-30</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>1.450</td>\n",
       "      <td>1.450</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>1.450</td>\n",
       "      <td>1.460000</td>\n",
       "      <td>48200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3845871 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date ticker  adj close   close       high     low       open      volume\n",
       "0       2022-10-03   AACB        NaN     NaN        NaN     NaN        NaN         NaN\n",
       "1       2022-10-03  AACBR        NaN     NaN        NaN     NaN        NaN         NaN\n",
       "2       2022-10-03  AACBU        NaN     NaN        NaN     NaN        NaN         NaN\n",
       "3       2022-10-03   AACG      1.750   1.750   1.750000   1.700   1.700000      1600.0\n",
       "4       2022-10-03   AACI        NaN     NaN        NaN     NaN        NaN         NaN\n",
       "...            ...    ...        ...     ...        ...     ...        ...         ...\n",
       "3845866 2025-09-30  ZXZZT     12.175  12.175  12.175000  12.110  12.110000      5222.0\n",
       "3845867 2025-09-30   ZYBT      2.620   2.620   4.380000   2.133   2.319000  11202500.0\n",
       "3845868 2025-09-30   ZYME     17.080  17.080  17.389999  16.875  17.190001    653100.0\n",
       "3845869 2025-09-30    ZYN     19.517  19.517  19.719000  19.517  19.719000       200.0\n",
       "3845870 2025-09-30   ZYXI      1.450   1.450   1.480000   1.450   1.460000     48200.0\n",
       "\n",
       "[3845871 rows x 8 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_tidy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "80a17981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, requests, pandas as pd, numpy as np\n",
    "import yfinance as yf\n",
    "\n",
    "try:\n",
    "    from yahoo_fin import stock_info as si\n",
    "    YAHOO_FIN_OK = True\n",
    "except Exception:\n",
    "    YAHOO_FIN_OK = False\n",
    "\n",
    "# <-- Put your email here; the SEC requires a descriptive User-Agent with contact info\n",
    "SEC_HEADERS = {\"User-Agent\": \"Ozkan Gelincik ozkangelincik@example.com\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3aef7884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_yahoo_meta(ticker: str):\n",
    "    mcap = sector = industry = None\n",
    "\n",
    "    # 1) yahoo_fin (fast & simple when available)\n",
    "    if YAHOO_FIN_OK:\n",
    "        try:\n",
    "            qd = si.get_quote_data(ticker)             # dict\n",
    "            mcap = qd.get(\"marketCap\", mcap)\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            info_df = si.get_company_info(ticker)      # DataFrame with index 'sector','industry'\n",
    "            if isinstance(info_df, pd.DataFrame) and \"Value\" in info_df.columns:\n",
    "                if \"sector\" in info_df.index:   sector   = info_df.loc[\"sector\", \"Value\"]\n",
    "                if \"industry\" in info_df.index: industry = info_df.loc[\"industry\", \"Value\"]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 2) yfinance fallback\n",
    "    try:\n",
    "        t = yf.Ticker(ticker)\n",
    "        if mcap is None:\n",
    "            fi = getattr(t, \"fast_info\", None)\n",
    "            if fi is not None:\n",
    "                mcap = getattr(fi, \"market_cap\", mcap)\n",
    "        if sector is None or industry is None:\n",
    "            # .info can be slower but fills sector/industry for many names\n",
    "            info = t.info\n",
    "            sector   = sector   or info.get(\"sector\")\n",
    "            industry = industry or info.get(\"industry\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return {\"ticker\": ticker, \"market_cap\": mcap, \"sector\": sector, \"industry\": industry}\n",
    "\n",
    "def get_yahoo_meta_batch(tickers, pause=0.05):\n",
    "    rows = []\n",
    "    for t in tickers:\n",
    "        rows.append(fetch_yahoo_meta(t))\n",
    "        time.sleep(pause)     # be polite; avoid throttling\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "becfdc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install yahooquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e7201845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: prices_raw already loaded from cache (Bootstrap).\n"
     ]
    }
   ],
   "source": [
    "import pandas as _pd\n",
    "if 'prices_raw' not in globals() or prices_raw is None or (isinstance(prices_raw, _pd.DataFrame) and prices_raw.empty):\n",
    "    from pathlib import Path\n",
    "    import pandas as pd\n",
    "\n",
    "    OUTPUT_DIR = Path(\"outputs\")\n",
    "\n",
    "    prices_raw = None\n",
    "\n",
    "    # Try single-file saves first\n",
    "    for p in [OUTPUT_DIR/\"prices_wide.parquet\", OUTPUT_DIR/\"prices_wide.pkl\"]:\n",
    "        if p.exists():\n",
    "            prices_raw = (pd.read_parquet(p) if p.suffix==\".parquet\" else pd.read_pickle(p))\n",
    "            break\n",
    "\n",
    "    # If not found, try reassembling from chunked downloads\n",
    "    if prices_raw is None:\n",
    "        chunk_dir = OUTPUT_DIR / \"chunks\"\n",
    "        parts = sorted(chunk_dir.glob(\"prices_wide_*.parquet\"))\n",
    "        if parts:\n",
    "            prices_raw = pd.concat([pd.read_parquet(p) for p in parts], axis=1).sort_index()\n",
    "\n",
    "    if prices_raw is None:\n",
    "        raise RuntimeError(\"Couldn't find saved prices. If you didn't save before, re-run the chunked downloader.\")\n",
    "    \n",
    "    # sanity checks\n",
    "    print(\"prices_raw shape:\", getattr(prices_raw, \"shape\", None))\n",
    "    print(\"column names:\", getattr(prices_raw, \"columns\", None).names if hasattr(prices_raw, \"columns\") else None)\n",
    "else:\n",
    "    print('Skipping: prices_raw already loaded from cache (Bootstrap).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f78ab0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(active_tickers) = 5118\n",
      "sample: ['AACB', 'AACBR', 'AACBU', 'AACG', 'AACI']\n",
      "types: <class 'dict'> <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(\"len(active_tickers) =\", len(active_tickers))\n",
    "print(\"sample:\", active_tickers[:5])\n",
    "from yahooquery import Ticker\n",
    "tq = Ticker(active_tickers[:3], asynchronous=False)\n",
    "print(\"types:\", type(tq.price), type(tq.summary_profile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "026df211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pass 1: chunks=200, pause=0.6s, retries=1, async=True, workers=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pass 1: 100%|██████████| 26/26 [00:00<00:00, 4311.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  collected this pass: 0 | remaining: 5118\n",
      "\n",
      "Pass 2: chunks=80, pause=0.8s, retries=2, async=True, workers=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pass 2: 100%|██████████| 64/64 [00:00<00:00, 6719.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  collected this pass: 0 | remaining: 5118\n",
      "\n",
      "Pass 3: chunks=20, pause=1.0s, retries=2, async=False, workers=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pass 3: 100%|██████████| 256/256 [00:00<00:00, 7157.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  collected this pass: 338 | remaining: 4780\n",
      "meta_yq rows: 338 | unique tickers: 338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>market_cap</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASTL</td>\n",
       "      <td>4.146867e+08</td>\n",
       "      <td>Basic Materials</td>\n",
       "      <td>Steel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASTLW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Basic Materials</td>\n",
       "      <td>Steel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASTS</td>\n",
       "      <td>1.773648e+10</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Communication Equipment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASUR</td>\n",
       "      <td>2.248851e+08</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Software - Application</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASYS</td>\n",
       "      <td>1.351238e+08</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Semiconductor Equipment &amp; Materials</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker    market_cap           sector                             industry\n",
       "0   ASTL  4.146867e+08  Basic Materials                                Steel\n",
       "1  ASTLW           NaN  Basic Materials                                Steel\n",
       "2   ASTS  1.773648e+10       Technology              Communication Equipment\n",
       "3   ASUR  2.248851e+08       Technology               Software - Application\n",
       "4   ASYS  1.351238e+08       Technology  Semiconductor Equipment & Materials"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time, json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from yahooquery import Ticker\n",
    "from tqdm import tqdm\n",
    "\n",
    "SYMS_UP = [s.upper() for s in active_tickers]\n",
    "CACHE = Path(\"outputs/yq_cache\"); CACHE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _mods_from_entry(entry):\n",
    "    # normalize yahooquery entry → modules dict\n",
    "    if isinstance(entry, dict):\n",
    "        qs = entry.get('quoteSummary')\n",
    "        if isinstance(qs, dict):\n",
    "            res = qs.get('result')\n",
    "            if isinstance(res, list) and res and isinstance(res[0], dict):\n",
    "                return res[0]\n",
    "        return entry\n",
    "    return {}\n",
    "\n",
    "def _fetch_batch(symbols, asynchronous=True, max_workers=8, formatted=False):\n",
    "    tq = Ticker(symbols, asynchronous=asynchronous, max_workers=max_workers, formatted=formatted)\n",
    "    data = tq.get_modules('price,assetProfile,summaryProfile')  # dict keyed by symbol; may contain 'error'\n",
    "    if not isinstance(data, dict):\n",
    "        return {}\n",
    "    return data\n",
    "\n",
    "def _rows_from_data(data):\n",
    "    rows = []\n",
    "    for sym, entry in (data or {}).items():\n",
    "        if not sym or str(sym).lower() == 'error':   # drop junk\n",
    "            continue\n",
    "        mods = _mods_from_entry(entry)\n",
    "        if not isinstance(mods, dict) or not mods:\n",
    "            continue\n",
    "        price   = mods.get('price') or {}\n",
    "        profile = mods.get('assetProfile') or mods.get('summaryProfile') or {}\n",
    "        ticker  = (price.get('symbol') if isinstance(price, dict) else None) or str(sym)\n",
    "        rows.append({\n",
    "            'ticker'    : str(ticker).upper(),\n",
    "            'market_cap': price.get('marketCap'),\n",
    "            'sector'    : profile.get('sector'),\n",
    "            'industry'  : profile.get('industry'),\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "def fetch_meta_multi_pass(symbols, passes=None):\n",
    "    \"\"\"Try larger → smaller chunks with growing pause; cache raw responses per pass/chunk; resume on rerun.\"\"\"\n",
    "    if passes is None:\n",
    "        passes = [\n",
    "            dict(chunk_size=200, pause=0.6, retries=1, async_=True,  workers=8),\n",
    "            dict(chunk_size=80,  pause=0.8, retries=2, async_=True,  workers=6),\n",
    "            dict(chunk_size=20,  pause=1.0, retries=2, async_=False, workers=4),  # conservative final pass\n",
    "        ]\n",
    "\n",
    "    remaining = [s for s in symbols]\n",
    "    all_rows  = []\n",
    "\n",
    "    for pi, p in enumerate(passes, start=1):\n",
    "        if not remaining:\n",
    "            break\n",
    "        chunk_size = p['chunk_size']; pause = p['pause']; retries = p['retries']\n",
    "        async_ = p['async_']; workers = p['workers']\n",
    "\n",
    "        print(f\"\\nPass {pi}: chunks={chunk_size}, pause={pause}s, retries={retries}, async={async_}, workers={workers}\")\n",
    "        got_this_pass = set()\n",
    "\n",
    "        for i in tqdm(range(0, len(remaining), chunk_size), desc=f\"pass {pi}\"):\n",
    "            batch = remaining[i:i+chunk_size]\n",
    "            key = f\"p{pi}_i{i:05d}_{len(batch)}.json\"\n",
    "            fp  = CACHE / key\n",
    "\n",
    "            if fp.exists():\n",
    "                data = json.loads(fp.read_text())\n",
    "            else:\n",
    "                last_e = None; data = {}\n",
    "                for a in range(retries + 1):\n",
    "                    try:\n",
    "                        data = _fetch_batch(batch, asynchronous=async_, max_workers=workers, formatted=False)\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        last_e = e; time.sleep(pause*(a+1))\n",
    "                if not isinstance(data, dict):\n",
    "                    data = {}\n",
    "                # cache raw\n",
    "                try:\n",
    "                    fp.write_text(json.dumps(data))\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                time.sleep(pause)  # be polite between batches\n",
    "\n",
    "            rows = _rows_from_data(data)\n",
    "            all_rows.extend(rows)\n",
    "            got_this_pass.update(r['ticker'] for r in rows if r.get('ticker'))\n",
    "\n",
    "        # prepare next pass on the ones still missing\n",
    "        got = got_this_pass\n",
    "        remaining = [s for s in remaining if s.upper() not in got]\n",
    "        print(f\"  collected this pass: {len(got_this_pass)} | remaining: {len(remaining)}\")\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    if not df.empty:\n",
    "        df = (df[df['ticker'].isin([s.upper() for s in symbols])]\n",
    "                .drop_duplicates('ticker', keep='first'))\n",
    "    return df\n",
    "\n",
    "meta_yq = fetch_meta_multi_pass(SYMS_UP)\n",
    "\n",
    "print(\"meta_yq rows:\", len(meta_yq), \"| unique tickers:\", meta_yq['ticker'].nunique())\n",
    "display(meta_yq.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "58e95cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already have: 338 | remaining: 4780\n",
      "['AACB', 'AACBR', 'AACBU', 'AACG', 'AACI']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# map UPPER -> original for stable requests\n",
    "upper_to_orig = {s.upper(): s for s in active_tickers}\n",
    "\n",
    "have_up = set(meta_yq['ticker'].str.upper())\n",
    "remaining_up = [u for u in upper_to_orig if u not in have_up]\n",
    "remaining_orig = [upper_to_orig[u] for u in remaining_up]\n",
    "\n",
    "print(\"already have:\", len(have_up), \"| remaining:\", len(remaining_orig))\n",
    "print(remaining_orig[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7483ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from yahooquery import Ticker\n",
    "\n",
    "def _rows_from_obj(obj, fields, prefer_col_symbol=True):\n",
    "    \"\"\"\n",
    "    Robustly convert yahooquery output (dict | DataFrame | other) into a DataFrame\n",
    "    with columns: ['ticker'] + fields (always present, even if empty).\n",
    "    Skips entries that aren't dict-like.\n",
    "    \"\"\"\n",
    "    cols = ['ticker'] + list(fields)\n",
    "    rows = []\n",
    "\n",
    "    if isinstance(obj, pd.DataFrame):\n",
    "        df = obj.copy()\n",
    "        # Use a 'symbol' column if present; otherwise use the index\n",
    "        if prefer_col_symbol and 'symbol' in df.columns:\n",
    "            take = ['symbol'] + [c for c in fields if c in df.columns]\n",
    "            for r in df[take].to_dict('records'):\n",
    "                sym = str(r.pop('symbol')).upper()\n",
    "                row = {'ticker': sym}\n",
    "                for f in fields: row[f] = r.get(f)\n",
    "                rows.append(row)\n",
    "        else:\n",
    "            have = [c for c in fields if c in df.columns]\n",
    "            for sym, r in df[have].iterrows():\n",
    "                row = {'ticker': str(sym).upper()}\n",
    "                for f in fields: row[f] = r.get(f) if f in r else None\n",
    "                rows.append(row)\n",
    "        return pd.DataFrame(rows, columns=cols)\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        for sym, val in obj.items():\n",
    "            if not isinstance(val, dict):\n",
    "                # sometimes 'error' or a string -> skip\n",
    "                continue\n",
    "            row = {'ticker': str(sym).upper()}\n",
    "            for f in fields: row[f] = val.get(f)\n",
    "            rows.append(row)\n",
    "        return pd.DataFrame(rows, columns=cols)\n",
    "\n",
    "    # anything else -> empty with expected columns\n",
    "    return pd.DataFrame(columns=cols)\n",
    "\n",
    "def fetch_meta_simple_safe(symbols, chunk_size=120, pause=0.8, async_=True, workers=6):\n",
    "    \"\"\"Chunked yahooquery pull that tolerates mixed shapes and guarantees 'ticker' exists.\"\"\"\n",
    "    frames = []\n",
    "    for i in range(0, len(symbols), chunk_size):\n",
    "        batch = symbols[i:i+chunk_size]  # ORIGINAL CASE for requests\n",
    "        tq = Ticker(batch, asynchronous=async_, max_workers=workers)\n",
    "\n",
    "        # price -> marketCap\n",
    "        price_df = _rows_from_obj(tq.price, ['marketCap'])\n",
    "        if 'marketCap' in price_df.columns:\n",
    "            price_df = price_df.rename(columns={'marketCap': 'market_cap'})\n",
    "        if 'market_cap' not in price_df.columns:\n",
    "            price_df['market_cap'] = pd.NA\n",
    "        price_df = price_df[['ticker','market_cap']]\n",
    "\n",
    "        # summary_profile -> sector, industry\n",
    "        prof_df = _rows_from_obj(tq.summary_profile, ['sector', 'industry'])\n",
    "        for c in ('sector','industry'):\n",
    "            if c not in prof_df.columns:\n",
    "                prof_df[c] = pd.NA\n",
    "        prof_df = prof_df[['ticker','sector','industry']]\n",
    "\n",
    "        # safe outer-join; both sides now guaranteed to have 'ticker'\n",
    "        chunk_df = (price_df.merge(prof_df, on='ticker', how='outer')\n",
    "                               .drop_duplicates('ticker'))\n",
    "\n",
    "        if not chunk_df.empty:\n",
    "            want = {s.upper() for s in batch}\n",
    "            chunk_df = chunk_df[chunk_df['ticker'].isin(want)]\n",
    "            frames.append(chunk_df)\n",
    "\n",
    "        time.sleep(pause)  # be polite\n",
    "\n",
    "    if not frames:\n",
    "        return pd.DataFrame(columns=['ticker','market_cap','sector','industry'])\n",
    "\n",
    "    out = (pd.concat(frames, ignore_index=True)\n",
    "             .drop_duplicates('ticker', keep='first'))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a122eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# empty-frame factory for convenience\n",
    "def _empty_meta():\n",
    "    return pd.DataFrame(columns=['ticker','market_cap','sector','industry'])\n",
    "\n",
    "def _safe_concat(a, b):\n",
    "    if a is None or len(a)==0: return b if b is not None else _empty_meta()\n",
    "    if b is None or len(b)==0: return a\n",
    "    return (pd.concat([a, b], ignore_index=True)\n",
    "              .drop_duplicates('ticker', keep='first'))\n",
    "\n",
    "def try_batch_resilient(batch, async_fast=True, workers_fast=6, subsize=10, workers_slow=2):\n",
    "    \"\"\"\n",
    "    Try one batch fast; on timeout, fall back to smaller, synchronous chunks.\n",
    "    Returns a meta DataFrame (may be empty).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # fast path (your tolerant fetcher)\n",
    "        got = fetch_meta_simple_safe(batch, chunk_size=len(batch), pause=0.0, async_=async_fast, workers=workers_fast)\n",
    "        return got\n",
    "    except Exception as e:\n",
    "        msg = str(e)\n",
    "        print(f\"[warn] batch timeout/err ({len(batch)}): {msg[:120]} ... -> splitting to {subsize}\")\n",
    "        # slow, reliable path: split into tiny sub-batches synchronously\n",
    "        out = _empty_meta()\n",
    "        for j in range(0, len(batch), subsize):\n",
    "            sub = batch[j:j+subsize]\n",
    "            try:\n",
    "                got = fetch_meta_simple_safe(sub, chunk_size=len(sub), pause=0.0, async_=False, workers=workers_slow)\n",
    "                out = _safe_concat(out, got)\n",
    "            except Exception as e2:\n",
    "                print(f\"[skip] sub-batch failed ({sub[:1]}…): {e2}\")\n",
    "        return out\n",
    "\n",
    "def fill_remaining_with_retries(remain_symbols,\n",
    "                                chunk_size=120,\n",
    "                                pause=0.8,\n",
    "                                async_fast=True,\n",
    "                                workers_fast=6,\n",
    "                                subsize_on_timeout=10,\n",
    "                                workers_slow=2,\n",
    "                                save_every=5,\n",
    "                                progress_path=Path(\"outputs/meta_yq_progress.parquet\")):\n",
    "    \"\"\"\n",
    "    Processes remain_symbols in chunks, resilient to timeouts.\n",
    "    Saves progress periodically to progress_path (parquet).\n",
    "    \"\"\"\n",
    "    progress_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    collected = _empty_meta()\n",
    "    batches_done = 0\n",
    "    for i in range(0, len(remain_symbols), chunk_size):\n",
    "        batch = remain_symbols[i:i+chunk_size]\n",
    "        got = try_batch_resilient(batch,\n",
    "                                  async_fast=async_fast, workers_fast=workers_fast,\n",
    "                                  subsize=subsize_on_timeout, workers_slow=workers_slow)\n",
    "        if not got.empty:\n",
    "            collected = _safe_concat(collected, got)\n",
    "        batches_done += 1\n",
    "        if batches_done % save_every == 0:\n",
    "            try:\n",
    "                collected.to_parquet(progress_path)\n",
    "                print(f\"[saved] {len(collected)} rows to {progress_path}\")\n",
    "            except Exception as e:\n",
    "                print(\"[warn] save failed:\", e)\n",
    "        time.sleep(pause)\n",
    "    # final save\n",
    "    if len(collected):\n",
    "        try:\n",
    "            collected.to_parquet(progress_path)\n",
    "            print(f\"[saved-final] {len(collected)} rows to {progress_path}\")\n",
    "        except Exception as e:\n",
    "            print(\"[warn] final save failed:\", e)\n",
    "    return collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fb4ad240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only backfill yahooquery metadata if anything is actually missing\n",
    "# from pathlib import Path\n",
    "# import pandas as pd\n",
    "\n",
    "# # make sure meta_yq exists (load from disk if needed)\n",
    "# if 'meta_yq' not in globals() or meta_yq is None or meta_yq.empty:\n",
    "#     p = Path(\"outputs/meta_yq.parquet\")\n",
    "#     meta_yq = pd.read_parquet(p) if p.exists() else pd.DataFrame(columns=['ticker','market_cap','sector','industry'])\n",
    "\n",
    "# filled = meta_yq.copy()\n",
    "\n",
    "# # compute remaining in a case-robust way\n",
    "# upper_to_orig = {s.upper(): s for s in active_tickers}\n",
    "# have_up = set(filled['ticker'].astype(str).str.upper())\n",
    "# remaining_up = [u for u in upper_to_orig if u not in have_up]\n",
    "# remaining_orig = [upper_to_orig[u] for u in remaining_up]\n",
    "\n",
    "# print(\"have:\", len(have_up), \"| remaining:\", len(remaining_orig))\n",
    "\n",
    "# # hard stop if nothing to do\n",
    "# if not remaining_orig:\n",
    "#     print(\"All covered — skipping yahooquery backfill.\")\n",
    "# else:\n",
    "#     # OPTIONAL toggle to avoid accidental long runs\n",
    "#     RUN_YQ_BACKFILL = True\n",
    "\n",
    "#     if not RUN_YQ_BACKFILL:\n",
    "#         print(\"Backfill disabled (set RUN_YQ_BACKFILL=True to run).\")\n",
    "#     else:\n",
    "#         gotA = fill_remaining_with_retries(\n",
    "#             remaining_orig,\n",
    "#             chunk_size=100, pause=1.0, async_fast=True, workers_fast=6,\n",
    "#             subsize_on_timeout=12, workers_slow=2,\n",
    "#             save_every=4, progress_path=Path(\"outputs/meta_yq_progress.parquet\")\n",
    "#         )\n",
    "#         filled = (pd.concat([filled, gotA], ignore_index=True)\n",
    "#                     .drop_duplicates('ticker', keep='first'))\n",
    "\n",
    "#         # recompute remaining\n",
    "#         have_up = set(filled['ticker'].astype(str).str.upper())\n",
    "#         remaining_up   = [u for u in upper_to_orig if u not in have_up]\n",
    "#         remaining_orig = [upper_to_orig[u] for u in remaining_up]\n",
    "#         print(\"after pass A, still remaining:\", len(remaining_orig))\n",
    "\n",
    "#         if remaining_orig:\n",
    "#             gotB = fill_remaining_with_retries(\n",
    "#                 remaining_orig,\n",
    "#                 chunk_size=30, pause=1.2, async_fast=False, workers_fast=4,\n",
    "#                 subsize_on_timeout=8, workers_slow=2,\n",
    "#                 save_every=8, progress_path=Path(\"outputs/meta_yq_progress.parquet\")\n",
    "#             )\n",
    "#             filled = (pd.concat([filled, gotB], ignore_index=True)\n",
    "#                         .drop_duplicates('ticker', keep='first'))\n",
    "\n",
    "#         meta_yq = filled.assign(ticker=lambda d: d['ticker'].str.upper())\n",
    "#         print(\"TOTAL meta_yq:\", len(meta_yq), \"| unique:\", meta_yq['ticker'].nunique())\n",
    "#         meta_yq.to_parquet(\"outputs/meta_yq.parquet\")\n",
    "#         meta_yq.to_csv(\"outputs/meta_yq.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6470286d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have: 338 | remaining: 4780\n",
      "Skipping yahooquery backfill (NO_NETWORK=True).\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Load existing meta; do not scrape if we have it\n",
    "if 'meta_yq' not in globals() or meta_yq is None or meta_yq.empty:\n",
    "    if P_META_YQ.exists():\n",
    "        meta_yq = pd.read_parquet(P_META_YQ)\n",
    "    else:\n",
    "        meta_yq = pd.DataFrame(columns=['ticker','market_cap','sector','industry'])\n",
    "\n",
    "filled = meta_yq.copy()\n",
    "\n",
    "# compute remaining robustly\n",
    "upper_to_orig = {s.upper(): s for s in active_tickers}\n",
    "have_up = set(filled['ticker'].astype(str).str.upper())\n",
    "remaining_up   = [u for u in upper_to_orig if u not in have_up]\n",
    "remaining_orig = [upper_to_orig[u] for u in remaining_up]\n",
    "\n",
    "print(\"have:\", len(have_up), \"| remaining:\", len(remaining_orig))\n",
    "\n",
    "# FAST EXIT: skip if nothing missing OR network disabled\n",
    "if not remaining_orig or NO_NETWORK or not RUN_YQ_BACKFILL:\n",
    "    reason = (\"nothing missing\" if not remaining_orig else \n",
    "              \"NO_NETWORK=True\" if NO_NETWORK else \n",
    "              \"RUN_YQ_BACKFILL=False\")\n",
    "    print(f\"Skipping yahooquery backfill ({reason}).\")\n",
    "else:\n",
    "    gotA = fill_remaining_with_retries(\n",
    "        remaining_orig,\n",
    "        chunk_size=100, pause=1.0, async_fast=True, workers_fast=6,\n",
    "        subsize_on_timeout=12, workers_slow=2,\n",
    "        save_every=4, progress_path=OUT/\"meta_yq_progress.parquet\"\n",
    "    )\n",
    "    filled = (pd.concat([filled, gotA], ignore_index=True)\n",
    "                .drop_duplicates('ticker', keep='first'))\n",
    "\n",
    "    # recompute once\n",
    "    have_up = set(filled['ticker'].astype(str).str.upper())\n",
    "    remaining_up   = [u for u in upper_to_orig if u not in have_up]\n",
    "    remaining_orig = [upper_to_orig[u] for u in remaining_up]\n",
    "    print(\"after pass A, still remaining:\", len(remaining_orig))\n",
    "\n",
    "    if remaining_orig:\n",
    "        gotB = fill_remaining_with_retries(\n",
    "            remaining_orig,\n",
    "            chunk_size=30, pause=1.2, async_fast=False, workers_fast=4,\n",
    "            subsize_on_timeout=8, workers_slow=2,\n",
    "            save_every=8, progress_path=OUT/\"meta_yq_progress.parquet\"\n",
    "        )\n",
    "        filled = (pd.concat([filled, gotB], ignore_index=True)\n",
    "                    .drop_duplicates('ticker', keep='first'))\n",
    "\n",
    "    meta_yq = filled.assign(ticker=lambda d: d['ticker'].str.upper())\n",
    "    print(\"TOTAL meta_yq:\", len(meta_yq), \"| unique:\", meta_yq['ticker'].nunique())\n",
    "    meta_yq.to_parquet(P_META_YQ)\n",
    "    meta_yq.to_csv(OUT/\"meta_yq.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "54801062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata rows: 368\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# start from what's in memory\n",
    "filled = meta_yq.copy()\n",
    "filled['ticker'] = filled['ticker'].str.upper()\n",
    "\n",
    "# union with progress parquet (incremental batches you saved)\n",
    "prog_path = Path(\"outputs/meta_yq_progress.parquet\")\n",
    "if prog_path.exists():\n",
    "    prog = pd.read_parquet(prog_path)\n",
    "    prog['ticker'] = prog['ticker'].str.upper()\n",
    "    filled = (pd.concat([filled, prog], ignore_index=True)\n",
    "                .drop_duplicates('ticker', keep='first'))\n",
    "\n",
    "print(\"metadata rows:\", len(filled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9e87a9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import pandas as pd, json, time, random\n",
    "# from yahooquery import Ticker\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Ensure meta_yq is loaded\n",
    "# if 'meta_yq' not in globals() or meta_yq is None or meta_yq.empty:\n",
    "#     meta_yq = pd.read_parquet(\"outputs/meta_yq.parquet\") if Path(\"outputs/meta_yq.parquet\").exists() \\\n",
    "#               else pd.DataFrame(columns=['ticker','market_cap','sector','industry'])\n",
    "\n",
    "# # Build missing list robustly (upper-case on both sides)\n",
    "# upper_to_orig = {s.upper(): s for s in active_tickers}\n",
    "# have_up = set(meta_yq['ticker'].astype(str).str.upper())\n",
    "# missing_up   = sorted(set(upper_to_orig) - have_up)\n",
    "# missing_orig = [upper_to_orig[u] for u in missing_up]\n",
    "# print(\"Missing:\", len(missing_orig))\n",
    "\n",
    "# # Hard stop if nothing to do\n",
    "# if not missing_orig:\n",
    "#     print(\"All tickers already cached — skipping yahooquery harvest.\")\n",
    "# else:\n",
    "#     CACHE = Path(\"outputs/yq_symbol_cache\"); CACHE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     def fetch_one_yq(sym):\n",
    "#         \"\"\"Fetch market_cap, sector, industry for one symbol; cache to disk.\"\"\"\n",
    "#         fp = CACHE / f\"{sym.upper()}.json\"\n",
    "#         if fp.exists():\n",
    "#             try: return json.loads(fp.read_text())\n",
    "#             except Exception: pass\n",
    "\n",
    "#         data = {'symbol': sym.upper(), 'market_cap': None, 'sector': None, 'industry': None}\n",
    "#         try:\n",
    "#             t = Ticker(sym, asynchronous=False, formatted=False)\n",
    "#             pr = t.price\n",
    "#             if isinstance(pr, dict):\n",
    "#                 rec = pr.get(sym) or (next(iter(pr.values())) if pr else {})\n",
    "#                 if isinstance(rec, dict):\n",
    "#                     data['market_cap'] = rec.get('marketCap')\n",
    "#                     data['symbol'] = (rec.get('symbol') or sym).upper()\n",
    "#             # Try both profiles; either may be present\n",
    "#             for attr in ('summary_profile', 'asset_profile'):\n",
    "#                 try:\n",
    "#                     prof = getattr(t, attr)\n",
    "#                     if isinstance(prof, dict):\n",
    "#                         rec = prof.get(sym) or (next(iter(prof.values())) if prof else {})\n",
    "#                         if isinstance(rec, dict):\n",
    "#                             data['sector']   = data['sector']   or rec.get('sector')\n",
    "#                             data['industry'] = data['industry'] or rec.get('industry')\n",
    "#                 except Exception:\n",
    "#                     pass\n",
    "#         except Exception:\n",
    "#             pass\n",
    "\n",
    "#         try: fp.write_text(json.dumps(data))\n",
    "#         except Exception: pass\n",
    "#         time.sleep(0.25 + random.random()*0.2)  # polite jitter\n",
    "#         return data\n",
    "\n",
    "#     # Sequential harvest (reliable; you can stop and resume anytime)\n",
    "#     rows = []\n",
    "#     for i, sym in enumerate(tqdm(missing_orig, desc=\"harvesting\")):\n",
    "#         rows.append(fetch_one_yq(sym))\n",
    "#         if (i+1) % 200 == 0:\n",
    "#             tmp = (pd.DataFrame(rows)\n",
    "#                      .rename(columns={'symbol':'ticker'})\n",
    "#                      .assign(ticker=lambda d: d['ticker'].str.upper()))\n",
    "#             tmp.to_parquet(\"outputs/meta_yq_more.parquet\")\n",
    "#             print(\"saved partial:\", len(tmp))\n",
    "\n",
    "#     meta_more = (pd.DataFrame(rows)\n",
    "#                    .rename(columns={'symbol':'ticker'})\n",
    "#                    .assign(ticker=lambda d: d['ticker'].str.upper())\n",
    "#                    [['ticker','market_cap','sector','industry']]\n",
    "#                    .drop_duplicates('ticker'))\n",
    "\n",
    "#     meta_yq = (pd.concat([meta_yq, meta_more], ignore_index=True)\n",
    "#                  .drop_duplicates('ticker', keep='first'))\n",
    "#     print(\"New total meta_yq:\", len(meta_yq))\n",
    "#     meta_yq.to_parquet(\"outputs/meta_yq.parquet\")\n",
    "#     meta_yq.to_csv(\"outputs/meta_yq.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "85fd9ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 4780\n",
      "Skipping per-symbol harvest (no work or NO_NETWORK=True).\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd, json, time, random\n",
    "from yahooquery import Ticker\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "if 'meta_yq' not in globals() or meta_yq is None or meta_yq.empty:\n",
    "    meta_yq = pd.read_parquet(P_META_YQ) if P_META_YQ.exists() else pd.DataFrame(columns=['ticker','market_cap','sector','industry'])\n",
    "\n",
    "upper_to_orig = {s.upper(): s for s in active_tickers}\n",
    "have_up = set(meta_yq['ticker'].astype(str).str.upper())\n",
    "missing_up   = sorted(set(upper_to_orig) - have_up)\n",
    "missing_orig = [upper_to_orig[u] for u in missing_up]\n",
    "print(\"Missing:\", len(missing_orig))\n",
    "\n",
    "# FAST EXIT: skip if no work or network disabled\n",
    "if not missing_orig or NO_NETWORK:\n",
    "    print(\"Skipping per-symbol harvest (no work or NO_NETWORK=True).\")\n",
    "else:\n",
    "    CACHE = P_YQ_SYM_DIR; CACHE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # optional: skip warrants/units/rights to save time\n",
    "    _SKIP_SUFFIX_RE = re.compile(r\".*(?:W|WS|WT|W[A-D]?|U|R)$\")\n",
    "\n",
    "    def fetch_one_yq(sym):\n",
    "        \"\"\"Fetch market_cap/sector/industry for one symbol; cache.\"\"\"\n",
    "        t = sym.upper()\n",
    "        if _SKIP_SUFFIX_RE.match(t):\n",
    "            fp = CACHE / f\"{t}.json\"\n",
    "            if not fp.exists():\n",
    "                fp.write_text(json.dumps({'symbol': t, 'market_cap': None, 'sector': None, 'industry': None}))\n",
    "            return {'symbol': t, 'market_cap': None, 'sector': None, 'industry': None}\n",
    "\n",
    "        fp = CACHE / f\"{t}.json\"\n",
    "        if fp.exists():\n",
    "            try: return json.loads(fp.read_text())\n",
    "            except Exception: pass\n",
    "\n",
    "        data = {'symbol': t, 'market_cap': None, 'sector': None, 'industry': None}\n",
    "        try:\n",
    "            tq = Ticker(sym, asynchronous=False, formatted=False)\n",
    "            pr = tq.price\n",
    "            if isinstance(pr, dict):\n",
    "                rec = pr.get(sym) or (next(iter(pr.values())) if pr else {})\n",
    "                if isinstance(rec, dict):\n",
    "                    data['market_cap'] = rec.get('marketCap')\n",
    "                    data['symbol'] = (rec.get('symbol') or t).upper()\n",
    "            for attr in ('summary_profile', 'asset_profile'):\n",
    "                try:\n",
    "                    prof = getattr(tq, attr)\n",
    "                    if isinstance(prof, dict):\n",
    "                        rec = prof.get(sym) or (next(iter(prof.values())) if prof else {})\n",
    "                        if isinstance(rec, dict):\n",
    "                            data['sector']   = data['sector']   or rec.get('sector')\n",
    "                            data['industry'] = data['industry'] or rec.get('industry')\n",
    "                except Exception:\n",
    "                    pass\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        try: fp.write_text(json.dumps(data))\n",
    "        except Exception: pass\n",
    "        time.sleep(0.25 + random.random()*0.2)\n",
    "        return data\n",
    "\n",
    "    rows = []\n",
    "    for i, sym in enumerate(tqdm(missing_orig, desc=\"harvesting\")):\n",
    "        rows.append(fetch_one_yq(sym))\n",
    "        if (i+1) % 200 == 0:\n",
    "            tmp = (pd.DataFrame(rows).rename(columns={'symbol':'ticker'})\n",
    "                                     .assign(ticker=lambda d: d['ticker'].str.upper()))\n",
    "            tmp.to_parquet(OUT/\"meta_yq_more.parquet\")\n",
    "            print(\"saved partial:\", len(tmp))\n",
    "\n",
    "    meta_more = (pd.DataFrame(rows)\n",
    "                   .rename(columns={'symbol':'ticker'})\n",
    "                   .assign(ticker=lambda d: d['ticker'].str.upper())\n",
    "                   [['ticker','market_cap','sector','industry']]\n",
    "                   .drop_duplicates('ticker'))\n",
    "\n",
    "    meta_yq = (pd.concat([meta_yq, meta_more], ignore_index=True)\n",
    "                 .drop_duplicates('ticker', keep='first'))\n",
    "    meta_yq.to_parquet(P_META_YQ)\n",
    "    meta_yq.to_csv(OUT/\"meta_yq.csv\", index=False)\n",
    "    print(\"New total meta_yq:\", len(meta_yq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "151dca54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>market_cap</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASTL</td>\n",
       "      <td>4.146867e+08</td>\n",
       "      <td>Basic Materials</td>\n",
       "      <td>Steel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASTLW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Basic Materials</td>\n",
       "      <td>Steel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASTS</td>\n",
       "      <td>1.773648e+10</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Communication Equipment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASUR</td>\n",
       "      <td>2.248851e+08</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Software - Application</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASYS</td>\n",
       "      <td>1.351238e+08</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Semiconductor Equipment &amp; Materials</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker    market_cap           sector                             industry\n",
       "0   ASTL  4.146867e+08  Basic Materials                                Steel\n",
       "1  ASTLW           NaN  Basic Materials                                Steel\n",
       "2   ASTS  1.773648e+10       Technology              Communication Equipment\n",
       "3   ASUR  2.248851e+08       Technology               Software - Application\n",
       "4   ASYS  1.351238e+08       Technology  Semiconductor Equipment & Materials"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_yq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6550cbe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th colspan=\"10\" halign=\"left\">Adj Close</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>AACB</th>\n",
       "      <th>AACBR</th>\n",
       "      <th>AACBU</th>\n",
       "      <th>AACG</th>\n",
       "      <th>AACI</th>\n",
       "      <th>AACIU</th>\n",
       "      <th>AACIW</th>\n",
       "      <th>AADR</th>\n",
       "      <th>AAL</th>\n",
       "      <th>AALG</th>\n",
       "      <th>...</th>\n",
       "      <th>ZUMZ</th>\n",
       "      <th>ZURA</th>\n",
       "      <th>ZVRA</th>\n",
       "      <th>ZVZZT</th>\n",
       "      <th>ZWZZT</th>\n",
       "      <th>ZXZZT</th>\n",
       "      <th>ZYBT</th>\n",
       "      <th>ZYME</th>\n",
       "      <th>ZYN</th>\n",
       "      <th>ZYXI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-10-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.794346</td>\n",
       "      <td>11.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>373000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>958200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>167500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.920731</td>\n",
       "      <td>12.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>390300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>317300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1142000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>114300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.814838</td>\n",
       "      <td>12.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>319600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>190100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>890900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>132600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.246822</td>\n",
       "      <td>12.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>387700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>555400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.649937</td>\n",
       "      <td>12.18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>201500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1057400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>141300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30726 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Price      Adj Close                                                            ...  Volume                           \\\n",
       "Ticker          AACB AACBR AACBU  AACG AACI AACIU AACIW       AADR    AAL AALG  ...    ZUMZ ZURA    ZVRA ZVZZT ZWZZT   \n",
       "Date                                                                            ...                                    \n",
       "2022-10-03       NaN   NaN   NaN  1.75  NaN   NaN   NaN  43.794346  11.92  NaN  ...  373000  NaN  214700   NaN   NaN   \n",
       "2022-10-04       NaN   NaN   NaN  1.82  NaN   NaN   NaN  44.920731  12.95  NaN  ...  390300  NaN  317300   NaN   NaN   \n",
       "2022-10-05       NaN   NaN   NaN  1.83  NaN   NaN   NaN  44.814838  12.87  NaN  ...  319600  NaN  190100   NaN   NaN   \n",
       "2022-10-06       NaN   NaN   NaN  1.96  NaN   NaN   NaN  44.246822  12.73  NaN  ...  387700  NaN  125300   NaN   NaN   \n",
       "2022-10-07       NaN   NaN   NaN  1.89  NaN   NaN   NaN  43.649937  12.18  NaN  ...  201500  NaN  269600   NaN   NaN   \n",
       "\n",
       "Price                                       \n",
       "Ticker     ZXZZT ZYBT     ZYME ZYN    ZYXI  \n",
       "Date                                        \n",
       "2022-10-03   NaN  NaN   958200 NaN  167500  \n",
       "2022-10-04   NaN  NaN  1142000 NaN  114300  \n",
       "2022-10-05   NaN  NaN   890900 NaN  132600  \n",
       "2022-10-06   NaN  NaN   555400 NaN   94600  \n",
       "2022-10-07   NaN  NaN  1057400 NaN  141300  \n",
       "\n",
       "[5 rows x 30726 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dce61f",
   "metadata": {},
   "source": [
    "RUN THIS BELOW NEXT AFTER THE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3b5a84d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEC_HEADERS = {\"User-Agent\": \"Ozkan Gelincik <ozkangelincik@gmail.com>\", \"Accept\": \"application/json\"}\n",
    "CACHE = Path(\"outputs/sec_cache\"); CACHE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Retry-capable session\n",
    "def make_session():\n",
    "    sess = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=5, connect=3, read=3, backoff_factor=0.5,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"]\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries, pool_connections=30, pool_maxsize=30)\n",
    "    sess.mount(\"https://\", adapter)\n",
    "    sess.headers.update(SEC_HEADERS)\n",
    "    return sess\n",
    "\n",
    "SESSION = make_session()\n",
    "\n",
    "# Ticker -> CIK (single download)\n",
    "def load_cik_map():\n",
    "    r = SESSION.get(\"https://www.sec.gov/files/company_tickers.json\", timeout=30)\n",
    "    r.raise_for_status()\n",
    "    m = r.json()\n",
    "    return {v[\"ticker\"].upper(): str(v[\"cik_str\"]).zfill(10) for v in m.values()}\n",
    "\n",
    "CIK_MAP = load_cik_map()\n",
    "\n",
    "def fetch_cik_json(cik: str):\n",
    "    \"\"\"Return dict or None; never raise on JSON decode.\"\"\"\n",
    "    url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
    "    try:\n",
    "        r = SESSION.get(url, timeout=30)\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "        try:\n",
    "            return r.json()\n",
    "        except Exception:\n",
    "            return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def sec_recent_one(ticker: str):\n",
    "    \"\"\"Robust per-ticker; caches; never raises.\"\"\"\n",
    "    t_up = ticker.upper()\n",
    "    cik = CIK_MAP.get(t_up)\n",
    "    base = {\"ticker\": t_up, \"cik\": cik, \"recent_form\": None, \"recent_filing_date\": None}\n",
    "    if not cik:\n",
    "        return base\n",
    "\n",
    "    fp = CACHE / f\"{cik}.json\"\n",
    "    if fp.exists():\n",
    "        try:\n",
    "            j = json.loads(fp.read_text())\n",
    "        except Exception:\n",
    "            j = None\n",
    "    else:\n",
    "        j = fetch_cik_json(cik)\n",
    "        if j:\n",
    "            try:\n",
    "                fp.write_text(json.dumps(j))\n",
    "            except Exception:\n",
    "                pass\n",
    "        time.sleep(0.08)  # be polite\n",
    "\n",
    "    if not isinstance(j, dict):\n",
    "        return base\n",
    "\n",
    "    recent = j.get(\"filings\", {}).get(\"recent\", {})\n",
    "    forms = recent.get(\"form\", []) or []\n",
    "    dates = recent.get(\"filingDate\", []) or []\n",
    "    if forms and dates:\n",
    "        base[\"recent_form\"] = forms[0]\n",
    "        base[\"recent_filing_date\"] = dates[0]\n",
    "    return base\n",
    "\n",
    "def sec_recent_parallel(tickers, max_workers=3):\n",
    "    \"\"\"Parallel but tolerant: exceptions are captured, progress continues.\"\"\"\n",
    "    rows = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futs = [ex.submit(sec_recent_one, t) for t in tickers]\n",
    "        for f in tqdm(as_completed(futs), total=len(futs), desc=\"SEC\"):\n",
    "            try:\n",
    "                rows.append(f.result())\n",
    "            except Exception as e:\n",
    "                rows.append({\"ticker\": None, \"cik\": None, \"recent_form\": None, \"recent_filing_date\": None, \"error\": str(e)[:120]})\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1c9927e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache-missing: 0 of 5118\n"
     ]
    }
   ],
   "source": [
    "# 1) Figure out which tickers still need a SEC JSON file\n",
    "upper_to_orig = {t.upper(): t for t in active_tickers}\n",
    "to_fetch = []\n",
    "for u, orig in upper_to_orig.items():\n",
    "    cik = CIK_MAP.get(u)\n",
    "    if cik and not (CACHE / f\"{cik}.json\").exists():\n",
    "        to_fetch.append(orig)\n",
    "\n",
    "print(\"cache-missing:\", len(to_fetch), \"of\", len(active_tickers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2d403a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SEC: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched rows: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2) Fetch the missing ones (polite, retrying, parallel)\n",
    "sec_new = sec_recent_parallel(to_fetch, max_workers=3)\n",
    "print(\"fetched rows:\", len(sec_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9c83a0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows: 3845871 | tickers: 5121 | has cols: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === M1: Build analysis_enriched with splits + time-varying shares/float ===\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Start from daily prices + fundamentals\n",
    "analysis_enriched = (\n",
    "    analysis_df\n",
    "      .merge(fundamentals, on='ticker', how='left')\n",
    "      .sort_values(['ticker','date'])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Add split flags (1 on event day, else 0)\n",
    "analysis_enriched = analysis_enriched.merge(split_flags, on=['ticker','date'], how='left')\n",
    "for c in ['performed_split','performed_reverse_split']:\n",
    "    if c not in analysis_enriched.columns:\n",
    "        analysis_enriched[c] = 0\n",
    "    analysis_enriched[c] = analysis_enriched[c].fillna(0).astype('int8')\n",
    "\n",
    "# Add time-varying shares & float_shares (SEC), forward-fill between filings\n",
    "analysis_enriched = analysis_enriched.merge(events_enriched, on=['ticker','date'], how='left')\n",
    "\n",
    "# Optional fallbacks from snapshots if present (avoid _x/_y residue)\n",
    "if 'shares_outstanding_x' in analysis_enriched.columns and 'shares_outstanding_y' in analysis_enriched.columns:\n",
    "    analysis_enriched['shares_outstanding'] = analysis_enriched['shares_outstanding_x'].where(\n",
    "        analysis_enriched['shares_outstanding_x'].notna(),\n",
    "        analysis_enriched['shares_outstanding_y']\n",
    "    )\n",
    "    analysis_enriched.drop(columns=['shares_outstanding_x','shares_outstanding_y'], inplace=True)\n",
    "\n",
    "if 'float_shares_x' in analysis_enriched.columns and 'float_shares_y' in analysis_enriched.columns:\n",
    "    analysis_enriched['float_shares'] = analysis_enriched['float_shares_x'].where(\n",
    "        analysis_enriched['float_shares_x'].notna(),\n",
    "        analysis_enriched['float_shares_y']\n",
    "    )\n",
    "    analysis_enriched.drop(columns=['float_shares_x','float_shares_y'], inplace=True)\n",
    "\n",
    "# Forward-fill time series within each ticker\n",
    "for col in ['shares_outstanding','float_shares']:\n",
    "    if col in analysis_enriched.columns:\n",
    "        analysis_enriched[col] = analysis_enriched.groupby('ticker')[col].ffill()\n",
    "\n",
    "# Alias\n",
    "analysis_enriched['free_float'] = analysis_enriched.get('float_shares')\n",
    "\n",
    "# Save\n",
    "OUT = Path(\"outputs\")\n",
    "analysis_enriched.to_parquet(OUT / \"analysis_enriched.parquet\")\n",
    "analysis_enriched.to_csv(OUT / \"analysis_enriched.csv\", index=False)\n",
    "\n",
    "print(\"rows:\", len(analysis_enriched),\n",
    "      \"| tickers:\", analysis_enriched['ticker'].nunique(),\n",
    "      \"| has cols:\", {'shares_outstanding','float_shares','free_float','performed_split','performed_reverse_split'}.issubset(analysis_enriched.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "123690df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_recent_df.to_parquet(\"outputs/sec_recent.parquet\", index=False)\n",
    "sec_recent_df.to_csv(\"outputs/sec_recent.csv\", index=False)\n",
    "\n",
    "def sic_to_sector(s):\n",
    "    try: s = int(s)\n",
    "    except: return None\n",
    "    if 1<=s<=9:   return 'Agriculture'\n",
    "    if 10<=s<=14: return 'Mining'\n",
    "    if 15<=s<=17: return 'Construction'\n",
    "    if 20<=s<=39: return 'Manufacturing'\n",
    "    if 40<=s<=49: return 'Transportation & Utilities'\n",
    "    if 50<=s<=51: return 'Wholesale'\n",
    "    if 52<=s<=59: return 'Retail'\n",
    "    if 60<=s<=67: return 'Finance'\n",
    "    if 70<=s<=89: return 'Services'\n",
    "    if 91<=s<=99: return 'Public Administration'\n",
    "    return None\n",
    "\n",
    "sec_recent_df[\"sector_sic\"] = sec_recent_df[\"sic\"].apply(sic_to_sector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e369b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: prices_raw already loaded from cache (Bootstrap).\n"
     ]
    }
   ],
   "source": [
    "import pandas as _pd\n",
    "if 'prices_raw' not in globals() or prices_raw is None or (isinstance(prices_raw, _pd.DataFrame) and prices_raw.empty):\n",
    "    # Rebuild analysis_df from prices_raw\n",
    "    import pandas as pd, numpy as np\n",
    "    from pathlib import Path\n",
    "\n",
    "    # If prices_raw isn't in memory, try to load it\n",
    "    if 'prices_raw' not in globals() or prices_raw is None:\n",
    "        p = Path(\"outputs/prices_wide.parquet\")\n",
    "        if p.exists():\n",
    "            prices_raw = pd.read_parquet(p)\n",
    "        else:\n",
    "            # try assembling from chunks\n",
    "            parts = sorted((Path(\"outputs\")/\"chunks\").glob(\"prices_wide_*.parquet\"))\n",
    "            if parts:\n",
    "                prices_raw = pd.concat([pd.read_parquet(x) for x in parts], axis=1).sort_index()\n",
    "            else:\n",
    "                raise RuntimeError(\"prices_raw not loaded and no saved files found.\")\n",
    "\n",
    "    def tidy_from_yf(wide: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = wide.copy(); df.index.name = 'date'\n",
    "        tidy = (df.stack(level=1, future_stack=True)\n",
    "                  .rename_axis(['date','ticker'])\n",
    "                  .reset_index()\n",
    "                  .sort_values(['ticker','date'])\n",
    "                  .reset_index(drop=True))\n",
    "        # lower-case column names to be consistent\n",
    "        tidy.columns = [c.lower() if isinstance(c, str) else c for c in tidy.columns]\n",
    "        return tidy\n",
    "\n",
    "    prices_tidy = tidy_from_yf(prices_raw)\n",
    "\n",
    "    # daily returns\n",
    "    adj = prices_tidy[['date','ticker','adj close']].dropna().sort_values(['ticker','date']).copy()\n",
    "    adj['ret']    = adj.groupby('ticker')['adj close'].pct_change()\n",
    "    adj['logret'] = np.log(adj['adj close'] / adj.groupby('ticker')['adj close'].shift(1))\n",
    "\n",
    "    analysis_df = (\n",
    "        prices_tidy[['date','ticker','open','high','low','close','adj close','volume']]\n",
    "        .merge(adj[['date','ticker','ret','logret']], on=['date','ticker'], how='left')\n",
    "    )\n",
    "else:\n",
    "    print('Skipping: prices_raw already loaded from cache (Bootstrap).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fe05ae73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fundamentals tickers: 338\n",
      "analysis_enriched rows: 3845871\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ret</th>\n",
       "      <th>logret</th>\n",
       "      <th>market_cap</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "      <th>cik</th>\n",
       "      <th>sic</th>\n",
       "      <th>sic_desc</th>\n",
       "      <th>recent_form</th>\n",
       "      <th>recent_filing_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>AACB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>AACB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>AACB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-06</td>\n",
       "      <td>AACB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-07</td>\n",
       "      <td>AACB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date ticker  open  high  low  close  adj close  volume  ret  logret  market_cap sector industry  cik  sic  \\\n",
       "0 2022-10-03   AACB   NaN   NaN  NaN    NaN        NaN     NaN  NaN     NaN         NaN    NaN      NaN  NaN  NaN   \n",
       "1 2022-10-04   AACB   NaN   NaN  NaN    NaN        NaN     NaN  NaN     NaN         NaN    NaN      NaN  NaN  NaN   \n",
       "2 2022-10-05   AACB   NaN   NaN  NaN    NaN        NaN     NaN  NaN     NaN         NaN    NaN      NaN  NaN  NaN   \n",
       "3 2022-10-06   AACB   NaN   NaN  NaN    NaN        NaN     NaN  NaN     NaN         NaN    NaN      NaN  NaN  NaN   \n",
       "4 2022-10-07   AACB   NaN   NaN  NaN    NaN        NaN     NaN  NaN     NaN         NaN    NaN      NaN  NaN  NaN   \n",
       "\n",
       "  sic_desc recent_form recent_filing_date  \n",
       "0      NaN         NaN                NaN  \n",
       "1      NaN         NaN                NaN  \n",
       "2      NaN         NaN                NaN  \n",
       "3      NaN         NaN                NaN  \n",
       "4      NaN         NaN                NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# normalize tickers\n",
    "meta_yq['ticker'] = meta_yq['ticker'].str.upper()\n",
    "sec_recent_df['ticker'] = sec_recent_df['ticker'].str.upper()\n",
    "analysis_df['ticker'] = analysis_df['ticker'].str.upper()\n",
    "\n",
    "# build fundamentals (prefer Yahoo, backfill with SIC sector if present)\n",
    "keep_sec_cols = ['ticker','cik','recent_form','recent_filing_date','sic','sic_desc']\n",
    "if 'sector_sic' in sec_recent_df.columns:\n",
    "    keep_sec_cols.append('sector_sic')\n",
    "\n",
    "fundamentals = meta_yq.merge(sec_recent_df[keep_sec_cols], on=\"ticker\", how=\"left\")\n",
    "\n",
    "if 'sector_sic' in fundamentals.columns:\n",
    "    fundamentals['sector'] = fundamentals['sector'].where(fundamentals['sector'].notna(), fundamentals['sector_sic'])\n",
    "\n",
    "# tidy columns & save\n",
    "cols = ['ticker','market_cap','sector','industry','cik','sic','sic_desc','recent_form','recent_filing_date']\n",
    "for c in cols:\n",
    "    if c not in fundamentals.columns:\n",
    "        fundamentals[c] = pd.NA\n",
    "fundamentals = fundamentals[cols].drop_duplicates('ticker')\n",
    "\n",
    "fundamentals.to_parquet(\"outputs/fundamentals.parquet\")\n",
    "fundamentals.to_csv(\"outputs/fundamentals.csv\", index=False)\n",
    "print(\"fundamentals tickers:\", fundamentals['ticker'].nunique())\n",
    "\n",
    "# merge into analysis & save\n",
    "analysis_enriched = (\n",
    "    analysis_df.merge(fundamentals, on=\"ticker\", how=\"left\")\n",
    "               .sort_values([\"ticker\",\"date\"])\n",
    "               .reset_index(drop=True)\n",
    ")\n",
    "analysis_enriched.to_parquet(\"outputs/analysis_enriched.parquet\")\n",
    "analysis_enriched.to_csv(\"outputs/analysis_enriched.csv\", index=False)\n",
    "print(\"analysis_enriched rows:\", len(analysis_enriched))\n",
    "display(analysis_enriched.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5ea74aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: True | rows: 3845871 | tickers: 5121\n",
      "Saved: outputs/analysis_enriched_v2.parquet outputs/analysis_enriched_v2.csv\n"
     ]
    }
   ],
   "source": [
    "# === FORCE FINALIZE (safe, idempotent) ===\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "OUT = Path(\"outputs\"); OUT.mkdir(exist_ok=True)\n",
    "\n",
    "# ---- 1) Base tables must exist in memory (from earlier cells)\n",
    "assert 'analysis_df' in globals(), \"analysis_df is missing – run the bootstrap cells first\"\n",
    "assert 'fundamentals' in globals(), \"fundamentals is missing – run the bootstrap cells first\"\n",
    "\n",
    "# ---- 2) Load split_flags from memory or disk\n",
    "SPLIT_FLAGS_PQ = OUT / \"split_flags.parquet\"\n",
    "if 'split_flags' not in globals():\n",
    "    if SPLIT_FLAGS_PQ.exists():\n",
    "        split_flags = pd.read_parquet(SPLIT_FLAGS_PQ)\n",
    "    else:\n",
    "        split_flags = pd.DataFrame(columns=['ticker','date','performed_split','performed_reverse_split'])\n",
    "\n",
    "# Normalize types\n",
    "split_flags = (\n",
    "    split_flags.assign(\n",
    "        ticker=lambda d: d['ticker'].astype(str),\n",
    "        date=lambda d: pd.to_datetime(d['date']).dt.tz_localize(None).dt.normalize()\n",
    "    )\n",
    "    .drop_duplicates(['ticker','date'])\n",
    ")\n",
    "\n",
    "# ---- 3) Build events_enriched from SEC shares_events + price table (robust, no merge_asof)\n",
    "SHARES_EVENTS_PQ = OUT / \"shares_events.parquet\"\n",
    "if 'events_enriched' in globals():\n",
    "    ee = events_enriched.copy()\n",
    "else:\n",
    "    if SHARES_EVENTS_PQ.exists():\n",
    "        shares_events = pd.read_parquet(SHARES_EVENTS_PQ)\n",
    "    else:\n",
    "        shares_events = pd.DataFrame(columns=['date','ticker','shares_outstanding','entity_public_float_usd'])\n",
    "\n",
    "    shares_events = (\n",
    "        shares_events.assign(\n",
    "            ticker=lambda d: d['ticker'].astype(str),\n",
    "            date=lambda d: pd.to_datetime(d['date']).dt.tz_localize(None).dt.normalize()\n",
    "        )\n",
    "        .dropna(subset=['ticker','date'])\n",
    "        .drop_duplicates(['ticker','date'])\n",
    "        .sort_values(['ticker','date'])\n",
    "    )\n",
    "\n",
    "    price_ev = (\n",
    "        analysis_df[['date','ticker','adj close']]\n",
    "        .rename(columns={'adj close':'adj_close'})\n",
    "        .assign(\n",
    "            ticker=lambda d: d['ticker'].astype(str),\n",
    "            date=lambda d: pd.to_datetime(d['date']).dt.tz_localize(None).dt.normalize()\n",
    "        )\n",
    "        .dropna(subset=['ticker','date','adj_close'])\n",
    "        .drop_duplicates(['ticker','date'])\n",
    "        .sort_values(['ticker','date'])\n",
    "    )\n",
    "\n",
    "    # map last price <= filing date per ticker via index union + ffill\n",
    "    events_enriched_list = []\n",
    "    g_prices = dict(tuple(price_ev.groupby('ticker')))\n",
    "    for tkr, g in shares_events.groupby('ticker', sort=False):\n",
    "        filing_idx = g[['date']].drop_duplicates().set_index('date')\n",
    "        p = g_prices.get(tkr)\n",
    "        if p is None or p.empty:\n",
    "            g2 = g.copy()\n",
    "            g2['adj_close'] = np.nan\n",
    "            events_enriched_list.append(g2)\n",
    "            continue\n",
    "        p = p.set_index('date')['adj_close'].sort_index()\n",
    "        union_idx = p.index.union(filing_idx.index).sort_values()\n",
    "        p_ffill = p.reindex(union_idx).ffill()\n",
    "        adj_on_filing = p_ffill.reindex(filing_idx.index)\n",
    "        g2 = g.copy()\n",
    "        g2['adj_close'] = adj_on_filing.values\n",
    "        events_enriched_list.append(g2)\n",
    "\n",
    "    ee = (pd.concat(events_enriched_list, ignore_index=True) if events_enriched_list else shares_events.copy())\n",
    "    ee['float_shares'] = np.where(\n",
    "        ee.get('entity_public_float_usd').notna() & ee['adj_close'].notna(),\n",
    "        ee['entity_public_float_usd'] / ee['adj_close'],\n",
    "        np.nan\n",
    "    )\n",
    "    ee = ee[['date','ticker','shares_outstanding','float_shares']].sort_values(['ticker','date'])\n",
    "\n",
    "# ---- 4) Rebuild analysis_enriched with new columns (and don’t overwrite older file)\n",
    "ae = (\n",
    "    analysis_df\n",
    "      .merge(fundamentals, on='ticker', how='left')\n",
    "      .merge(split_flags, on=['ticker','date'], how='left')\n",
    "      .merge(ee, on=['ticker','date'], how='left')\n",
    "      .sort_values(['ticker','date'])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Fill/convert flags; forward-fill shares/float within each ticker\n",
    "for c in ['performed_split','performed_reverse_split']:\n",
    "    if c not in ae.columns:\n",
    "        ae[c] = 0\n",
    "    ae[c] = ae[c].fillna(0).astype('int8')\n",
    "\n",
    "for c in ['shares_outstanding','float_shares']:\n",
    "    if c in ae.columns:\n",
    "        ae[c] = ae.groupby('ticker')[c].ffill()\n",
    "\n",
    "ae['free_float'] = ae.get('float_shares')\n",
    "\n",
    "# ---- 5) Save as new version to avoid clobbering old outputs\n",
    "ae.to_parquet(OUT / \"analysis_enriched_v2.parquet\")\n",
    "ae.to_csv(OUT / \"analysis_enriched_v2.csv\", index=False)\n",
    "\n",
    "# Put back into the working var name if you want to continue with it\n",
    "analysis_enriched = ae\n",
    "\n",
    "need = {'shares_outstanding','float_shares','free_float','performed_split','performed_reverse_split'}\n",
    "print(\"OK:\", need.issubset(analysis_enriched.columns),\n",
    "      \"| rows:\", len(analysis_enriched),\n",
    "      \"| tickers:\", analysis_enriched['ticker'].nunique())\n",
    "print(\"Saved:\", (OUT/'analysis_enriched_v2.parquet'), (OUT/'analysis_enriched_v2.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b8cf525b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tickers still missing both (pre-yf): 1544\n",
      "…processed 200/1544\n",
      "…processed 400/1544\n",
      "…processed 600/1544\n",
      "…processed 800/1544\n",
      "…processed 1000/1544\n",
      "…processed 1200/1544\n",
      "…processed 1400/1544\n",
      "yfinance snapshot rows usable: 327\n",
      "Backfill applied. Saved analysis_enriched_v6.*\n",
      "Coverage now -> shares_outstanding: 3658 | float_shares: 2752\n"
     ]
    }
   ],
   "source": [
    "# === Backfill v6 (yfinance snapshots): shares_outstanding / float_shares ===\n",
    "import time, json, re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "assert 'analysis_enriched' in globals(), \"Run the finalize cell so analysis_enriched exists.\"\n",
    "\n",
    "OUT = Path(\"outputs\"); OUT.mkdir(exist_ok=True)\n",
    "YF_CACHE = OUT / \"yf_snapshot_cache\"; YF_CACHE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Skip obvious non-common-stock symbols (warrants/units/rights/when-issued)\n",
    "_SKIP_SUFFIX_RE = re.compile(r\".*(?:W|WS|WT|W[A-D]?|U|R)$\")\n",
    "\n",
    "def tickers_missing_both(ae: pd.DataFrame) -> list:\n",
    "    g = (ae.groupby('ticker')[['shares_outstanding','float_shares']]\n",
    "           .apply(lambda df: pd.Series({\n",
    "               'has_shares': df['shares_outstanding'].notna().any(),\n",
    "               'has_float' : df['float_shares'].notna().any()\n",
    "           })))\n",
    "    return sorted(g.index[(~g['has_shares']) & (~g['has_float'])].tolist())\n",
    "\n",
    "def _safe_float(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _read_cache(sym):\n",
    "    p = YF_CACHE / f\"{sym}.json\"\n",
    "    if p.exists():\n",
    "        try:\n",
    "            return json.loads(p.read_text())\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def _write_cache(sym, d):\n",
    "    p = YF_CACHE / f\"{sym}.json\"\n",
    "    try:\n",
    "        p.write_text(json.dumps(d))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def fetch_yf_snapshot_one(sym: str):\n",
    "    \"\"\"Return {'ticker', 'shares_outstanding_yf', 'float_shares_yf'} using yfinance with fallbacks.\"\"\"\n",
    "    t = str(sym).upper()\n",
    "    if _SKIP_SUFFIX_RE.match(t):\n",
    "        return {'ticker': t, 'shares_outstanding_yf': None, 'float_shares_yf': None}\n",
    "\n",
    "    # cache first\n",
    "    cached = _read_cache(t)\n",
    "    if isinstance(cached, dict) and 'shares_outstanding_yf' in cached and 'float_shares_yf' in cached:\n",
    "        return {'ticker': t, **cached}\n",
    "\n",
    "    out = {'ticker': t, 'shares_outstanding_yf': None, 'float_shares_yf': None}\n",
    "    try:\n",
    "        tk = yf.Ticker(t)\n",
    "\n",
    "        # --- 1) Fast path\n",
    "        fast = {}\n",
    "        try:\n",
    "            fast = tk.fast_info or {}\n",
    "        except Exception:\n",
    "            fast = {}\n",
    "\n",
    "        last_price = _safe_float(fast.get('last_price')) or _safe_float(fast.get('lastPrice'))  # yfinance names vary\n",
    "        if last_price is None:\n",
    "            # short history fallback for price\n",
    "            try:\n",
    "                h = tk.history(period='5d', interval='1d')\n",
    "                if not h.empty:\n",
    "                    last_price = _safe_float(h['Close'].iloc[-1])\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        market_cap = _safe_float(fast.get('market_cap')) or _safe_float(fast.get('marketCap'))\n",
    "        shares_fast = _safe_float(fast.get('shares'))\n",
    "\n",
    "        # --- 2) Slower info fallback\n",
    "        info = {}\n",
    "        if shares_fast is None or market_cap is None:\n",
    "            try:\n",
    "                info = tk.info or {}\n",
    "            except Exception:\n",
    "                info = {}\n",
    "\n",
    "        shares_info = _safe_float(info.get('sharesOutstanding'))\n",
    "        float_info  = _safe_float(info.get('floatShares'))\n",
    "        insiders    = _safe_float(info.get('heldPercentInsiders'))  # 0..1\n",
    "\n",
    "        # choose shares_outstanding\n",
    "        shares = shares_fast or shares_info\n",
    "        if shares is None and market_cap is not None and last_price not in (None, 0.0):\n",
    "            shares = market_cap / last_price\n",
    "\n",
    "        # choose float_shares\n",
    "        flt = float_info\n",
    "        if flt is None and insiders is not None and shares is not None:\n",
    "            flt = (1.0 - insiders) * shares\n",
    "\n",
    "        out['shares_outstanding_yf'] = shares\n",
    "        out['float_shares_yf'] = flt\n",
    "\n",
    "    except Exception:\n",
    "        # keep Nones\n",
    "        pass\n",
    "\n",
    "    _write_cache(t, {'shares_outstanding_yf': out['shares_outstanding_yf'], 'float_shares_yf': out['float_shares_yf']})\n",
    "    time.sleep(0.12)  # polite pause\n",
    "    return out\n",
    "\n",
    "# 1) who still needs backfill?\n",
    "need_yf = tickers_missing_both(analysis_enriched)\n",
    "print(f\"Tickers still missing both (pre-yf): {len(need_yf)}\")\n",
    "\n",
    "# 2) harvest with caching (sequential = most reliable for yfinance)\n",
    "rows = []\n",
    "for i, tkr in enumerate(need_yf, 1):\n",
    "    rows.append(fetch_yf_snapshot_one(tkr))\n",
    "    if i % 200 == 0:\n",
    "        print(f\"…processed {i}/{len(need_yf)}\")\n",
    "\n",
    "yf_snap = (pd.DataFrame(rows)\n",
    "             .dropna(how='all', subset=['shares_outstanding_yf','float_shares_yf'])\n",
    "             .drop_duplicates('ticker'))\n",
    "\n",
    "print(\"yfinance snapshot rows usable:\", len(yf_snap))\n",
    "\n",
    "# 3) apply only where still missing; then forward-fill within ticker\n",
    "if not yf_snap.empty:\n",
    "    yf_snap['ticker'] = yf_snap['ticker'].astype(str).str.upper()\n",
    "    map_sh = yf_snap.set_index('ticker')['shares_outstanding_yf'].to_dict()\n",
    "    map_ff = yf_snap.set_index('ticker')['float_shares_yf'].to_dict()\n",
    "\n",
    "    mask_sh = analysis_enriched['shares_outstanding'].isna()\n",
    "    analysis_enriched.loc[mask_sh, 'shares_outstanding'] = analysis_enriched.loc[mask_sh, 'ticker'].map(map_sh)\n",
    "\n",
    "    mask_ff = analysis_enriched['float_shares'].isna()\n",
    "    analysis_enriched.loc[mask_ff, 'float_shares'] = analysis_enriched.loc[mask_ff, 'ticker'].map(map_ff)\n",
    "\n",
    "    analysis_enriched['free_float'] = analysis_enriched['float_shares']\n",
    "\n",
    "    for c in ['shares_outstanding','float_shares','free_float']:\n",
    "        if c in analysis_enriched.columns:\n",
    "            analysis_enriched[c] = analysis_enriched.groupby('ticker')[c].ffill()\n",
    "\n",
    "    # Save new version\n",
    "    analysis_enriched.to_parquet(OUT / \"analysis_enriched_v6.parquet\")\n",
    "    analysis_enriched.to_csv(OUT / \"analysis_enriched_v6.csv\", index=False)\n",
    "    print(\"Backfill applied. Saved analysis_enriched_v6.*\")\n",
    "\n",
    "# Coverage report\n",
    "cov_sh = analysis_enriched.groupby('ticker')['shares_outstanding'].apply(lambda s: s.notna().any()).sum()\n",
    "cov_ff = analysis_enriched.groupby('ticker')['float_shares'].apply(lambda s: s.notna().any()).sum()\n",
    "print(\"Coverage now -> shares_outstanding:\", cov_sh, \"| float_shares:\", cov_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e088cbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yx/5t5dfjxj4pv8tjn2zvbfc2540000gn/T/ipykernel_4700/3485701038.py:86: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sec_enriched = (pd.concat([p for p in parts if not p.empty], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEC-enriched rows: 14067 | tickers: 256\n",
      "Coverage after SEC enrichment -> shares_outstanding: 3668 | float_shares: 2752\n"
     ]
    }
   ],
   "source": [
    "# === SEC enrichment from cache (per-ticker join; robust) ===\n",
    "from pathlib import Path\n",
    "import json, numpy as np, pandas as pd\n",
    "\n",
    "OUT = Path(\"outputs\")\n",
    "SEC_FACTS_DIR = OUT / \"sec_facts\"\n",
    "\n",
    "assert 'analysis_enriched' in globals(), \"Run your finalize/backfill cells first.\"\n",
    "assert SEC_FACTS_DIR.exists(), \"No sec_facts cache found. Run the SEC companyfacts step at least once.\"\n",
    "\n",
    "# 1) Helpers to read cached companyfacts and extract expanded shares/public float\n",
    "def _collect_any_units(facts, tax, name):\n",
    "    try:\n",
    "        units = facts['facts'][tax][name]['units']\n",
    "    except Exception:\n",
    "        return []\n",
    "    out = []\n",
    "    for unit, arr in (units or {}).items():\n",
    "        for d in arr or []:\n",
    "            end = d.get('end'); val = d.get('val')\n",
    "            if end and val is not None:\n",
    "                out.append({\n",
    "                    'date': pd.to_datetime(end).tz_localize(None).normalize(),\n",
    "                    'val' : float(val),\n",
    "                    'unit': unit,\n",
    "                    'form': d.get('form')\n",
    "                })\n",
    "    return out\n",
    "\n",
    "def extract_shares_expanded(facts_json: dict) -> pd.DataFrame:\n",
    "    keys = [\n",
    "        ('us-gaap',  'CommonStockSharesOutstanding'),\n",
    "        ('dei',      'EntityCommonStockSharesOutstanding'),\n",
    "        ('us-gaap',  'CommonStockSharesIssued'),\n",
    "        ('ifrs-full','NumberOfSharesOutstanding'),\n",
    "        ('ifrs-full','IssuedCapitalNumberOfShares'),\n",
    "        # weighted-average fallbacks (approximate):\n",
    "        ('us-gaap',  'WeightedAverageNumberOfSharesOutstandingBasic'),\n",
    "        ('us-gaap',  'WeightedAverageNumberOfDilutedSharesOutstanding'),\n",
    "        ('ifrs-full','WeightedAverageNumberOfOrdinarySharesOutstandingBasic'),\n",
    "        ('ifrs-full','WeightedAverageNumberOfOrdinarySharesOutstandingDiluted'),\n",
    "    ]\n",
    "    rows = []\n",
    "    for tax, name in keys:\n",
    "        rows += [r for r in _collect_any_units(facts_json, tax, name) if r['unit'] == 'shares']\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=['date','shares_outstanding'])\n",
    "    df = (pd.DataFrame(rows)\n",
    "            .sort_values('date')\n",
    "            .drop_duplicates('date', keep='last')\n",
    "            .rename(columns={'val':'shares_outstanding'})[['date','shares_outstanding']])\n",
    "    return df\n",
    "\n",
    "def extract_public_float_any_currency(facts_json: dict) -> pd.DataFrame:\n",
    "    rows = _collect_any_units(facts_json, 'dei', 'EntityPublicFloat')\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=['date','entity_public_float_val','entity_public_float_unit'])\n",
    "    df = (pd.DataFrame(rows)\n",
    "            .sort_values('date')\n",
    "            .drop_duplicates('date', keep='last')\n",
    "            .rename(columns={'val':'entity_public_float_val','unit':'entity_public_float_unit'})\n",
    "            [['date','entity_public_float_val','entity_public_float_unit']])\n",
    "    return df\n",
    "\n",
    "# 2) Build events from cached files only\n",
    "parts = []\n",
    "tk_to_cik = dict(zip(fundamentals['ticker'].astype(str), fundamentals['cik']))\n",
    "for tkr, cik in tk_to_cik.items():\n",
    "    if not isinstance(cik, str) or pd.isna(cik):\n",
    "        continue\n",
    "    fp = SEC_FACTS_DIR / f\"{str(cik).zfill(10)}.json\"\n",
    "    if not fp.exists():\n",
    "        continue\n",
    "    try:\n",
    "        facts = json.loads(fp.read_text())\n",
    "    except Exception:\n",
    "        continue\n",
    "    sh = extract_shares_expanded(facts)\n",
    "    pf = extract_public_float_any_currency(facts)\n",
    "    if sh.empty and pf.empty:\n",
    "        continue\n",
    "    ev = sh.merge(pf, on='date', how='outer').sort_values('date')\n",
    "    ev['ticker'] = str(tkr)\n",
    "    parts.append(ev[['date','ticker','shares_outstanding','entity_public_float_val','entity_public_float_unit']])\n",
    "\n",
    "sec_enriched = (pd.concat([p for p in parts if not p.empty], ignore_index=True)\n",
    "                if parts else pd.DataFrame(columns=['date','ticker','shares_outstanding','entity_public_float_val','entity_public_float_unit']))\n",
    "\n",
    "print(\"SEC-enriched rows:\", len(sec_enriched), \"| tickers:\", (sec_enriched['ticker'].nunique() if not sec_enriched.empty else 0))\n",
    "\n",
    "# 3) Find your adjusted-close column (supports 'adj close' or 'adj_close', etc.)\n",
    "adj_candidates = ['adj close','adj_close','Adj Close','adjClose']\n",
    "adj_col = next((c for c in adj_candidates if c in analysis_enriched.columns), None)\n",
    "if adj_col is None:\n",
    "    raise KeyError(\"Couldn't find an adjusted-close column in analysis_enriched. \"\n",
    "                   \"Looked for: \" + \", \".join(adj_candidates))\n",
    "\n",
    "price_ev = (analysis_enriched[['date','ticker',adj_col]]\n",
    "            .dropna()\n",
    "            .rename(columns={adj_col:'adj_close'})\n",
    "            .astype({'ticker':str})\n",
    "            .assign(date=lambda d: pd.to_datetime(d['date']).dt.tz_localize(None).dt.normalize())\n",
    "            .sort_values(['ticker','date']))\n",
    "\n",
    "# 4) Per-ticker asof join (robust to any sorting/categorical issues)\n",
    "sec_enriched = sec_enriched.assign(\n",
    "    ticker=lambda d: d['ticker'].astype(str),\n",
    "    date=lambda d: pd.to_datetime(d['date']).dt.tz_localize(None).dt.normalize()\n",
    ").dropna(subset=['date'])\n",
    "\n",
    "events_list = []\n",
    "for t, left_t in sec_enriched.groupby('ticker', sort=True):\n",
    "    left_t = left_t.sort_values('date')\n",
    "    right_t = price_ev[price_ev['ticker'] == t][['date','adj_close']]\n",
    "    if right_t.empty:\n",
    "        tmp = left_t.copy()\n",
    "        tmp['adj_close'] = np.nan\n",
    "    else:\n",
    "        tmp = pd.merge_asof(left_t, right_t.sort_values('date'),\n",
    "                            on='date', direction='backward', allow_exact_matches=True)\n",
    "    tmp['ticker'] = t\n",
    "    events_list.append(tmp)\n",
    "\n",
    "sec_enriched_px = pd.concat(events_list, ignore_index=True) if events_list else pd.DataFrame(columns=['ticker','date','adj_close'])\n",
    "\n",
    "# 5) Convert USD public float to shares, then merge into analysis_enriched (fill only gaps)\n",
    "sec_enriched_px['float_shares_from_usd'] = np.where(\n",
    "    (sec_enriched_px.get('entity_public_float_val').notna()) &\n",
    "    (sec_enriched_px.get('entity_public_float_unit') == 'USD') &\n",
    "    (sec_enriched_px['adj_close'].notna()) &\n",
    "    (sec_enriched_px['adj_close'] != 0),\n",
    "    sec_enriched_px['entity_public_float_val'] / sec_enriched_px['adj_close'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "merged = analysis_enriched.merge(\n",
    "    sec_enriched_px[['ticker','date','shares_outstanding','float_shares_from_usd']],\n",
    "    on=['ticker','date'], how='left', suffixes=('','_sec')\n",
    ")\n",
    "\n",
    "# Fill where missing\n",
    "if 'shares_outstanding_sec' in merged.columns:\n",
    "    mask_sh = merged['shares_outstanding'].isna() & merged['shares_outstanding_sec'].notna()\n",
    "    merged.loc[mask_sh, 'shares_outstanding'] = merged.loc[mask_sh, 'shares_outstanding_sec']\n",
    "    merged = merged.drop(columns=['shares_outstanding_sec'])\n",
    "\n",
    "if 'float_shares' not in merged.columns:\n",
    "    merged['float_shares'] = np.nan\n",
    "if 'float_shares_from_usd' in merged.columns:\n",
    "    mask_ff = merged['float_shares'].isna() & merged['float_shares_from_usd'].notna()\n",
    "    merged.loc[mask_ff, 'float_shares'] = merged.loc[mask_ff, 'float_shares_from_usd']\n",
    "    merged = merged.drop(columns=['float_shares_from_usd'])\n",
    "\n",
    "# Alias + forward fill within ticker\n",
    "merged['free_float'] = merged['float_shares']\n",
    "for c in ['shares_outstanding','float_shares','free_float']:\n",
    "    merged[c] = merged.groupby('ticker')[c].ffill()\n",
    "\n",
    "analysis_enriched = merged\n",
    "\n",
    "# 6) Save new artifact\n",
    "analysis_enriched.to_parquet(OUT / \"analysis_enriched_v8.parquet\")\n",
    "analysis_enriched.to_csv(OUT / \"analysis_enriched_v8.csv\", index=False)\n",
    "\n",
    "cov_sh = analysis_enriched.groupby('ticker')['shares_outstanding'].apply(lambda s: s.notna().any()).sum()\n",
    "cov_ff = analysis_enriched.groupby('ticker')['float_shares'].apply(lambda s: s.notna().any()).sum()\n",
    "print(\"Coverage after SEC enrichment -> shares_outstanding:\", cov_sh, \"| float_shares:\", cov_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ad860f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tickers missing float_shares: 2369\n",
      "[YF float] Applied cached snapshot (no network).\n"
     ]
    }
   ],
   "source": [
    "# === YF float-only: cache/NO_NETWORK apply (standalone safe) ===\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, json\n",
    "\n",
    "OUT = Path(\"outputs\")\n",
    "YF_CACHE = OUT / \"yf_snapshot_cache\"\n",
    "P_YQ_FLOAT_SNAP = OUT / \"yq_float_snapshot.parquet\"\n",
    "\n",
    "# Ensure flags exist (safe defaults)\n",
    "YQ_FLOAT_CACHE_ONLY = globals().get('YQ_FLOAT_CACHE_ONLY', False)\n",
    "NO_NETWORK          = globals().get('NO_NETWORK', False)\n",
    "\n",
    "# Recompute who is missing float_shares\n",
    "need_float = (analysis_enriched.groupby('ticker')['float_shares']\n",
    "                .apply(lambda s: s.notna().any())\n",
    "                .pipe(lambda s: sorted(s.index[~s].tolist())))\n",
    "print(\"Tickers missing float_shares:\", len(need_float))\n",
    "\n",
    "if not need_float:\n",
    "    print(\"Skipping YF float-only backfill (nothing missing).\")\n",
    "\n",
    "elif YQ_FLOAT_CACHE_ONLY or NO_NETWORK:\n",
    "    # --- helpers ---\n",
    "    def _read_snap():\n",
    "        \"\"\"Load consolidated snapshot if present; return DataFrame or empty.\"\"\"\n",
    "        if not P_YQ_FLOAT_SNAP.exists():\n",
    "            return pd.DataFrame()\n",
    "        try:\n",
    "            df = pd.read_parquet(P_YQ_FLOAT_SNAP)\n",
    "        except Exception:\n",
    "            return pd.DataFrame()\n",
    "        if 'ticker' not in df.columns:  # ticker stored as index? move to column\n",
    "            df = df.reset_index().rename(columns={'index': 'ticker', 'symbol': 'ticker'})\n",
    "        df['ticker'] = df['ticker'].astype(str).str.upper()\n",
    "        return df\n",
    "\n",
    "    def _rebuild_snap_from_cache():\n",
    "        \"\"\"Build snapshot from local per-ticker JSON files only (no network).\"\"\"\n",
    "        rows = []\n",
    "        if YF_CACHE.exists():\n",
    "            for p in YF_CACHE.glob(\"*.json\"):\n",
    "                try:\n",
    "                    j = json.loads(p.read_text())\n",
    "                    tkr = p.stem.upper()\n",
    "                    # Accept any plausible key name and normalize to float_shares_yf\n",
    "                    v = j.get('float_shares_yf')\n",
    "                    if v is None: v = j.get('float_shares')\n",
    "                    if v is None: v = j.get('floatShares')\n",
    "                    if v is not None:\n",
    "                        rows.append({'ticker': tkr, 'float_shares_yf': float(v)})\n",
    "                except Exception:\n",
    "                    pass\n",
    "        snap = pd.DataFrame(rows)\n",
    "        if not snap.empty:\n",
    "            snap = snap.drop_duplicates('ticker', keep='last')\n",
    "            try: snap.to_parquet(P_YQ_FLOAT_SNAP)\n",
    "            except Exception: pass\n",
    "            print(f\"[YF float] Rebuilt snapshot from cache: {len(snap)} rows.\")\n",
    "        else:\n",
    "            print(\"[YF float] No snapshot and no cache contents — nothing to apply.\")\n",
    "        return snap\n",
    "\n",
    "    # Try to read snapshot; if empty, rebuild from local cache\n",
    "    snap = _read_snap()\n",
    "    if snap.empty:\n",
    "        snap = _rebuild_snap_from_cache()\n",
    "\n",
    "    if not snap.empty:\n",
    "        # Find usable float column\n",
    "        float_col = next((c for c in ['float_shares_yf', 'float_shares', 'free_float', 'floatShares']\n",
    "                          if c in snap.columns), None)\n",
    "        if float_col is None:\n",
    "            print(\"[YF float] Snapshot lacks a usable float column; skipping apply.\")\n",
    "        else:\n",
    "            mapper = (snap.dropna(subset=[float_col])\n",
    "                        .drop_duplicates('ticker', keep='last')\n",
    "                        .set_index('ticker')[float_col].astype(float).to_dict())\n",
    "\n",
    "            m = analysis_enriched['float_shares'].isna()\n",
    "            analysis_enriched.loc[m, 'float_shares'] = analysis_enriched.loc[m, 'ticker'].map(mapper)\n",
    "\n",
    "            if 'free_float' not in analysis_enriched.columns:\n",
    "                analysis_enriched['free_float'] = np.nan\n",
    "            analysis_enriched['free_float'] = analysis_enriched['free_float'].fillna(analysis_enriched['float_shares'])\n",
    "\n",
    "            for c in ['float_shares', 'free_float']:\n",
    "                analysis_enriched[c] = pd.to_numeric(analysis_enriched[c], errors='coerce')\n",
    "                analysis_enriched[c] = analysis_enriched.groupby('ticker')[c].ffill()\n",
    "\n",
    "            print(\"[YF float] Applied cached snapshot (no network).\")\n",
    "    # else: nothing to do (message already printed)\n",
    "\n",
    "else:\n",
    "    print(\"Cache-only flags are off; run your networked YF backfill cell instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "96ede22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage after FX float enrichment -> shares_outstanding: 3668 | float_shares: 3966\n"
     ]
    }
   ],
   "source": [
    "# === T2b (robust): FX-aware SEC public float -> float_shares (per-currency joins) ===\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, json, re\n",
    "import yfinance as yf\n",
    "\n",
    "OUT = Path(\"outputs\")\n",
    "SEC_FACTS_DIR = OUT / \"sec_facts\"\n",
    "assert SEC_FACTS_DIR.exists(), \"Missing outputs/sec_facts; run the SEC companyfacts step first.\"\n",
    "assert 'analysis_enriched' in globals(), \"analysis_enriched not found. Run finalize/backfill first.\"\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def _collect_any_units(facts, tax, name):\n",
    "    try:\n",
    "        units = facts['facts'][tax][name]['units']\n",
    "    except Exception:\n",
    "        return []\n",
    "    out = []\n",
    "    for unit, arr in (units or {}).items():\n",
    "        for d in arr or []:\n",
    "            end = d.get('end'); val = d.get('val')\n",
    "            if end and val is not None:\n",
    "                out.append({\n",
    "                    'date': pd.to_datetime(end).tz_localize(None).normalize(),\n",
    "                    'val' : float(val),\n",
    "                    'unit': str(unit).upper()\n",
    "                })\n",
    "    return out\n",
    "\n",
    "def extract_public_float_any_currency(facts_json: dict) -> pd.DataFrame:\n",
    "    rows = _collect_any_units(facts_json, 'dei', 'EntityPublicFloat')\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=['date','entity_public_float_val','entity_public_float_unit'])\n",
    "    df = (pd.DataFrame(rows)\n",
    "            .sort_values('date')\n",
    "            .drop_duplicates('date', keep='last')\n",
    "            .rename(columns={'val':'entity_public_float_val','unit':'entity_public_float_unit'})\n",
    "            [['date','entity_public_float_val','entity_public_float_unit']])\n",
    "    return df\n",
    "\n",
    "def get_fx_usd_series(cur: str, start: pd.Timestamp, end: pd.Timestamp) -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Return a daily series with columns ['date','fx_usd'] representing USD per 1 CUR.\n",
    "    Tries 'CURUSD=X' first; if missing, tries 'USDCUR=X' and inverts.\n",
    "    \"\"\"\n",
    "    cur = cur.upper()\n",
    "    pairs = [(f\"{cur}USD=X\", False), (f\"USD{cur}=X\", True)]  # (symbol, invert)\n",
    "    for sym, invert in pairs:\n",
    "        try:\n",
    "            h = yf.download(\n",
    "                tickers=sym,\n",
    "                start=start.date().isoformat(),\n",
    "                end=(end + pd.Timedelta(days=1)).date().isoformat(),\n",
    "                interval=\"1d\",\n",
    "                auto_adjust=True,\n",
    "                progress=False\n",
    "            )\n",
    "        except Exception:\n",
    "            h = None\n",
    "        if h is None or h.empty:\n",
    "            continue\n",
    "\n",
    "        if isinstance(h.columns, pd.MultiIndex):\n",
    "            # multi-index (Close under sym)\n",
    "            if 'Close' in h.columns.get_level_values(-1):\n",
    "                sub = h.xs('Close', axis=1, level=-1).iloc[:, 0].to_frame('fx')\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            # single-index\n",
    "            if 'Close' not in h.columns:\n",
    "                continue\n",
    "            sub = h[['Close']].rename(columns={'Close':'fx'})\n",
    "\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        sub = sub.reset_index().rename(columns={'Date':'date'})\n",
    "        sub['date'] = pd.to_datetime(sub['date']).dt.tz_localize(None).dt.normalize()\n",
    "        fx = sub['fx'].astype(float)\n",
    "        if invert:\n",
    "            # sym is USD/CUR -> convert to USD per 1 CUR\n",
    "            fx = 1.0 / fx.replace(0, np.nan)\n",
    "        out = pd.DataFrame({'date': sub['date'], 'fx_usd': fx})\n",
    "        out = out.dropna(subset=['fx_usd']).sort_values('date').drop_duplicates('date', keep='last')\n",
    "        if not out.empty:\n",
    "            return out\n",
    "    return None\n",
    "\n",
    "# ---------- rebuild SEC public-float table from cache (fast, no network) ----------\n",
    "parts = []\n",
    "tk_to_cik = dict(zip(fundamentals['ticker'].astype(str), fundamentals['cik']))\n",
    "for tkr, cik in tk_to_cik.items():\n",
    "    if not isinstance(cik, str) or pd.isna(cik):\n",
    "        continue\n",
    "    fp = SEC_FACTS_DIR / f\"{str(cik).zfill(10)}.json\"\n",
    "    if not fp.exists():\n",
    "        continue\n",
    "    try:\n",
    "        facts = json.loads(fp.read_text())\n",
    "    except Exception:\n",
    "        continue\n",
    "    pf = extract_public_float_any_currency(facts)\n",
    "    if pf.empty:\n",
    "        continue\n",
    "    pf['ticker'] = str(tkr)\n",
    "    parts.append(pf)\n",
    "\n",
    "sec_float = (pd.concat(parts, ignore_index=True)\n",
    "             if parts else pd.DataFrame(columns=['date','entity_public_float_val','entity_public_float_unit','ticker']))\n",
    "\n",
    "if sec_float.empty:\n",
    "    print(\"No SEC public-float facts found; nothing to do.\")\n",
    "else:\n",
    "    # keep only non-USD floats with sane 3-letter ISO unit codes\n",
    "    sec_float = sec_float.assign(\n",
    "        ticker=lambda d: d['ticker'].astype(str),\n",
    "        date=lambda d: pd.to_datetime(d['date']).dt.tz_localize(None).dt.normalize(),\n",
    "        unit=lambda d: d['entity_public_float_unit'].astype(str).str.upper()\n",
    "    )\n",
    "    non_usd = sec_float[(sec_float['unit'] != 'USD') & sec_float['unit'].str.fullmatch(r'[A-Z]{3}')].copy()\n",
    "\n",
    "    if non_usd.empty:\n",
    "        print(\"All SEC floats are USD; nothing to add.\")\n",
    "    else:\n",
    "        # date window for FX\n",
    "        start = non_usd['date'].min() - pd.Timedelta(days=5)\n",
    "        end   = non_usd['date'].max() + pd.Timedelta(days=2)\n",
    "\n",
    "        # build FX joined table per-currency (avoids asof sorting errors with 'by=')\n",
    "        fx_joined = []\n",
    "        for cur, grp in non_usd.groupby('unit', sort=True):\n",
    "            fx = get_fx_usd_series(cur, start, end)\n",
    "            g  = grp[['date','ticker','entity_public_float_val','unit']].sort_values('date').copy()\n",
    "            if fx is None or fx.empty:\n",
    "                g['fx_usd'] = np.nan\n",
    "            else:\n",
    "                g = pd.merge_asof(g, fx, on='date', direction='backward', allow_exact_matches=True)\n",
    "            g['unit'] = cur\n",
    "            fx_joined.append(g)\n",
    "        non_usd_px = (pd.concat(fx_joined, ignore_index=True)\n",
    "                      if fx_joined else pd.DataFrame(columns=['date','ticker','entity_public_float_val','unit','fx_usd']))\n",
    "\n",
    "        # adjusted-close column name\n",
    "        adj_candidates = ['adj close','adj_close','Adj Close','adjClose']\n",
    "        adj_col = next((c for c in adj_candidates if c in analysis_enriched.columns), None)\n",
    "        if adj_col is None:\n",
    "            raise KeyError(\"Couldn't find an adjusted-close column in analysis_enriched.\")\n",
    "\n",
    "        price_ev = (analysis_enriched[['date','ticker',adj_col]]\n",
    "                    .dropna()\n",
    "                    .rename(columns={adj_col:'adj_close'})\n",
    "                    .astype({'ticker':str})\n",
    "                    .assign(date=lambda d: pd.to_datetime(d['date']).dt.tz_localize(None).dt.normalize())\n",
    "                    .sort_values(['ticker','date']))\n",
    "\n",
    "        # per-ticker asof join to price\n",
    "        rows = []\n",
    "        for t, grp in non_usd_px.groupby('ticker', sort=True):\n",
    "            left_t = grp.sort_values('date')\n",
    "            right_t = price_ev[price_ev['ticker'] == t][['date','adj_close']].sort_values('date')\n",
    "            if right_t.empty:\n",
    "                tmp = left_t.copy(); tmp['adj_close'] = np.nan\n",
    "            else:\n",
    "                tmp = pd.merge_asof(left_t, right_t, on='date', direction='backward', allow_exact_matches=True)\n",
    "            tmp['ticker'] = t\n",
    "            rows.append(tmp)\n",
    "        non_usd_full = (pd.concat(rows, ignore_index=True)\n",
    "                        if rows else pd.DataFrame(columns=['ticker','date','entity_public_float_val','fx_usd','adj_close']))\n",
    "\n",
    "        # convert: (float_value_in_unit * USD_per_unit) / price\n",
    "        non_usd_full['float_shares_from_fx'] = np.where(\n",
    "            non_usd_full['entity_public_float_val'].notna() &\n",
    "            non_usd_full['fx_usd'].notna() &\n",
    "            non_usd_full['adj_close'].notna() &\n",
    "            (non_usd_full['adj_close'] != 0),\n",
    "            (non_usd_full['entity_public_float_val'] * non_usd_full['fx_usd']) / non_usd_full['adj_close'],\n",
    "            np.nan\n",
    "        )\n",
    "\n",
    "        # merge into analysis_enriched (fill only gaps) and ffill by ticker\n",
    "        merged = analysis_enriched.merge(\n",
    "            non_usd_full[['ticker','date','float_shares_from_fx']],\n",
    "            on=['ticker','date'], how='left', suffixes=('','_fx')\n",
    "        )\n",
    "        if 'float_shares' not in merged.columns:\n",
    "            merged['float_shares'] = np.nan\n",
    "        mask_ff = merged['float_shares'].isna() & merged['float_shares_from_fx'].notna()\n",
    "        merged.loc[mask_ff, 'float_shares'] = merged.loc[mask_ff, 'float_shares_from_fx']\n",
    "        merged = merged.drop(columns=['float_shares_from_fx'])\n",
    "\n",
    "        merged['free_float'] = merged['float_shares']\n",
    "        for c in ['shares_outstanding','float_shares','free_float']:\n",
    "            merged[c] = merged.groupby('ticker')[c].ffill()\n",
    "\n",
    "        analysis_enriched = merged\n",
    "\n",
    "        # save as v9\n",
    "        analysis_enriched.to_parquet(OUT / \"analysis_enriched_v9.parquet\")\n",
    "        analysis_enriched.to_csv(OUT / \"analysis_enriched_v9.csv\", index=False)\n",
    "\n",
    "        cov_sh = analysis_enriched.groupby('ticker')['shares_outstanding'].apply(lambda s: s.notna().any()).sum()\n",
    "        cov_ff = analysis_enriched.groupby('ticker')['float_shares'].apply(lambda s: s.notna().any()).sum()\n",
    "        print(\"Coverage after FX float enrichment -> shares_outstanding:\", cov_sh, \"| float_shares:\", cov_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8392e1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No us-gaap:PublicFloat found; nothing to add.\n"
     ]
    }
   ],
   "source": [
    "# === T2c: SEC us-gaap:PublicFloat -> float_shares (fills only where missing) ===\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, json\n",
    "\n",
    "OUT = Path(\"outputs\")\n",
    "SEC_FACTS_DIR = OUT / \"sec_facts\"\n",
    "assert SEC_FACTS_DIR.exists(), \"Missing outputs/sec_facts; run the SEC companyfacts step first.\"\n",
    "assert 'analysis_enriched' in globals(), \"analysis_enriched not found.\"\n",
    "\n",
    "def _collect_any_units(facts, tax, name):\n",
    "    try:\n",
    "        units = facts['facts'][tax][name]['units']\n",
    "    except Exception:\n",
    "        return []\n",
    "    out = []\n",
    "    for unit, arr in (units or {}).items():\n",
    "        for d in arr or []:\n",
    "            end = d.get('end'); val = d.get('val')\n",
    "            if end and val is not None:\n",
    "                out.append({\n",
    "                    'date': pd.to_datetime(end).tz_localize(None).normalize(),\n",
    "                    'val' : float(val),\n",
    "                    'unit': str(unit).upper()\n",
    "                })\n",
    "    return out\n",
    "\n",
    "def extract_public_float_usgaap_any_currency(facts_json: dict) -> pd.DataFrame:\n",
    "    rows = _collect_any_units(facts_json, 'us-gaap', 'PublicFloat')\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=['date','public_float_val','public_float_unit'])\n",
    "    df = (pd.DataFrame(rows)\n",
    "            .sort_values('date')\n",
    "            .drop_duplicates('date', keep='last')\n",
    "            .rename(columns={'val':'public_float_val','unit':'public_float_unit'})\n",
    "            [['date','public_float_val','public_float_unit']])\n",
    "    return df\n",
    "\n",
    "# Build from cache only (fast)\n",
    "parts = []\n",
    "tk_to_cik = dict(zip(fundamentals['ticker'].astype(str), fundamentals['cik']))\n",
    "for tkr, cik in tk_to_cik.items():\n",
    "    if not isinstance(cik, str) or pd.isna(cik): \n",
    "        continue\n",
    "    fp = SEC_FACTS_DIR / f\"{str(cik).zfill(10)}.json\"\n",
    "    if not fp.exists(): \n",
    "        continue\n",
    "    try:\n",
    "        facts = json.loads(fp.read_text())\n",
    "    except Exception:\n",
    "        continue\n",
    "    pf = extract_public_float_usgaap_any_currency(facts)\n",
    "    if pf.empty:\n",
    "        continue\n",
    "    pf['ticker'] = str(tkr)\n",
    "    parts.append(pf)\n",
    "\n",
    "usgaap_float = (pd.concat(parts, ignore_index=True)\n",
    "                if parts else pd.DataFrame(columns=['date','public_float_val','public_float_unit','ticker']))\n",
    "\n",
    "if usgaap_float.empty:\n",
    "    print(\"No us-gaap:PublicFloat found; nothing to add.\")\n",
    "else:\n",
    "    # We already handled USD & FX for dei:EntityPublicFloat.\n",
    "    # Use the SAME conversion logic: value in USD / adj_close => shares.\n",
    "    # First identify the adjusted-close column\n",
    "    adj_candidates = ['adj close','adj_close','Adj Close','adjClose']\n",
    "    adj_col = next((c for c in adj_candidates if c in analysis_enriched.columns), None)\n",
    "    if adj_col is None:\n",
    "        raise KeyError(\"Couldn't find an adjusted-close column in analysis_enriched.\")\n",
    "\n",
    "    usgaap_float = (usgaap_float\n",
    "        .assign(\n",
    "            ticker=lambda d: d['ticker'].astype(str),\n",
    "            date=lambda d: pd.to_datetime(d['date']).dt.tz_localize(None).dt.normalize(),\n",
    "            unit=lambda d: d['public_float_unit'].astype(str).upper()\n",
    "        )\n",
    "        .sort_values(['ticker','date'])\n",
    "    )\n",
    "\n",
    "    # USD-only conversion here; (optional) you could port the FX step to us-gaap too.\n",
    "    usd_only = usgaap_float[usgaap_float['unit'] == 'USD'].copy()\n",
    "    price_ev = (analysis_enriched[['date','ticker',adj_col]]\n",
    "                .dropna()\n",
    "                .rename(columns={adj_col:'adj_close'})\n",
    "                .astype({'ticker':str})\n",
    "                .assign(date=lambda d: pd.to_datetime(d['date']).dt.tz_localize(None).dt.normalize())\n",
    "                .sort_values(['ticker','date']))\n",
    "\n",
    "    # per-ticker asof join for robustness\n",
    "    rows = []\n",
    "    for t, grp in usd_only.groupby('ticker', sort=True):\n",
    "        left_t = grp.sort_values('date')\n",
    "        right_t = price_ev[price_ev['ticker']==t][['date','adj_close']].sort_values('date')\n",
    "        if right_t.empty:\n",
    "            tmp = left_t.copy(); tmp['adj_close'] = np.nan\n",
    "        else:\n",
    "            tmp = pd.merge_asof(left_t, right_t, on='date', direction='backward', allow_exact_matches=True)\n",
    "        tmp['ticker'] = t\n",
    "        rows.append(tmp)\n",
    "    usd_join = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=['ticker','date','public_float_val','adj_close'])\n",
    "\n",
    "    usd_join['float_shares_from_usgaap'] = np.where(\n",
    "        usd_join['public_float_val'].notna() &\n",
    "        usd_join['adj_close'].notna() &\n",
    "        (usd_join['adj_close'] != 0),\n",
    "        usd_join['public_float_val'] / usd_join['adj_close'],\n",
    "        np.nan\n",
    "    )\n",
    "\n",
    "    merged = analysis_enriched.merge(\n",
    "        usd_join[['ticker','date','float_shares_from_usgaap']],\n",
    "        on=['ticker','date'], how='left', suffixes=('','_usgaap')\n",
    "    )\n",
    "    if 'float_shares' not in merged.columns:\n",
    "        merged['float_shares'] = np.nan\n",
    "    mask_ff = merged['float_shares'].isna() & merged['float_shares_from_usgaap'].notna()\n",
    "    merged.loc[mask_ff, 'float_shares'] = merged.loc[mask_ff, 'float_shares_from_usgaap']\n",
    "    merged = merged.drop(columns=['float_shares_from_usgaap'])\n",
    "\n",
    "    merged['free_float'] = merged['float_shares']\n",
    "    for c in ['shares_outstanding','float_shares','free_float']:\n",
    "        merged[c] = merged.groupby('ticker')[c].ffill()\n",
    "\n",
    "    analysis_enriched = merged\n",
    "    analysis_enriched.to_parquet(OUT / \"analysis_enriched_v10.parquet\")\n",
    "    analysis_enriched.to_csv(OUT / \"analysis_enriched_v10.csv\", index=False)\n",
    "\n",
    "    cov_sh = analysis_enriched.groupby('ticker')['shares_outstanding'].apply(lambda s: s.notna().any()).sum()\n",
    "    cov_ff = analysis_enriched.groupby('ticker')['float_shares'].apply(lambda s: s.notna().any()).sum()\n",
    "    print(\"Coverage after us-gaap PublicFloat -> shares_outstanding:\", cov_sh, \"| float_shares:\", cov_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "00ffece6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[yq-float] loaded snapshot: 1498 rows from yq_float_snapshot.parquet\n",
      "Coverage after T3r -> shares_outstanding: 3668 | float_shares: 3966\n"
     ]
    }
   ],
   "source": [
    "# === T3r (robust + cached-first): normalize & apply Yahoo float snapshot ===\n",
    "import time, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Optional: yahooquery import (not used for network here, but kept for completeness)\n",
    "try:\n",
    "    from yahooquery import Ticker as YQ\n",
    "except Exception:\n",
    "    YQ = None\n",
    "\n",
    "assert 'analysis_enriched' in globals(), \"analysis_enriched not found. Run the finalize/backfill first.\"\n",
    "\n",
    "# Paths & flags\n",
    "OUT = Path(\"outputs\"); OUT.mkdir(parents=True, exist_ok=True)\n",
    "P_YQ_FLOAT_SNAP = OUT / \"yq_float_snapshot.parquet\"\n",
    "YQ_FLOAT_CACHE_ONLY = globals().get('YQ_FLOAT_CACHE_ONLY', False)\n",
    "REBUILD_YQ_FLOAT   = globals().get('REBUILD_YQ_FLOAT', False)\n",
    "\n",
    "# Ensure columns exist\n",
    "for col in ['shares_outstanding', 'float_shares', 'free_float']:\n",
    "    if col not in analysis_enriched.columns:\n",
    "        analysis_enriched[col] = pd.NA\n",
    "\n",
    "# Helper: normalize any snapshot schema to ['ticker','float_shares_yq']\n",
    "def _normalize_snapshot(df: pd.DataFrame, ae: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame(columns=['ticker','float_shares_yq'])\n",
    "    # If ticker is index, move it to a column\n",
    "    if 'ticker' not in df.columns:\n",
    "        df = df.reset_index().rename(columns={'index':'ticker', 'symbol':'ticker'})\n",
    "    df['ticker'] = df['ticker'].astype(str).str.upper()\n",
    "\n",
    "    # 1) If it already has 'float_shares_yq', use it\n",
    "    if 'float_shares_yq' in df.columns:\n",
    "        out = df[['ticker','float_shares_yq']].copy()\n",
    "        out['float_shares_yq'] = pd.to_numeric(out['float_shares_yq'], errors='coerce')\n",
    "        return out.dropna(subset=['float_shares_yq']).drop_duplicates('ticker', keep='last')\n",
    "\n",
    "    # 2) If it has another usable shares column, adopt it\n",
    "    for alt in ['float_shares', 'free_float', 'floatShares', 'float_shares_yf']:\n",
    "        if alt in df.columns:\n",
    "            out = df[['ticker', alt]].copy().rename(columns={alt:'float_shares_yq'})\n",
    "            out['float_shares_yq'] = pd.to_numeric(out['float_shares_yq'], errors='coerce')\n",
    "            return out.dropna(subset=['float_shares_yq']).drop_duplicates('ticker', keep='last')\n",
    "\n",
    "    # 3) If it only has float_percent, derive shares = float_percent * latest shares_outstanding\n",
    "    if 'float_percent' in df.columns:\n",
    "        tmp = df[['ticker','float_percent']].copy()\n",
    "        tmp['float_percent'] = pd.to_numeric(tmp['float_percent'], errors='coerce')\n",
    "        tmp = tmp.dropna(subset=['float_percent']).drop_duplicates('ticker', keep='last')\n",
    "\n",
    "        # latest non-null shares_outstanding per ticker from analysis_enriched\n",
    "        sh_last = (\n",
    "            ae.dropna(subset=['shares_outstanding'])\n",
    "              .sort_values(['ticker','date'])\n",
    "              .groupby('ticker', as_index=False)\n",
    "              .tail(1)[['ticker','shares_outstanding']]\n",
    "              .assign(shares_outstanding=lambda d: pd.to_numeric(d['shares_outstanding'], errors='coerce'))\n",
    "        )\n",
    "        joined = tmp.merge(sh_last, on='ticker', how='left')\n",
    "        joined['float_shares_yq'] = joined['float_percent'] * joined['shares_outstanding']\n",
    "        out = joined[['ticker','float_shares_yq']]\n",
    "        return out.dropna(subset=['float_shares_yq']).drop_duplicates('ticker', keep='last')\n",
    "\n",
    "    # Nothing usable\n",
    "    return pd.DataFrame(columns=['ticker','float_shares_yq'])\n",
    "\n",
    "# ----------------- LOAD OR BUILD SNAPSHOT (cache-first, no network here) -----------------\n",
    "if (not REBUILD_YQ_FLOAT) and P_YQ_FLOAT_SNAP.exists():\n",
    "    raw_snap = pd.read_parquet(P_YQ_FLOAT_SNAP)\n",
    "    print(f\"[yq-float] loaded snapshot: {len(raw_snap)} rows from {P_YQ_FLOAT_SNAP.name}\")\n",
    "else:\n",
    "    # If you previously saved a snapshot under a different schema or it's missing,\n",
    "    # we just start with an empty frame; other cells can rebuild from local JSON cache.\n",
    "    raw_snap = pd.DataFrame()\n",
    "    print(\"[yq-float] no usable snapshot found (or REBUILD_YQ_FLOAT=True). Proceeding with empty snapshot.\")\n",
    "\n",
    "# Normalize whatever we have\n",
    "snap = _normalize_snapshot(raw_snap, analysis_enriched)\n",
    "\n",
    "# ----------------- APPLY TO ANALYSIS -----------------\n",
    "if not snap.empty:\n",
    "    snap_map = snap.set_index('ticker')['float_shares_yq'].to_dict()\n",
    "    mask_ff = analysis_enriched['float_shares'].isna()\n",
    "    analysis_enriched.loc[mask_ff, 'float_shares'] = analysis_enriched.loc[mask_ff, 'ticker'].map(snap_map)\n",
    "\n",
    "# Keep free_float in sync and forward-fill within ticker\n",
    "if 'free_float' not in analysis_enriched.columns:\n",
    "    analysis_enriched['free_float'] = np.nan\n",
    "analysis_enriched['free_float'] = analysis_enriched['free_float'].fillna(analysis_enriched['float_shares'])\n",
    "\n",
    "for c in ['shares_outstanding','float_shares','free_float']:\n",
    "    analysis_enriched[c] = pd.to_numeric(analysis_enriched[c], errors='coerce')\n",
    "    analysis_enriched[c] = analysis_enriched.groupby('ticker')[c].ffill()\n",
    "\n",
    "# Coverage + save\n",
    "cov_sh = analysis_enriched.groupby('ticker')['shares_outstanding'].apply(lambda s: s.notna().any()).sum()\n",
    "cov_ff = analysis_enriched.groupby('ticker')['float_shares'].apply(lambda s: s.notna().any()).sum()\n",
    "print(\"Coverage after T3r -> shares_outstanding:\", cov_sh, \"| float_shares:\", cov_ff)\n",
    "\n",
    "analysis_enriched.to_parquet(OUT / \"analysis_enriched_v11.parquet\")\n",
    "analysis_enriched.to_csv(OUT / \"analysis_enriched_v11.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "29fc1c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yx/5t5dfjxj4pv8tjn2zvbfc2540000gn/T/ipykernel_4700/2765554751.py:26: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(out['ticker']):\n",
      "/var/folders/yx/5t5dfjxj4pv8tjn2zvbfc2540000gn/T/ipykernel_4700/2765554751.py:26: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(out['ticker']):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL after float resolver:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>shares_outstanding</th>\n",
       "      <th>float_shares</th>\n",
       "      <th>free_float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12016</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12017</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12018</th>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12019</th>\n",
       "      <td>2022-10-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12020</th>\n",
       "      <td>2022-10-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12023</th>\n",
       "      <td>2022-10-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12024</th>\n",
       "      <td>2022-10-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12025</th>\n",
       "      <td>2022-10-14</td>\n",
       "      <td>1.590812e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12026</th>\n",
       "      <td>2022-10-17</td>\n",
       "      <td>1.590812e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12027</th>\n",
       "      <td>2022-10-18</td>\n",
       "      <td>1.590812e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  shares_outstanding  float_shares    free_float\n",
       "12016 2022-10-03                 NaN  1.481427e+10  1.481427e+10\n",
       "12017 2022-10-04                 NaN  1.481427e+10  1.481427e+10\n",
       "12018 2022-10-05                 NaN  1.481427e+10  1.481427e+10\n",
       "12019 2022-10-06                 NaN  1.481427e+10  1.481427e+10\n",
       "12020 2022-10-07                 NaN  1.481427e+10  1.481427e+10\n",
       "...          ...                 ...           ...           ...\n",
       "12023 2022-10-12                 NaN  1.481427e+10  1.481427e+10\n",
       "12024 2022-10-13                 NaN  1.481427e+10  1.481427e+10\n",
       "12025 2022-10-14        1.590812e+10  1.481427e+10  1.481427e+10\n",
       "12026 2022-10-17        1.590812e+10  1.481427e+10  1.481427e+10\n",
       "12027 2022-10-18        1.590812e+10  1.481427e+10  1.481427e+10\n",
       "\n",
       "[12 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage -> shares_outstanding: 3668 | float_shares: 3966\n"
     ]
    }
   ],
   "source": [
    "# === Float resolver (vectorized + snapshot-first; robust sort & fallback) ===\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, json\n",
    "\n",
    "OUT = Path(\"outputs\")\n",
    "YF_CACHE = OUT / \"yf_snapshot_cache\"\n",
    "P_YQ_FLOAT_SNAP = OUT / \"yq_float_snapshot.parquet\"\n",
    "\n",
    "def _prep_events(df, colname):\n",
    "    if df is None or df.empty or colname not in df.columns:\n",
    "        return pd.DataFrame(columns=['ticker','date','value'])\n",
    "    out = (\n",
    "        df[['ticker','date',colname]]\n",
    "        .rename(columns={colname:'value'})\n",
    "        .dropna(subset=['value'])\n",
    "        .assign(\n",
    "            ticker=lambda d: d['ticker'].astype(str).str.upper(),\n",
    "            date=lambda d: pd.to_datetime(d['date'], errors='coerce')\n",
    "                              .dt.tz_localize(None).dt.normalize(),\n",
    "        )\n",
    "        .dropna(subset=['date'])\n",
    "        .sort_values(['ticker','date'], kind='mergesort')\n",
    "        .drop_duplicates(['ticker','date'], keep='last')\n",
    "    )\n",
    "    # avoid categorical dtypes\n",
    "    if pd.api.types.is_categorical_dtype(out['ticker']):\n",
    "        out['ticker'] = out['ticker'].astype(str)\n",
    "    return out\n",
    "\n",
    "# 1) Build a single time-varying “float events” table from everything we already computed\n",
    "sources = []\n",
    "if 'events_enriched' in globals():  sources.append(_prep_events(events_enriched, 'float_shares'))\n",
    "if 'sec_enriched_px' in globals():  sources.append(_prep_events(sec_enriched_px, 'float_shares_from_usd'))\n",
    "if 'fx_float_enriched' in globals(): sources.append(_prep_events(fx_float_enriched, 'float_shares_fx'))\n",
    "\n",
    "float_events = (\n",
    "    pd.concat([s for s in sources if not s.empty], ignore_index=True)\n",
    "    if any(not s.empty for s in sources) else\n",
    "    pd.DataFrame(columns=['ticker','date','value'])\n",
    ")\n",
    "\n",
    "# 2) Prep analysis_enriched (only rows that actually need filling)\n",
    "ae = analysis_enriched.copy()\n",
    "ae['ticker'] = ae['ticker'].astype(str).str.upper()\n",
    "ae['date']   = pd.to_datetime(ae['date'], errors='coerce').dt.tz_localize(None).dt.normalize()\n",
    "ae = ae.dropna(subset=['date'])\n",
    "if 'float_shares' not in ae.columns: ae['float_shares'] = np.nan\n",
    "\n",
    "need_mask = ae['float_shares'].isna()\n",
    "left = (ae.loc[need_mask, ['ticker','date']]\n",
    "          .astype({'ticker':'string'})\n",
    "          .sort_values(['ticker','date'], kind='mergesort'))\n",
    "\n",
    "# 3) Vectorized as-of merge across ALL tickers, with defensive sorting and dtype alignment\n",
    "if not float_events.empty and not left.empty:\n",
    "    right = (float_events.astype({'ticker':'string'})\n",
    "                        .sort_values(['ticker','date'], kind='mergesort'))\n",
    "    try:\n",
    "        tmp = pd.merge_asof(\n",
    "            left=left, right=right,\n",
    "            by='ticker', on='date',\n",
    "            direction='backward', allow_exact_matches=True\n",
    "        )\n",
    "        ae.loc[left.index, '__float_ev'] = tmp['value'].to_numpy()\n",
    "    except ValueError:\n",
    "        # Fallback: only for the few tickers that need it (keeps it fast)\n",
    "        ae['__float_ev'] = np.nan\n",
    "        for t in left['ticker'].unique():\n",
    "            L = left[left['ticker'] == t]\n",
    "            R = right[right['ticker'] == t]\n",
    "            if R.empty:\n",
    "                continue\n",
    "            # both sides already sorted; do a vectorized searchsorted\n",
    "            ldates = L['date'].to_numpy()\n",
    "            rdates = R['date'].to_numpy()\n",
    "            rval   = R['value'].to_numpy()\n",
    "            idx = np.searchsorted(rdates, ldates, side='right') - 1\n",
    "            vals = np.where(idx >= 0, rval[idx], np.nan)\n",
    "            ae.loc[L.index, '__float_ev'] = vals\n",
    "else:\n",
    "    ae['__float_ev'] = np.nan\n",
    "\n",
    "# Apply event values where missing\n",
    "mask_ev = ae['float_shares'].isna() & ae['__float_ev'].notna()\n",
    "ae.loc[mask_ev, 'float_shares'] = ae.loc[mask_ev, '__float_ev']\n",
    "ae.drop(columns=['__float_ev'], inplace=True, errors='ignore')\n",
    "\n",
    "# 4) If still missing, try a consolidated snapshot first; rebuild from local cache only if needed\n",
    "def _load_snapshot():\n",
    "    if not P_YQ_FLOAT_SNAP.exists():\n",
    "        return pd.DataFrame()\n",
    "    try:\n",
    "        df = pd.read_parquet(P_YQ_FLOAT_SNAP)\n",
    "        if 'ticker' not in df.columns:  # in case saved with ticker as index\n",
    "            df = df.reset_index().rename(columns={'index':'ticker'})\n",
    "        return df.assign(ticker=lambda d: d['ticker'].astype(str).str.upper())\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def _rebuild_snapshot_from_cache():\n",
    "    rows = []\n",
    "    if YF_CACHE.exists():\n",
    "        for p in YF_CACHE.glob(\"*.json\"):\n",
    "            try:\n",
    "                j = json.loads(p.read_text())\n",
    "                tkr = p.stem.upper()\n",
    "                v = j.get('float_shares_yf') or j.get('float_shares') or j.get('floatShares')\n",
    "                if v is not None:\n",
    "                    rows.append({'ticker': tkr, 'float_shares_yf': float(v)})\n",
    "            except Exception:\n",
    "                pass\n",
    "    snap = pd.DataFrame(rows).drop_duplicates('ticker', keep='last') if rows else pd.DataFrame()\n",
    "    if not snap.empty:\n",
    "        try: snap.to_parquet(P_YQ_FLOAT_SNAP, index=False)\n",
    "        except Exception: pass\n",
    "    return snap\n",
    "\n",
    "if ae['float_shares'].isna().any():\n",
    "    snap = _load_snapshot()\n",
    "    if snap.empty:\n",
    "        snap = _rebuild_snapshot_from_cache()\n",
    "    if not snap.empty:\n",
    "        float_col = next((c for c in ['float_shares_yf','float_shares','free_float','floatShares'] if c in snap.columns), None)\n",
    "        if float_col:\n",
    "            mapper = (snap.dropna(subset=[float_col])\n",
    "                        .drop_duplicates('ticker', keep='last')\n",
    "                        .set_index('ticker')[float_col].astype(float).to_dict())\n",
    "            m = ae['float_shares'].isna()\n",
    "            ae.loc[m, 'float_shares'] = ae.loc[m, 'ticker'].map(mapper)\n",
    "\n",
    "# 5) free_float alias + forward fill\n",
    "if 'free_float' not in ae.columns:\n",
    "    ae['free_float'] = np.nan\n",
    "ae['free_float'] = ae['free_float'].fillna(ae['float_shares'])\n",
    "\n",
    "for c in ['float_shares','free_float','shares_outstanding']:\n",
    "    ae[c] = pd.to_numeric(ae[c], errors='coerce')\n",
    "    ae[c] = ae.groupby('ticker')[c].ffill()\n",
    "\n",
    "analysis_enriched = ae\n",
    "\n",
    "# quick peek and coverage\n",
    "print(\"AAPL after float resolver:\")\n",
    "display(analysis_enriched.loc[analysis_enriched['ticker']=='AAPL',\n",
    "                              ['date','shares_outstanding','float_shares','free_float']].head(12))\n",
    "cov_sh = analysis_enriched.groupby('ticker')['shares_outstanding'].apply(lambda s: s.notna().any()).sum()\n",
    "cov_ff = analysis_enriched.groupby('ticker')['float_shares'].apply(lambda s: s.notna().any()).sum()\n",
    "print(\"Coverage -> shares_outstanding:\", cov_sh, \"| float_shares:\", cov_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "206a93ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tickers missing float_shares: 1155\n",
      "[YF float] Applied cached snapshot; no network.\n",
      "Coverage -> shares_outstanding: 3668 | float_shares: 3966\n"
     ]
    }
   ],
   "source": [
    "# === YF float-only backfill (with snapshot+cache guard; idempotent) ===\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, json, time\n",
    "import yfinance as yf\n",
    "\n",
    "assert 'analysis_enriched' in globals(), \"Run the finalize/SEC steps first.\"\n",
    "\n",
    "OUT = Path(\"outputs\"); OUT.mkdir(parents=True, exist_ok=True)\n",
    "YF_CACHE = OUT / \"yf_snapshot_cache\"; YF_CACHE.mkdir(parents=True, exist_ok=True)\n",
    "P_YQ_FLOAT_SNAP = OUT / \"yq_float_snapshot.parquet\"\n",
    "\n",
    "# Defaults if not already defined in your bootstrap\n",
    "if 'YQ_FLOAT_CACHE_ONLY' not in globals(): YQ_FLOAT_CACHE_ONLY = False\n",
    "if 'NO_NETWORK' not in globals():          NO_NETWORK = False\n",
    "\n",
    "def _safe_float(x):\n",
    "    try: return float(x)\n",
    "    except Exception: return None\n",
    "\n",
    "def _read_cache(sym):\n",
    "    p = YF_CACHE / f\"{sym}.json\"\n",
    "    if p.exists():\n",
    "        try: return json.loads(p.read_text())\n",
    "        except Exception: return {}\n",
    "    return {}\n",
    "\n",
    "def _write_cache(sym, d):\n",
    "    p = YF_CACHE / f\"{sym}.json\"\n",
    "    try: p.write_text(json.dumps(d))\n",
    "    except Exception: pass\n",
    "\n",
    "def fetch_yf_float_only(sym: str):\n",
    "    \"\"\"Return {'ticker','float_shares_yf'} and update per-ticker cache.\"\"\"\n",
    "    t = str(sym).upper()\n",
    "\n",
    "    # cache first\n",
    "    cached = _read_cache(t)\n",
    "    if 'float_shares_yf' in cached and cached['float_shares_yf'] is not None:\n",
    "        return {'ticker': t, 'float_shares_yf': cached['float_shares_yf']}\n",
    "\n",
    "    out = {'ticker': t, 'float_shares_yf': None}\n",
    "    try:\n",
    "        tk = yf.Ticker(t)\n",
    "        info = {}\n",
    "        try:\n",
    "            info = tk.info or {}\n",
    "        except Exception:\n",
    "            info = {}\n",
    "\n",
    "        flt = _safe_float(info.get('floatShares'))\n",
    "        # fallback from insiders %\n",
    "        if flt is None:\n",
    "            shares = _safe_float(info.get('sharesOutstanding'))\n",
    "            insiders = _safe_float(info.get('heldPercentInsiders'))\n",
    "            if shares is not None and insiders is not None:\n",
    "                flt = (1.0 - insiders) * shares\n",
    "\n",
    "        # last resort: if your SEC event pipeline produced float_shares, use latest\n",
    "        if flt is None and 'events_enriched' in globals():\n",
    "            ee = events_enriched.query(\"ticker == @t\")\n",
    "            if not ee.empty and 'float_shares' in ee and ee['float_shares'].notna().any():\n",
    "                flt = _safe_float(ee['float_shares'].dropna().iloc[-1])\n",
    "\n",
    "        out['float_shares_yf'] = flt\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    merged = _read_cache(t)\n",
    "    merged.update(out)\n",
    "    _write_cache(t, merged)\n",
    "    time.sleep(0.10)\n",
    "    return out\n",
    "\n",
    "# 1) who is missing float_shares?\n",
    "need_float = (analysis_enriched.groupby('ticker')['float_shares']\n",
    "                .apply(lambda s: s.notna().any())\n",
    "                .pipe(lambda s: sorted(s.index[~s].tolist())))\n",
    "print(\"Tickers missing float_shares:\", len(need_float))\n",
    "\n",
    "# 2) Fast exits / cache-only mode\n",
    "if not need_float:\n",
    "    print(\"[YF float] Nothing missing — skipping.\")\n",
    "elif YQ_FLOAT_CACHE_ONLY or NO_NETWORK:\n",
    "    if P_YQ_FLOAT_SNAP.exists():\n",
    "        snap = pd.read_parquet(P_YQ_FLOAT_SNAP)\n",
    "        if not snap.empty:\n",
    "            mapper = snap.set_index('ticker')['float_shares_yf'].to_dict()\n",
    "            m = analysis_enriched['float_shares'].isna()\n",
    "            analysis_enriched.loc[m, 'float_shares'] = analysis_enriched.loc[m, 'ticker'].map(mapper)\n",
    "\n",
    "            if 'free_float' not in analysis_enriched.columns:\n",
    "                analysis_enriched['free_float'] = np.nan\n",
    "            analysis_enriched['free_float'] = analysis_enriched['free_float'].fillna(analysis_enriched['float_shares'])\n",
    "\n",
    "            for c in ['float_shares','free_float']:\n",
    "                analysis_enriched[c] = pd.to_numeric(analysis_enriched[c], errors='coerce')\n",
    "                analysis_enriched[c] = analysis_enriched.groupby('ticker')[c].ffill()\n",
    "\n",
    "            print(\"[YF float] Applied cached snapshot; no network.\")\n",
    "    else:\n",
    "        print(\"[YF float] Cache-only but no snapshot found — skipped by design.\")\n",
    "else:\n",
    "    # 3) Networked harvest (only for those still missing)\n",
    "    rows = []\n",
    "    for i, sym in enumerate(need_float, 1):\n",
    "        rows.append(fetch_yf_float_only(sym))\n",
    "        if i % 200 == 0:\n",
    "            print(f\"... processed {i}/{len(need_float)}\")\n",
    "\n",
    "    yf_float_df = (pd.DataFrame(rows)\n",
    "                     .dropna(subset=['float_shares_yf'])\n",
    "                     .drop_duplicates('ticker'))\n",
    "    print(\"New float entries:\", len(yf_float_df))\n",
    "\n",
    "    # Save/merge consolidated snapshot for future cache-only runs\n",
    "    prev = pd.read_parquet(P_YQ_FLOAT_SNAP) if P_YQ_FLOAT_SNAP.exists() else pd.DataFrame(columns=['ticker','float_shares_yf'])\n",
    "    snap = (pd.concat([prev, yf_float_df], ignore_index=True)\n",
    "              .dropna(subset=['ticker'])\n",
    "              .drop_duplicates('ticker', keep='last'))\n",
    "    snap.to_parquet(P_YQ_FLOAT_SNAP)\n",
    "    print(f\"[YF float] Snapshot saved -> {P_YQ_FLOAT_SNAP.name} ({len(snap)} tickers).\")\n",
    "\n",
    "    # Apply to analysis_enriched + free_float + ffill\n",
    "    if not yf_float_df.empty:\n",
    "        mapper = snap.set_index('ticker')['float_shares_yf'].to_dict()\n",
    "        mask = analysis_enriched['float_shares'].isna()\n",
    "        analysis_enriched.loc[mask, 'float_shares'] = analysis_enriched.loc[mask, 'ticker'].map(mapper)\n",
    "\n",
    "        if 'free_float' not in analysis_enriched.columns:\n",
    "            analysis_enriched['free_float'] = np.nan\n",
    "        analysis_enriched['free_float'] = analysis_enriched['free_float'].fillna(analysis_enriched['float_shares'])\n",
    "\n",
    "        for c in ['float_shares','free_float']:\n",
    "            analysis_enriched[c] = pd.to_numeric(analysis_enriched[c], errors='coerce')\n",
    "            analysis_enriched[c] = analysis_enriched.groupby('ticker')[c].ffill()\n",
    "\n",
    "# Coverage summary\n",
    "cov_sh = analysis_enriched.groupby('ticker')['shares_outstanding'].apply(lambda s: s.notna().any()).sum()\n",
    "cov_ff = analysis_enriched.groupby('ticker')['float_shares'].apply(lambda s: s.notna().any()).sum()\n",
    "print(\"Coverage -> shares_outstanding:\", cov_sh, \"| float_shares:\", cov_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d3c5236d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL float from yfinance (or fallback): 14814270914.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>shares_outstanding</th>\n",
       "      <th>float_shares</th>\n",
       "      <th>free_float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12016</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12017</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12018</th>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12019</th>\n",
       "      <td>2022-10-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12020</th>\n",
       "      <td>2022-10-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12023</th>\n",
       "      <td>2022-10-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12024</th>\n",
       "      <td>2022-10-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12025</th>\n",
       "      <td>2022-10-14</td>\n",
       "      <td>1.590812e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12026</th>\n",
       "      <td>2022-10-17</td>\n",
       "      <td>1.590812e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12027</th>\n",
       "      <td>2022-10-18</td>\n",
       "      <td>1.590812e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  shares_outstanding  float_shares    free_float\n",
       "12016 2022-10-03                 NaN  1.481427e+10  1.481427e+10\n",
       "12017 2022-10-04                 NaN  1.481427e+10  1.481427e+10\n",
       "12018 2022-10-05                 NaN  1.481427e+10  1.481427e+10\n",
       "12019 2022-10-06                 NaN  1.481427e+10  1.481427e+10\n",
       "12020 2022-10-07                 NaN  1.481427e+10  1.481427e+10\n",
       "...          ...                 ...           ...           ...\n",
       "12023 2022-10-12                 NaN  1.481427e+10  1.481427e+10\n",
       "12024 2022-10-13                 NaN  1.481427e+10  1.481427e+10\n",
       "12025 2022-10-14        1.590812e+10  1.481427e+10  1.481427e+10\n",
       "12026 2022-10-17        1.590812e+10  1.481427e+10  1.481427e+10\n",
       "12027 2022-10-18        1.590812e+10  1.481427e+10  1.481427e+10\n",
       "\n",
       "[12 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage -> shares_outstanding: 3668 | float_shares: 3966\n"
     ]
    }
   ],
   "source": [
    "# === Targeted float fix for a ticker (e.g., AAPL) ===\n",
    "import json, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "assert 'analysis_enriched' in globals()\n",
    "\n",
    "OUT = Path(\"outputs\"); OUT.mkdir(exist_ok=True)\n",
    "YF_CACHE = OUT / \"yf_snapshot_cache\"; YF_CACHE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _safe_float(x):\n",
    "    try: return float(x)\n",
    "    except Exception: return None\n",
    "\n",
    "def _read_cache(sym):\n",
    "    p = YF_CACHE / f\"{sym}.json\"\n",
    "    if p.exists():\n",
    "        try: return json.loads(p.read_text())\n",
    "        except Exception: return {}\n",
    "    return {}\n",
    "\n",
    "def _write_cache(sym, d):\n",
    "    p = YF_CACHE / f\"{sym}.json\"\n",
    "    try: p.write_text(json.dumps(d))\n",
    "    except Exception: pass\n",
    "\n",
    "def ensure_float_for(sym: str):\n",
    "    t = str(sym).upper()\n",
    "\n",
    "    # 1) check cache\n",
    "    cache = _read_cache(t)\n",
    "    float_cached = cache.get('float_shares_yf')\n",
    "\n",
    "    # 2) fetch if missing\n",
    "    if float_cached is None:\n",
    "        flt = None\n",
    "        try:\n",
    "            tk = yf.Ticker(t)\n",
    "\n",
    "            # direct floatShares from info\n",
    "            info = {}\n",
    "            try: info = tk.info or {}\n",
    "            except Exception: info = {}\n",
    "\n",
    "            flt = _safe_float(info.get('floatShares'))\n",
    "\n",
    "            # fallback: free float ≈ (1 - insiders%) * sharesOutstanding\n",
    "            if flt is None:\n",
    "                shares   = _safe_float(info.get('sharesOutstanding'))\n",
    "                insiders = _safe_float(info.get('heldPercentInsiders'))\n",
    "                if shares is not None and insiders is not None:\n",
    "                    flt = (1.0 - insiders) * shares\n",
    "        except Exception:\n",
    "            flt = None\n",
    "\n",
    "        # update cache (preserve any other keys)\n",
    "        cache.update({'float_shares_yf': flt})\n",
    "        _write_cache(t, cache)\n",
    "        float_cached = flt\n",
    "\n",
    "    # 3) apply to analysis_enriched\n",
    "    if float_cached is not None:\n",
    "        mask = (analysis_enriched['ticker'].str.upper()==t) & (analysis_enriched['float_shares'].isna())\n",
    "        analysis_enriched.loc[mask, 'float_shares'] = float(float_cached)\n",
    "\n",
    "        if 'free_float' not in analysis_enriched.columns:\n",
    "            analysis_enriched['free_float'] = np.nan\n",
    "        analysis_enriched['free_float'] = analysis_enriched['free_float'].fillna(analysis_enriched['float_shares'])\n",
    "\n",
    "        for c in ['float_shares','free_float']:\n",
    "            analysis_enriched[c] = pd.to_numeric(analysis_enriched[c], errors='coerce')\n",
    "            analysis_enriched[c] = analysis_enriched.groupby('ticker')[c].ffill()\n",
    "\n",
    "    return float_cached\n",
    "\n",
    "# --- run it for AAPL (change symbol if needed) ---\n",
    "val = ensure_float_for(\"AAPL\")\n",
    "print(\"AAPL float from yfinance (or fallback):\", val)\n",
    "\n",
    "display(analysis_enriched.loc[analysis_enriched['ticker']=='AAPL',\n",
    "                              ['date','shares_outstanding','float_shares','free_float']].head(12))\n",
    "\n",
    "# coverage snapshot after fix\n",
    "cov_sh = analysis_enriched.groupby('ticker')['shares_outstanding'].apply(lambda s: s.notna().any()).sum()\n",
    "cov_ff = analysis_enriched.groupby('ticker')['float_shares'].apply(lambda s: s.notna().any()).sum()\n",
    "print(\"Coverage -> shares_outstanding:\", cov_sh, \"| float_shares:\", cov_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f11903df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "present: {'float_shares', 'free_float', 'performed_reverse_split', 'performed_split', 'shares_outstanding'}\n",
      "tickers with any shares_outstanding: 3668\n",
      "tickers with any float_shares: 3966\n"
     ]
    }
   ],
   "source": [
    "need = {'shares_outstanding','float_shares','free_float','performed_split','performed_reverse_split'}\n",
    "print(\"present:\", need & set(analysis_enriched.columns))\n",
    "print(\"tickers with any shares_outstanding:\", \n",
    "      analysis_enriched.groupby('ticker')['shares_outstanding'].apply(lambda s: s.notna().any()).sum())\n",
    "print(\"tickers with any float_shares:\", \n",
    "      analysis_enriched.groupby('ticker')['float_shares'].apply(lambda s: s.notna().any()).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c64eb24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still missing both: 1053\n",
      "… warrant/unit-ish suffix: 157\n",
      "… dollar-prefixed/test-ish: 0\n",
      "… with no CIK in fundamentals: 1050\n",
      "\n",
      "Examples (warrant/unit-ish): ['AADR', 'AAPU', 'AIRR', 'ALLW', 'AMDU', 'AMUU', 'AMZU', 'APACU', 'AQWA', 'ARVR']\n",
      "Examples (no CIK): ['AADR', 'AALG', 'AAPB', 'AAPD', 'AAPU', 'AAUS', 'AAVM', 'AAXJ', 'ABCS', 'ABI']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# which tickers have neither series anywhere in analysis_enriched?\n",
    "missing = (analysis_enriched.groupby('ticker')[['shares_outstanding','float_shares']]\n",
    "           .apply(lambda df: pd.Series({'has_sh': df['shares_outstanding'].notna().any(),\n",
    "                                        'has_ff': df['float_shares'].notna().any()}))\n",
    "           .reset_index())\n",
    "still_none = missing[(~missing['has_sh']) & (~missing['has_ff'])]['ticker'].astype(str)\n",
    "\n",
    "warrant_like = still_none[still_none.str.match(r'.*(?:W|WS|WT|W[A-D]?|U|R)$')]\n",
    "dollar_pref  = still_none[still_none.str.startswith('$')]\n",
    "no_cik       = set(still_none) - set(fundamentals.dropna(subset=['cik'])['ticker'])\n",
    "\n",
    "print(\"Still missing both:\", len(still_none))\n",
    "print(\"… warrant/unit-ish suffix:\", len(warrant_like))\n",
    "print(\"… dollar-prefixed/test-ish:\", len(dollar_pref))\n",
    "print(\"… with no CIK in fundamentals:\", len(no_cik))\n",
    "\n",
    "# peek a few examples\n",
    "print(\"\\nExamples (warrant/unit-ish):\", list(warrant_like[:10]))\n",
    "print(\"Examples (no CIK):\", list(sorted(no_cik))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e9c8c9cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ret</th>\n",
       "      <th>logret</th>\n",
       "      <th>...</th>\n",
       "      <th>cik</th>\n",
       "      <th>sic</th>\n",
       "      <th>sic_desc</th>\n",
       "      <th>recent_form</th>\n",
       "      <th>recent_filing_date</th>\n",
       "      <th>performed_split</th>\n",
       "      <th>performed_reverse_split</th>\n",
       "      <th>shares_outstanding</th>\n",
       "      <th>float_shares</th>\n",
       "      <th>free_float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3670610</th>\n",
       "      <td>2024-08-21</td>\n",
       "      <td>WABF</td>\n",
       "      <td>26.320000</td>\n",
       "      <td>26.320000</td>\n",
       "      <td>26.309999</td>\n",
       "      <td>26.309999</td>\n",
       "      <td>24.530266</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771432</th>\n",
       "      <td>2023-09-20</td>\n",
       "      <td>PULM</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>2.190000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>14300.0</td>\n",
       "      <td>-0.013636</td>\n",
       "      <td>-0.013730</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3652285.0</td>\n",
       "      <td>3.652200e+06</td>\n",
       "      <td>3.652200e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73940</th>\n",
       "      <td>2024-02-13</td>\n",
       "      <td>ADTX</td>\n",
       "      <td>35700.000000</td>\n",
       "      <td>41500.000000</td>\n",
       "      <td>35700.000000</td>\n",
       "      <td>39000.000000</td>\n",
       "      <td>39000.000000</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.051213</td>\n",
       "      <td>0.049945</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>441851.0</td>\n",
       "      <td>1.671270e+01</td>\n",
       "      <td>1.671270e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797437</th>\n",
       "      <td>2025-04-02</td>\n",
       "      <td>COLB</td>\n",
       "      <td>24.450001</td>\n",
       "      <td>25.139999</td>\n",
       "      <td>24.450001</td>\n",
       "      <td>25.120001</td>\n",
       "      <td>24.412327</td>\n",
       "      <td>1029100.0</td>\n",
       "      <td>0.014950</td>\n",
       "      <td>0.014839</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>210112415.0</td>\n",
       "      <td>1.919472e+08</td>\n",
       "      <td>1.919472e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740991</th>\n",
       "      <td>2024-10-07</td>\n",
       "      <td>CLAR</td>\n",
       "      <td>4.370000</td>\n",
       "      <td>4.390000</td>\n",
       "      <td>4.290000</td>\n",
       "      <td>4.390000</td>\n",
       "      <td>4.274121</td>\n",
       "      <td>233500.0</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38362000.0</td>\n",
       "      <td>3.217769e+07</td>\n",
       "      <td>3.217769e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295227</th>\n",
       "      <td>2023-02-02</td>\n",
       "      <td>ATHR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0002026353</td>\n",
       "      <td>7372</td>\n",
       "      <td>Services-Prepackaged Software</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>2025-08-19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.014718e+06</td>\n",
       "      <td>4.014718e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149933</th>\n",
       "      <td>2023-05-11</td>\n",
       "      <td>EUDA</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>1.230000</td>\n",
       "      <td>1.290000</td>\n",
       "      <td>1.290000</td>\n",
       "      <td>103300.0</td>\n",
       "      <td>-0.116438</td>\n",
       "      <td>-0.123794</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20191770.0</td>\n",
       "      <td>1.840318e+07</td>\n",
       "      <td>1.840318e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224182</th>\n",
       "      <td>2022-12-15</td>\n",
       "      <td>FDT</td>\n",
       "      <td>48.570000</td>\n",
       "      <td>48.590000</td>\n",
       "      <td>47.820000</td>\n",
       "      <td>48.020000</td>\n",
       "      <td>43.102501</td>\n",
       "      <td>45100.0</td>\n",
       "      <td>-0.022195</td>\n",
       "      <td>-0.022445</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96852</th>\n",
       "      <td>2025-08-22</td>\n",
       "      <td>AFRIW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26901592.0</td>\n",
       "      <td>3.281994e+06</td>\n",
       "      <td>3.281994e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051690</th>\n",
       "      <td>2024-04-04</td>\n",
       "      <td>SDVY</td>\n",
       "      <td>34.660000</td>\n",
       "      <td>34.730000</td>\n",
       "      <td>33.900002</td>\n",
       "      <td>33.990002</td>\n",
       "      <td>33.194687</td>\n",
       "      <td>733300.0</td>\n",
       "      <td>-0.011056</td>\n",
       "      <td>-0.011118</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date ticker          open          high           low         close     adj close     volume       ret  \\\n",
       "3670610 2024-08-21   WABF     26.320000     26.320000     26.309999     26.309999     24.530266      200.0  0.002286   \n",
       "2771432 2023-09-20   PULM      2.110000      2.190000      2.110000      2.170000      2.170000    14300.0 -0.013636   \n",
       "73940   2024-02-13   ADTX  35700.000000  41500.000000  35700.000000  39000.000000  39000.000000       35.0  0.051213   \n",
       "797437  2025-04-02   COLB     24.450001     25.139999     24.450001     25.120001     24.412327  1029100.0  0.014950   \n",
       "740991  2024-10-07   CLAR      4.370000      4.390000      4.290000      4.390000      4.274121   233500.0  0.002283   \n",
       "...            ...    ...           ...           ...           ...           ...           ...        ...       ...   \n",
       "295227  2023-02-02   ATHR           NaN           NaN           NaN           NaN           NaN        NaN       NaN   \n",
       "1149933 2023-05-11   EUDA      1.480000      1.480000      1.230000      1.290000      1.290000   103300.0 -0.116438   \n",
       "1224182 2022-12-15    FDT     48.570000     48.590000     47.820000     48.020000     43.102501    45100.0 -0.022195   \n",
       "96852   2025-08-22  AFRIW           NaN           NaN           NaN           NaN           NaN        NaN       NaN   \n",
       "3051690 2024-04-04   SDVY     34.660000     34.730000     33.900002     33.990002     33.194687   733300.0 -0.011056   \n",
       "\n",
       "           logret  ...         cik   sic                       sic_desc recent_form recent_filing_date  \\\n",
       "3670610  0.002283  ...        None  None                           None        None               None   \n",
       "2771432 -0.013730  ...         NaN   NaN                            NaN         NaN                NaN   \n",
       "73940    0.049945  ...         NaN   NaN                            NaN         NaN                NaN   \n",
       "797437   0.014839  ...         NaN   NaN                            NaN         NaN                NaN   \n",
       "740991   0.002281  ...         NaN   NaN                            NaN         NaN                NaN   \n",
       "...           ...  ...         ...   ...                            ...         ...                ...   \n",
       "295227        NaN  ...  0002026353  7372  Services-Prepackaged Software        10-Q         2025-08-19   \n",
       "1149933 -0.123794  ...         NaN   NaN                            NaN         NaN                NaN   \n",
       "1224182 -0.022445  ...         NaN   NaN                            NaN         NaN                NaN   \n",
       "96852         NaN  ...         NaN   NaN                            NaN         NaN                NaN   \n",
       "3051690 -0.011118  ...         NaN   NaN                            NaN         NaN                NaN   \n",
       "\n",
       "        performed_split performed_reverse_split shares_outstanding  float_shares    free_float  \n",
       "3670610               0                       0                NaN           NaN           NaN  \n",
       "2771432               0                       0          3652285.0  3.652200e+06  3.652200e+06  \n",
       "73940                 0                       0           441851.0  1.671270e+01  1.671270e+01  \n",
       "797437                0                       0        210112415.0  1.919472e+08  1.919472e+08  \n",
       "740991                0                       0         38362000.0  3.217769e+07  3.217769e+07  \n",
       "...                 ...                     ...                ...           ...           ...  \n",
       "295227                0                       0                NaN  4.014718e+06  4.014718e+06  \n",
       "1149933               0                       0         20191770.0  1.840318e+07  1.840318e+07  \n",
       "1224182               0                       0                NaN           NaN           NaN  \n",
       "96852                 0                       0         26901592.0  3.281994e+06  3.281994e+06  \n",
       "3051690               0                       0                NaN           NaN           NaN  \n",
       "\n",
       "[20 rows x 23 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_enriched.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c1719f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "present: {'float_shares', 'free_float', 'performed_reverse_split', 'performed_split', 'shares_outstanding'}\n",
      "missing: set()\n",
      "rows: 3845871 | tickers: 5121\n"
     ]
    }
   ],
   "source": [
    "need = {'shares_outstanding','float_shares','free_float','performed_split','performed_reverse_split'}\n",
    "print(\"present:\", need & set(analysis_enriched.columns))\n",
    "print(\"missing:\", need - set(analysis_enriched.columns))\n",
    "print(\"rows:\", len(analysis_enriched), \"| tickers:\", analysis_enriched['ticker'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "da657da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3845871 entries, 0 to 3845870\n",
      "Data columns (total 23 columns):\n",
      " #   Column                   Dtype         \n",
      "---  ------                   -----         \n",
      " 0   date                     datetime64[ns]\n",
      " 1   ticker                   object        \n",
      " 2   open                     float64       \n",
      " 3   high                     float64       \n",
      " 4   low                      float64       \n",
      " 5   close                    float64       \n",
      " 6   adj close                float64       \n",
      " 7   volume                   float64       \n",
      " 8   ret                      float64       \n",
      " 9   logret                   float64       \n",
      " 10  market_cap               float64       \n",
      " 11  sector                   object        \n",
      " 12  industry                 object        \n",
      " 13  cik                      object        \n",
      " 14  sic                      object        \n",
      " 15  sic_desc                 object        \n",
      " 16  recent_form              object        \n",
      " 17  recent_filing_date       object        \n",
      " 18  performed_split          int8          \n",
      " 19  performed_reverse_split  int8          \n",
      " 20  shares_outstanding       float64       \n",
      " 21  float_shares             float64       \n",
      " 22  free_float               float64       \n",
      "dtypes: datetime64[ns](1), float64(12), int8(2), object(8)\n",
      "memory usage: 623.5+ MB\n"
     ]
    }
   ],
   "source": [
    "analysis_enriched.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19fcb2b",
   "metadata": {},
   "source": [
    "Here’s a compact data dictionary for your current analysis_enriched (21 columns shown in your screenshot) 👇\n",
    "* `date (datetime64[ns])` — Trading date (tz-naive).\n",
    "* `ticker (object)` — Upper-cased symbol (e.g., AAPL).\n",
    "\n",
    "Prices (Yahoo)\n",
    "* `open (float64)` — Session open (split-adjusted).\n",
    "* `high (float64)` — Session high (split-adjusted).\n",
    "* `low (float64)` — Session low (split-adjusted).\n",
    "* `close (float64)` — Session close (split-adjusted, not dividend-adjusted).\n",
    "* `adj close (float64)` — Adjusted close (split and dividend adjusted). Use for return calcs.\n",
    "* `volume (float64)` — Shares traded that day (float due to missing values).\n",
    "\n",
    "Returns\n",
    "* `ret (float64)` — Simple daily return from adj close: (\\text{AdjClose}t / \\text{AdjClose}{t-1}) - 1.\n",
    "* `logret (float64)` — Log daily return: \\ln(\\text{AdjClose}t / \\text{AdjClose}{t-1}).\n",
    "\n",
    "Firm metadata\n",
    "* `market_cap (float64)` — Latest market cap from Yahoo (generally USD). Snapshot value repeated across dates (not historical).\n",
    "* `sector (object)` — Sector (Yahoo; backfilled from SEC SIC when missing).\n",
    "* `industry (object)` — Industry (Yahoo; may be missing).\n",
    "* `cik (object)` — 10-digit SEC Central Index Key.\n",
    "* `sic (object)` — 4-digit SEC Standard Industrial Classification code (string).\n",
    "* `sic_desc (object)` — Text description of the SIC code.\n",
    "* `recent_form (object)` — Most recent SEC filing form (e.g., 10-K, 10-Q, 8-K) from cached SEC data.\n",
    "* `recent_filing_date (object)` — Filing date as string; convert to datetime for comparisons.\n",
    "\n",
    "Share supply\n",
    "* `shares_outstanding (object → numeric)` — Outstanding shares (point-in-time). Built from SEC companyfacts when available, then backfilled via Yahoo snapshots; forward-filled within ticker.\n",
    "* `float_shares (object → numeric)` — Tradable “free float” shares. From SEC public float (USD) converted to shares using price on/as-of filing date, else from Yahoo; forward-filled within ticker.\n",
    "* `free_float (object → numeric)` — Alias of float_shares for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "43a7ade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert recent_filing_date to datetime\n",
    "analysis_enriched['recent_filing_date'] = pd.to_datetime(\n",
    "    analysis_enriched['recent_filing_date'], errors='coerce'\n",
    ")\n",
    "\n",
    "# Volume as nullable integer (if you prefer)\n",
    "if 'volume' in analysis_enriched:\n",
    "    analysis_enriched['volume'] = analysis_enriched['volume'].round().astype('Int64')\n",
    "\n",
    "# Sector/industry as categories (saves memory, faster groupbys)\n",
    "for col in ['sector', 'industry', 'recent_form', 'sic_desc']:\n",
    "    if col in analysis_enriched:\n",
    "        analysis_enriched[col] = analysis_enriched[col].astype('category')\n",
    "\n",
    "for c in ['shares_outstanding', 'float_shares', 'free_float']:\n",
    "    analysis_enriched[c] = pd.to_numeric(analysis_enriched[c], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d5b413ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3845871 entries, 0 to 3845870\n",
      "Data columns (total 23 columns):\n",
      " #   Column                   Dtype         \n",
      "---  ------                   -----         \n",
      " 0   date                     datetime64[ns]\n",
      " 1   ticker                   object        \n",
      " 2   open                     float64       \n",
      " 3   high                     float64       \n",
      " 4   low                      float64       \n",
      " 5   close                    float64       \n",
      " 6   adj close                float64       \n",
      " 7   volume                   Int64         \n",
      " 8   ret                      float64       \n",
      " 9   logret                   float64       \n",
      " 10  market_cap               float64       \n",
      " 11  sector                   category      \n",
      " 12  industry                 category      \n",
      " 13  cik                      object        \n",
      " 14  sic                      object        \n",
      " 15  sic_desc                 category      \n",
      " 16  recent_form              category      \n",
      " 17  recent_filing_date       datetime64[ns]\n",
      " 18  performed_split          int8          \n",
      " 19  performed_reverse_split  int8          \n",
      " 20  shares_outstanding       float64       \n",
      " 21  float_shares             float64       \n",
      " 22  free_float               float64       \n",
      "dtypes: Int64(1), category(4), datetime64[ns](2), float64(11), int8(2), object(3)\n",
      "memory usage: 524.5+ MB\n"
     ]
    }
   ],
   "source": [
    "analysis_enriched.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ce87c384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prices_shape': (751, 30726), 'fundamentals_tickers': 338, 'analysis_rows': 3845871}\n"
     ]
    }
   ],
   "source": [
    "# Quick summary after full run\n",
    "try:\n",
    "    print({\n",
    "        \"prices_shape\": None if prices_raw is None else prices_raw.shape,\n",
    "        \"fundamentals_tickers\": fundamentals['ticker'].nunique(),\n",
    "        \"analysis_rows\": len(analysis_enriched),\n",
    "    })\n",
    "except Exception as e:\n",
    "    print(\"Summary skipped:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "99fcfdee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ret</th>\n",
       "      <th>logret</th>\n",
       "      <th>...</th>\n",
       "      <th>cik</th>\n",
       "      <th>sic</th>\n",
       "      <th>sic_desc</th>\n",
       "      <th>recent_form</th>\n",
       "      <th>recent_filing_date</th>\n",
       "      <th>performed_split</th>\n",
       "      <th>performed_reverse_split</th>\n",
       "      <th>shares_outstanding</th>\n",
       "      <th>float_shares</th>\n",
       "      <th>free_float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12016</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>138.210007</td>\n",
       "      <td>143.070007</td>\n",
       "      <td>137.690002</td>\n",
       "      <td>142.449997</td>\n",
       "      <td>140.236282</td>\n",
       "      <td>114311700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12017</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>145.029999</td>\n",
       "      <td>146.220001</td>\n",
       "      <td>144.259995</td>\n",
       "      <td>146.100006</td>\n",
       "      <td>143.829605</td>\n",
       "      <td>87830100</td>\n",
       "      <td>0.025623</td>\n",
       "      <td>0.025301</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12018</th>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>144.070007</td>\n",
       "      <td>147.380005</td>\n",
       "      <td>143.009995</td>\n",
       "      <td>146.399994</td>\n",
       "      <td>144.124924</td>\n",
       "      <td>79471000</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>0.002051</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12019</th>\n",
       "      <td>2022-10-06</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>145.809998</td>\n",
       "      <td>147.539993</td>\n",
       "      <td>145.220001</td>\n",
       "      <td>145.429993</td>\n",
       "      <td>143.169983</td>\n",
       "      <td>68402200</td>\n",
       "      <td>-0.006626</td>\n",
       "      <td>-0.006648</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12020</th>\n",
       "      <td>2022-10-07</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>142.539993</td>\n",
       "      <td>143.100006</td>\n",
       "      <td>139.449997</td>\n",
       "      <td>140.089996</td>\n",
       "      <td>137.912994</td>\n",
       "      <td>85925600</td>\n",
       "      <td>-0.036719</td>\n",
       "      <td>-0.037410</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12762</th>\n",
       "      <td>2025-09-24</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>255.220001</td>\n",
       "      <td>255.740005</td>\n",
       "      <td>251.039993</td>\n",
       "      <td>252.309998</td>\n",
       "      <td>252.309998</td>\n",
       "      <td>42303700</td>\n",
       "      <td>-0.008332</td>\n",
       "      <td>-0.008367</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.484039e+10</td>\n",
       "      <td>1.591087e+10</td>\n",
       "      <td>1.591087e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12763</th>\n",
       "      <td>2025-09-25</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>253.210007</td>\n",
       "      <td>257.170013</td>\n",
       "      <td>251.710007</td>\n",
       "      <td>256.869995</td>\n",
       "      <td>256.869995</td>\n",
       "      <td>55202100</td>\n",
       "      <td>0.018073</td>\n",
       "      <td>0.017912</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.484039e+10</td>\n",
       "      <td>1.591087e+10</td>\n",
       "      <td>1.591087e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12764</th>\n",
       "      <td>2025-09-26</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>254.100006</td>\n",
       "      <td>257.600006</td>\n",
       "      <td>253.779999</td>\n",
       "      <td>255.460007</td>\n",
       "      <td>255.460007</td>\n",
       "      <td>46076300</td>\n",
       "      <td>-0.005489</td>\n",
       "      <td>-0.005504</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.484039e+10</td>\n",
       "      <td>1.591087e+10</td>\n",
       "      <td>1.591087e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12765</th>\n",
       "      <td>2025-09-29</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>254.559998</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>253.009995</td>\n",
       "      <td>254.429993</td>\n",
       "      <td>254.429993</td>\n",
       "      <td>40127700</td>\n",
       "      <td>-0.004032</td>\n",
       "      <td>-0.004040</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.484039e+10</td>\n",
       "      <td>1.591087e+10</td>\n",
       "      <td>1.591087e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12766</th>\n",
       "      <td>2025-09-30</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>254.860001</td>\n",
       "      <td>255.919998</td>\n",
       "      <td>253.110001</td>\n",
       "      <td>254.630005</td>\n",
       "      <td>254.630005</td>\n",
       "      <td>37666900</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.484039e+10</td>\n",
       "      <td>1.591087e+10</td>\n",
       "      <td>1.591087e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>751 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date ticker        open        high         low       close   adj close     volume       ret    logret  \\\n",
       "12016 2022-10-03   AAPL  138.210007  143.070007  137.690002  142.449997  140.236282  114311700       NaN       NaN   \n",
       "12017 2022-10-04   AAPL  145.029999  146.220001  144.259995  146.100006  143.829605   87830100  0.025623  0.025301   \n",
       "12018 2022-10-05   AAPL  144.070007  147.380005  143.009995  146.399994  144.124924   79471000  0.002053  0.002051   \n",
       "12019 2022-10-06   AAPL  145.809998  147.539993  145.220001  145.429993  143.169983   68402200 -0.006626 -0.006648   \n",
       "12020 2022-10-07   AAPL  142.539993  143.100006  139.449997  140.089996  137.912994   85925600 -0.036719 -0.037410   \n",
       "...          ...    ...         ...         ...         ...         ...         ...        ...       ...       ...   \n",
       "12762 2025-09-24   AAPL  255.220001  255.740005  251.039993  252.309998  252.309998   42303700 -0.008332 -0.008367   \n",
       "12763 2025-09-25   AAPL  253.210007  257.170013  251.710007  256.869995  256.869995   55202100  0.018073  0.017912   \n",
       "12764 2025-09-26   AAPL  254.100006  257.600006  253.779999  255.460007  255.460007   46076300 -0.005489 -0.005504   \n",
       "12765 2025-09-29   AAPL  254.559998  255.000000  253.009995  254.429993  254.429993   40127700 -0.004032 -0.004040   \n",
       "12766 2025-09-30   AAPL  254.860001  255.919998  253.110001  254.630005  254.630005   37666900  0.000786  0.000786   \n",
       "\n",
       "       ...  cik  sic sic_desc recent_form recent_filing_date performed_split performed_reverse_split  \\\n",
       "12016  ...  NaN  NaN      NaN         NaN                NaT               1                       0   \n",
       "12017  ...  NaN  NaN      NaN         NaN                NaT               0                       0   \n",
       "12018  ...  NaN  NaN      NaN         NaN                NaT               0                       0   \n",
       "12019  ...  NaN  NaN      NaN         NaN                NaT               0                       0   \n",
       "12020  ...  NaN  NaN      NaN         NaN                NaT               0                       0   \n",
       "...    ...  ...  ...      ...         ...                ...             ...                     ...   \n",
       "12762  ...  NaN  NaN      NaN         NaN                NaT               0                       0   \n",
       "12763  ...  NaN  NaN      NaN         NaN                NaT               0                       0   \n",
       "12764  ...  NaN  NaN      NaN         NaN                NaT               0                       0   \n",
       "12765  ...  NaN  NaN      NaN         NaN                NaT               0                       0   \n",
       "12766  ...  NaN  NaN      NaN         NaN                NaT               0                       0   \n",
       "\n",
       "      shares_outstanding  float_shares    free_float  \n",
       "12016                NaN  1.481427e+10  1.481427e+10  \n",
       "12017                NaN  1.481427e+10  1.481427e+10  \n",
       "12018                NaN  1.481427e+10  1.481427e+10  \n",
       "12019                NaN  1.481427e+10  1.481427e+10  \n",
       "12020                NaN  1.481427e+10  1.481427e+10  \n",
       "...                  ...           ...           ...  \n",
       "12762       1.484039e+10  1.591087e+10  1.591087e+10  \n",
       "12763       1.484039e+10  1.591087e+10  1.591087e+10  \n",
       "12764       1.484039e+10  1.591087e+10  1.591087e+10  \n",
       "12765       1.484039e+10  1.591087e+10  1.591087e+10  \n",
       "12766       1.484039e+10  1.591087e+10  1.591087e+10  \n",
       "\n",
       "[751 rows x 23 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl = analysis_enriched[analysis_enriched[\"ticker\"] == \"AAPL\"]\n",
    "aapl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0c29e035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events_enriched rows for AAPL: 145\n",
      "          date\n",
      "min 2022-10-03\n",
      "max 2025-09-30\n"
     ]
    }
   ],
   "source": [
    "# AAPL presence in the SEC-derived events\n",
    "print(\"events_enriched rows for AAPL:\",\n",
    "      0 if 'events_enriched' not in globals() else\n",
    "      len(events_enriched.query(\"ticker == 'AAPL'\")))\n",
    "\n",
    "# First/last dates we have for AAPL in analysis_enriched\n",
    "ae_aapl = analysis_enriched.query(\"ticker == 'AAPL'\")[['date']].agg(['min','max'])\n",
    "print(ae_aapl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fdbfdd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shares_outstanding coverage: 3668 -> 3705 tickers\n",
      "\n",
      "AACB sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>adj close</th>\n",
       "      <th>market_cap</th>\n",
       "      <th>shares_outstanding</th>\n",
       "      <th>float_shares</th>\n",
       "      <th>free_float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27675000.0</td>\n",
       "      <td>22147902.0</td>\n",
       "      <td>22147902.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27675000.0</td>\n",
       "      <td>22147902.0</td>\n",
       "      <td>22147902.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27675000.0</td>\n",
       "      <td>22147902.0</td>\n",
       "      <td>22147902.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27675000.0</td>\n",
       "      <td>22147902.0</td>\n",
       "      <td>22147902.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27675000.0</td>\n",
       "      <td>22147902.0</td>\n",
       "      <td>22147902.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-10-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27675000.0</td>\n",
       "      <td>22147902.0</td>\n",
       "      <td>22147902.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-10-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27675000.0</td>\n",
       "      <td>22147902.0</td>\n",
       "      <td>22147902.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  adj close  market_cap  shares_outstanding  float_shares  free_float\n",
       "0 2022-10-03        NaN         NaN          27675000.0    22147902.0  22147902.0\n",
       "1 2022-10-04        NaN         NaN          27675000.0    22147902.0  22147902.0\n",
       "2 2022-10-05        NaN         NaN          27675000.0    22147902.0  22147902.0\n",
       "3 2022-10-06        NaN         NaN          27675000.0    22147902.0  22147902.0\n",
       "4 2022-10-07        NaN         NaN          27675000.0    22147902.0  22147902.0\n",
       "5 2022-10-10        NaN         NaN          27675000.0    22147902.0  22147902.0\n",
       "6 2022-10-11        NaN         NaN          27675000.0    22147902.0  22147902.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AAPL sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>adj close</th>\n",
       "      <th>market_cap</th>\n",
       "      <th>shares_outstanding</th>\n",
       "      <th>float_shares</th>\n",
       "      <th>free_float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12016</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>140.236282</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.594342e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12017</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>143.829605</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.594342e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12018</th>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>144.124924</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.594342e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12019</th>\n",
       "      <td>2022-10-06</td>\n",
       "      <td>143.169983</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.594342e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12020</th>\n",
       "      <td>2022-10-07</td>\n",
       "      <td>137.912994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.594342e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12021</th>\n",
       "      <td>2022-10-10</td>\n",
       "      <td>138.237854</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.594342e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12022</th>\n",
       "      <td>2022-10-11</td>\n",
       "      <td>136.820251</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.594342e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "      <td>1.481427e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date   adj close  market_cap  shares_outstanding  float_shares    free_float\n",
       "12016 2022-10-03  140.236282         NaN        1.594342e+10  1.481427e+10  1.481427e+10\n",
       "12017 2022-10-04  143.829605         NaN        1.594342e+10  1.481427e+10  1.481427e+10\n",
       "12018 2022-10-05  144.124924         NaN        1.594342e+10  1.481427e+10  1.481427e+10\n",
       "12019 2022-10-06  143.169983         NaN        1.594342e+10  1.481427e+10  1.481427e+10\n",
       "12020 2022-10-07  137.912994         NaN        1.594342e+10  1.481427e+10  1.481427e+10\n",
       "12021 2022-10-10  138.237854         NaN        1.594342e+10  1.481427e+10  1.481427e+10\n",
       "12022 2022-10-11  136.820251         NaN        1.594342e+10  1.481427e+10  1.481427e+10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === S_final (robust, per-ticker, cache-only): fill missing shares_outstanding ===\n",
    "import pandas as pd, numpy as np, json\n",
    "from pathlib import Path\n",
    "\n",
    "assert 'analysis_enriched' in globals(), \"Run your finalize cell so analysis_enriched exists.\"\n",
    "\n",
    "OUT = Path(\"outputs\")\n",
    "YF_CACHE = OUT / \"yf_snapshot_cache\"\n",
    "\n",
    "ae = analysis_enriched.copy()\n",
    "ae['ticker'] = ae['ticker'].astype(str).str.upper()\n",
    "ae['date']   = pd.to_datetime(ae['date'], errors='coerce').dt.tz_localize(None)\n",
    "ae = ae.dropna(subset=['date']).sort_values(['ticker','date'])\n",
    "\n",
    "before = ae.groupby('ticker')['shares_outstanding'].apply(lambda s: s.notna().any()).sum()\n",
    "\n",
    "# 1) Fill from SEC events (time-varying) per ticker if available\n",
    "if 'events_enriched' in globals() and not events_enriched.empty:\n",
    "    ev = events_enriched[['ticker','date','shares_outstanding']].copy()\n",
    "    ev['ticker'] = ev['ticker'].astype(str).str.upper()\n",
    "    ev['date']   = pd.to_datetime(ev['date'], errors='coerce').dt.tz_localize(None)\n",
    "    ev = ev.dropna(subset=['date']).sort_values(['ticker','date'])\n",
    "\n",
    "    parts = []\n",
    "    g_ev = ev.groupby('ticker', sort=False)\n",
    "    for tkr, left in ae.groupby('ticker', sort=False):\n",
    "        left = left.sort_values('date').copy()\n",
    "        right = g_ev.get_group(tkr).sort_values('date') if tkr in g_ev.groups else None\n",
    "\n",
    "        if right is None or right.empty:\n",
    "            parts.append(left); continue\n",
    "\n",
    "        j = pd.merge_asof(\n",
    "            left=left,\n",
    "            right=right[['date','shares_outstanding']].rename(\n",
    "                columns={'shares_outstanding':'shares_outstanding_ev'}\n",
    "            ),\n",
    "            on='date',\n",
    "            direction='backward',\n",
    "            allow_exact_matches=True\n",
    "        )\n",
    "        m = j['shares_outstanding'].isna() & j['shares_outstanding_ev'].notna()\n",
    "        j.loc[m, 'shares_outstanding'] = j.loc[m, 'shares_outstanding_ev']\n",
    "        j.drop(columns=['shares_outstanding_ev'], inplace=True, errors='ignore')\n",
    "        parts.append(j)\n",
    "\n",
    "    ae = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "# 2) Static snapshots from yfinance cache (no network)\n",
    "if YF_CACHE.exists():\n",
    "    rows = []\n",
    "    for p in YF_CACHE.glob(\"*.json\"):\n",
    "        try:\n",
    "            j = json.loads(p.read_text())\n",
    "            v = j.get('shares_outstanding_yf')\n",
    "            if v is not None:\n",
    "                rows.append((p.stem.upper(), float(v)))\n",
    "        except Exception:\n",
    "            pass\n",
    "    if rows:\n",
    "        map_sh = dict(rows)\n",
    "        mask = ae['shares_outstanding'].isna()\n",
    "        ae.loc[mask, 'shares_outstanding'] = ae.loc[mask, 'ticker'].map(map_sh)\n",
    "\n",
    "# 3) Last-resort estimate = market_cap / latest adj close (per ticker)\n",
    "adj_col = next((c for c in ['adj close','adj_close','Adj Close'] if c in ae.columns), None)\n",
    "if adj_col is not None and 'market_cap' in ae.columns:\n",
    "    # find the index of latest row (max date) with a non-null adj close per ticker\n",
    "    valid = ae.dropna(subset=[adj_col])\n",
    "    if not valid.empty:\n",
    "        idx_last = (valid.sort_values('date')\n",
    "                         .groupby('ticker')['date']\n",
    "                         .idxmax())\n",
    "        last_px = (ae.loc[idx_last, ['ticker', adj_col]]\n",
    "                     .rename(columns={adj_col: '_ref_px'}))\n",
    "        map_px = last_px.set_index('ticker')['_ref_px'].to_dict()\n",
    "        map_mc = (ae.drop_duplicates('ticker')\n",
    "                    .set_index('ticker')['market_cap']\n",
    "                    .to_dict())\n",
    "\n",
    "        est = {}\n",
    "        for t in ae['ticker'].unique():\n",
    "            px = map_px.get(t); mc = map_mc.get(t)\n",
    "            if px not in (None, 0, np.nan) and mc not in (None, 0, np.nan):\n",
    "                est[t] = float(mc) / float(px)\n",
    "\n",
    "        if est:\n",
    "            mask = ae['shares_outstanding'].isna()\n",
    "            ae.loc[mask, 'shares_outstanding'] = ae.loc[mask, 'ticker'].map(est)\n",
    "\n",
    "# Final tidy: numeric + ffill within ticker\n",
    "ae['shares_outstanding'] = pd.to_numeric(ae['shares_outstanding'], errors='coerce')\n",
    "ae['shares_outstanding'] = ae.groupby('ticker')['shares_outstanding'].ffill()\n",
    "\n",
    "after = ae.groupby('ticker')['shares_outstanding'].apply(lambda s: s.notna().any()).sum()\n",
    "print(f\"shares_outstanding coverage: {before} -> {after} tickers\")\n",
    "\n",
    "analysis_enriched = ae\n",
    "\n",
    "# quick peek on first few rows (AACB/AAPL if present)\n",
    "for t in ['AACB','AAPL']:\n",
    "    if t in analysis_enriched['ticker'].unique():\n",
    "        print(f\"\\n{t} sample:\")\n",
    "        display(analysis_enriched.loc[analysis_enriched['ticker'].eq(t),\n",
    "                  ['date', adj_col if adj_col else 'adj close', 'market_cap',\n",
    "                   'shares_outstanding','float_shares','free_float']].head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "16476816",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_map = {}\n",
    "if 'Adj Close' in analysis_enriched.columns: rename_map['Adj Close'] = 'adj_close'\n",
    "if 'adj close' in analysis_enriched.columns: rename_map['adj close'] = 'adj_close'\n",
    "analysis_enriched = analysis_enriched.rename(columns=rename_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c91c4a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ret</th>\n",
       "      <th>logret</th>\n",
       "      <th>...</th>\n",
       "      <th>cik</th>\n",
       "      <th>sic</th>\n",
       "      <th>sic_desc</th>\n",
       "      <th>recent_form</th>\n",
       "      <th>recent_filing_date</th>\n",
       "      <th>performed_split</th>\n",
       "      <th>performed_reverse_split</th>\n",
       "      <th>shares_outstanding</th>\n",
       "      <th>float_shares</th>\n",
       "      <th>free_float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>AACB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27675000.0</td>\n",
       "      <td>22147902.0</td>\n",
       "      <td>22147902.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>AACB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27675000.0</td>\n",
       "      <td>22147902.0</td>\n",
       "      <td>22147902.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>AACB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27675000.0</td>\n",
       "      <td>22147902.0</td>\n",
       "      <td>22147902.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-06</td>\n",
       "      <td>AACB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27675000.0</td>\n",
       "      <td>22147902.0</td>\n",
       "      <td>22147902.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-07</td>\n",
       "      <td>AACB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27675000.0</td>\n",
       "      <td>22147902.0</td>\n",
       "      <td>22147902.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date ticker  open  high  low  close  adj_close  volume  ret  logret  ...  cik  sic sic_desc recent_form  \\\n",
       "0 2022-10-03   AACB   NaN   NaN  NaN    NaN        NaN    <NA>  NaN     NaN  ...  NaN  NaN      NaN         NaN   \n",
       "1 2022-10-04   AACB   NaN   NaN  NaN    NaN        NaN    <NA>  NaN     NaN  ...  NaN  NaN      NaN         NaN   \n",
       "2 2022-10-05   AACB   NaN   NaN  NaN    NaN        NaN    <NA>  NaN     NaN  ...  NaN  NaN      NaN         NaN   \n",
       "3 2022-10-06   AACB   NaN   NaN  NaN    NaN        NaN    <NA>  NaN     NaN  ...  NaN  NaN      NaN         NaN   \n",
       "4 2022-10-07   AACB   NaN   NaN  NaN    NaN        NaN    <NA>  NaN     NaN  ...  NaN  NaN      NaN         NaN   \n",
       "\n",
       "  recent_filing_date performed_split performed_reverse_split shares_outstanding  float_shares  free_float  \n",
       "0                NaT               0                       0         27675000.0    22147902.0  22147902.0  \n",
       "1                NaT               0                       0         27675000.0    22147902.0  22147902.0  \n",
       "2                NaT               0                       0         27675000.0    22147902.0  22147902.0  \n",
       "3                NaT               0                       0         27675000.0    22147902.0  22147902.0  \n",
       "4                NaT               0                       0         27675000.0    22147902.0  22147902.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_enriched.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7b459182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any adj_close for AACB ?: True\n",
      "Any close for AACB ?: True\n",
      "SIC / SIC desc:    sic sic_desc\n",
      "0  NaN      NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "t = \"AACB\"\n",
    "\n",
    "ae = analysis_enriched.copy()\n",
    "ae['ticker'] = ae['ticker'].astype(str).str.upper()\n",
    "\n",
    "print(\"Any adj_close for\", t, \"?:\", ae.loc[ae['ticker'].eq(t), 'adj_close'].notna().any())\n",
    "print(\"Any close for\", t, \"?:\",     ae.loc[ae['ticker'].eq(t), 'close'].notna().any())\n",
    "print(\"SIC / SIC desc:\", (ae.loc[ae['ticker'].eq(t), ['sic','sic_desc']]\n",
    "                            .drop_duplicates()\n",
    "                            .head(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1821a07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any adj_close for AACB ?: True\n",
      "Any close for AACB ?: True\n",
      "Non-null adj_close rows: 122\n",
      "First/last non-null dates: 2025-04-07 → 2025-09-30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>adj_close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>2025-04-07</td>\n",
       "      <td>9.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>2025-04-08</td>\n",
       "      <td>9.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>2025-04-09</td>\n",
       "      <td>9.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  adj_close\n",
       "629 2025-04-07       9.88\n",
       "630 2025-04-08       9.95\n",
       "631 2025-04-09       9.90"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>adj_close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>2025-09-26</td>\n",
       "      <td>10.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>2025-09-29</td>\n",
       "      <td>10.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>2025-09-30</td>\n",
       "      <td>10.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  adj_close\n",
       "748 2025-09-26      10.18\n",
       "749 2025-09-29      10.18\n",
       "750 2025-09-30      10.18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = \"AACB\"\n",
    "ae = analysis_enriched.copy()\n",
    "ae['ticker'] = ae['ticker'].astype(str).str.upper()\n",
    "\n",
    "col = 'adj_close'\n",
    "print(\"Any adj_close for\", t, \"?:\", ae.loc[ae['ticker'].eq(t), col].notna().any())\n",
    "print(\"Any close for\", t, \"?:\",     ae.loc[ae['ticker'].eq(t), 'close'].notna().any())\n",
    "\n",
    "# How many non-null points and their date span?\n",
    "s = (ae.loc[ae['ticker'].eq(t), ['date', col]]\n",
    "       .dropna()\n",
    "       .sort_values('date'))\n",
    "print(\"Non-null adj_close rows:\", len(s))\n",
    "if not s.empty:\n",
    "    print(\"First/last non-null dates:\", s['date'].iloc[0].date(), \"→\", s['date'].iloc[-1].date())\n",
    "    display(s.head(3))\n",
    "    display(s.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ca6daeed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ret</th>\n",
       "      <th>logret</th>\n",
       "      <th>...</th>\n",
       "      <th>cik</th>\n",
       "      <th>sic</th>\n",
       "      <th>sic_desc</th>\n",
       "      <th>recent_form</th>\n",
       "      <th>recent_filing_date</th>\n",
       "      <th>performed_split</th>\n",
       "      <th>performed_reverse_split</th>\n",
       "      <th>shares_outstanding</th>\n",
       "      <th>float_shares</th>\n",
       "      <th>free_float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>287633</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>ASUR</td>\n",
       "      <td>5.63</td>\n",
       "      <td>5.71</td>\n",
       "      <td>5.21</td>\n",
       "      <td>5.37</td>\n",
       "      <td>5.37</td>\n",
       "      <td>50100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0000884144</td>\n",
       "      <td>7373</td>\n",
       "      <td>Services-Computer Integrated Systems Design</td>\n",
       "      <td>SCHEDULE 13G</td>\n",
       "      <td>2025-09-17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20160000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287634</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>ASUR</td>\n",
       "      <td>5.31</td>\n",
       "      <td>5.49</td>\n",
       "      <td>5.10</td>\n",
       "      <td>5.20</td>\n",
       "      <td>5.20</td>\n",
       "      <td>58600</td>\n",
       "      <td>-0.031657</td>\n",
       "      <td>-0.032169</td>\n",
       "      <td>...</td>\n",
       "      <td>0000884144</td>\n",
       "      <td>7373</td>\n",
       "      <td>Services-Computer Integrated Systems Design</td>\n",
       "      <td>SCHEDULE 13G</td>\n",
       "      <td>2025-09-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287635</th>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>ASUR</td>\n",
       "      <td>5.20</td>\n",
       "      <td>5.31</td>\n",
       "      <td>5.14</td>\n",
       "      <td>5.25</td>\n",
       "      <td>5.25</td>\n",
       "      <td>16000</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.009569</td>\n",
       "      <td>...</td>\n",
       "      <td>0000884144</td>\n",
       "      <td>7373</td>\n",
       "      <td>Services-Computer Integrated Systems Design</td>\n",
       "      <td>SCHEDULE 13G</td>\n",
       "      <td>2025-09-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287636</th>\n",
       "      <td>2022-10-06</td>\n",
       "      <td>ASUR</td>\n",
       "      <td>5.34</td>\n",
       "      <td>5.52</td>\n",
       "      <td>5.16</td>\n",
       "      <td>5.51</td>\n",
       "      <td>5.51</td>\n",
       "      <td>22200</td>\n",
       "      <td>0.049524</td>\n",
       "      <td>0.048337</td>\n",
       "      <td>...</td>\n",
       "      <td>0000884144</td>\n",
       "      <td>7373</td>\n",
       "      <td>Services-Computer Integrated Systems Design</td>\n",
       "      <td>SCHEDULE 13G</td>\n",
       "      <td>2025-09-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287637</th>\n",
       "      <td>2022-10-07</td>\n",
       "      <td>ASUR</td>\n",
       "      <td>5.38</td>\n",
       "      <td>5.40</td>\n",
       "      <td>5.15</td>\n",
       "      <td>5.15</td>\n",
       "      <td>5.15</td>\n",
       "      <td>6300</td>\n",
       "      <td>-0.065336</td>\n",
       "      <td>-0.067568</td>\n",
       "      <td>...</td>\n",
       "      <td>0000884144</td>\n",
       "      <td>7373</td>\n",
       "      <td>Services-Computer Integrated Systems Design</td>\n",
       "      <td>SCHEDULE 13G</td>\n",
       "      <td>2025-09-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845866</th>\n",
       "      <td>2025-09-24</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.50</td>\n",
       "      <td>80600</td>\n",
       "      <td>-0.019608</td>\n",
       "      <td>-0.019803</td>\n",
       "      <td>...</td>\n",
       "      <td>0000846475</td>\n",
       "      <td>3845</td>\n",
       "      <td>Electromedical &amp; Electrotherapeutic Apparatus</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30297442.0</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>2.139729e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845867</th>\n",
       "      <td>2025-09-25</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.47</td>\n",
       "      <td>109300</td>\n",
       "      <td>-0.020000</td>\n",
       "      <td>-0.020203</td>\n",
       "      <td>...</td>\n",
       "      <td>0000846475</td>\n",
       "      <td>3845</td>\n",
       "      <td>Electromedical &amp; Electrotherapeutic Apparatus</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30297442.0</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>2.139729e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845868</th>\n",
       "      <td>2025-09-26</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.45</td>\n",
       "      <td>54700</td>\n",
       "      <td>-0.013605</td>\n",
       "      <td>-0.013699</td>\n",
       "      <td>...</td>\n",
       "      <td>0000846475</td>\n",
       "      <td>3845</td>\n",
       "      <td>Electromedical &amp; Electrotherapeutic Apparatus</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30297442.0</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>2.139729e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845869</th>\n",
       "      <td>2025-09-29</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.42</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.47</td>\n",
       "      <td>115800</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>...</td>\n",
       "      <td>0000846475</td>\n",
       "      <td>3845</td>\n",
       "      <td>Electromedical &amp; Electrotherapeutic Apparatus</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30297442.0</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>2.139729e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845870</th>\n",
       "      <td>2025-09-30</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>1.46</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.45</td>\n",
       "      <td>48200</td>\n",
       "      <td>-0.013605</td>\n",
       "      <td>-0.013699</td>\n",
       "      <td>...</td>\n",
       "      <td>0000846475</td>\n",
       "      <td>3845</td>\n",
       "      <td>Electromedical &amp; Electrotherapeutic Apparatus</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30297442.0</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>2.139729e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98381 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date ticker  open  high   low  close  adj_close  volume       ret    logret  ...         cik   sic  \\\n",
       "287633  2022-10-03   ASUR  5.63  5.71  5.21   5.37       5.37   50100       NaN       NaN  ...  0000884144  7373   \n",
       "287634  2022-10-04   ASUR  5.31  5.49  5.10   5.20       5.20   58600 -0.031657 -0.032169  ...  0000884144  7373   \n",
       "287635  2022-10-05   ASUR  5.20  5.31  5.14   5.25       5.25   16000  0.009615  0.009569  ...  0000884144  7373   \n",
       "287636  2022-10-06   ASUR  5.34  5.52  5.16   5.51       5.51   22200  0.049524  0.048337  ...  0000884144  7373   \n",
       "287637  2022-10-07   ASUR  5.38  5.40  5.15   5.15       5.15    6300 -0.065336 -0.067568  ...  0000884144  7373   \n",
       "...            ...    ...   ...   ...   ...    ...        ...     ...       ...       ...  ...         ...   ...   \n",
       "3845866 2025-09-24   ZYXI  1.55  1.59  1.50   1.50       1.50   80600 -0.019608 -0.019803  ...  0000846475  3845   \n",
       "3845867 2025-09-25   ZYXI  1.51  1.54  1.45   1.47       1.47  109300 -0.020000 -0.020203  ...  0000846475  3845   \n",
       "3845868 2025-09-26   ZYXI  1.47  1.50  1.44   1.45       1.45   54700 -0.013605 -0.013699  ...  0000846475  3845   \n",
       "3845869 2025-09-29   ZYXI  1.45  1.49  1.42   1.47       1.47  115800  0.013793  0.013699  ...  0000846475  3845   \n",
       "3845870 2025-09-30   ZYXI  1.46  1.48  1.45   1.45       1.45   48200 -0.013605 -0.013699  ...  0000846475  3845   \n",
       "\n",
       "                                              sic_desc   recent_form recent_filing_date performed_split  \\\n",
       "287633     Services-Computer Integrated Systems Design  SCHEDULE 13G         2025-09-17               0   \n",
       "287634     Services-Computer Integrated Systems Design  SCHEDULE 13G         2025-09-17               0   \n",
       "287635     Services-Computer Integrated Systems Design  SCHEDULE 13G         2025-09-17               0   \n",
       "287636     Services-Computer Integrated Systems Design  SCHEDULE 13G         2025-09-17               0   \n",
       "287637     Services-Computer Integrated Systems Design  SCHEDULE 13G         2025-09-17               0   \n",
       "...                                                ...           ...                ...             ...   \n",
       "3845866  Electromedical & Electrotherapeutic Apparatus           8-K         2025-09-22               0   \n",
       "3845867  Electromedical & Electrotherapeutic Apparatus           8-K         2025-09-22               0   \n",
       "3845868  Electromedical & Electrotherapeutic Apparatus           8-K         2025-09-22               0   \n",
       "3845869  Electromedical & Electrotherapeutic Apparatus           8-K         2025-09-22               0   \n",
       "3845870  Electromedical & Electrotherapeutic Apparatus           8-K         2025-09-22               0   \n",
       "\n",
       "        performed_reverse_split shares_outstanding  float_shares    free_float  \n",
       "287633                        1         20160000.0           NaN           NaN  \n",
       "287634                        0         20160000.0           NaN           NaN  \n",
       "287635                        0         20160000.0           NaN           NaN  \n",
       "287636                        0         20160000.0           NaN           NaN  \n",
       "287637                        0         20160000.0           NaN           NaN  \n",
       "...                         ...                ...           ...           ...  \n",
       "3845866                       0         30297442.0  2.139729e+07  2.139729e+07  \n",
       "3845867                       0         30297442.0  2.139729e+07  2.139729e+07  \n",
       "3845868                       0         30297442.0  2.139729e+07  2.139729e+07  \n",
       "3845869                       0         30297442.0  2.139729e+07  2.139729e+07  \n",
       "3845870                       0         30297442.0  2.139729e+07  2.139729e+07  \n",
       "\n",
       "[98381 rows x 23 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_enriched[analysis_enriched[\"market_cap\"] < 300000000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b957cc34",
   "metadata": {},
   "source": [
    "1) Setup & column detection (handles adj_close vs adj close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "13eac739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using adjusted close column: adj_close\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "ae = analysis_enriched.copy()\n",
    "ae['ticker'] = ae['ticker'].astype(str).str.upper()\n",
    "ae['date']   = pd.to_datetime(ae['date']).dt.tz_localize(None)\n",
    "\n",
    "# Find the adjusted close column robustly\n",
    "ADJ_COLS = ['adj_close','adj close','Adj Close','adjClose']\n",
    "adj_col = next((c for c in ADJ_COLS if c in ae.columns), None)\n",
    "print(\"Using adjusted close column:\", adj_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20f551e",
   "metadata": {},
   "source": [
    "2) Schema, key uniqueness, date window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9c32f271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 3845871 | Columns: 23\n",
      "Columns: ['date', 'ticker', 'open', 'high', 'low', 'close', 'adj_close', 'volume', 'ret', 'logret', 'market_cap', 'sector', 'industry', 'cik', 'sic', 'sic_desc', 'recent_form', 'recent_filing_date', 'performed_split', 'performed_reverse_split', 'shares_outstanding', 'float_shares', 'free_float']\n",
      "Duplicate (ticker,date) rows: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>first_date</th>\n",
       "      <th>last_date</th>\n",
       "      <th>n_rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AACB</td>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>2025-09-30</td>\n",
       "      <td>751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AACBR</td>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>2025-09-30</td>\n",
       "      <td>751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AACBU</td>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>2025-09-30</td>\n",
       "      <td>751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AACG</td>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>2025-09-30</td>\n",
       "      <td>751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AACI</td>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>2025-09-30</td>\n",
       "      <td>751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker first_date  last_date  n_rows\n",
       "0   AACB 2022-10-03 2025-09-30     751\n",
       "1  AACBR 2022-10-03 2025-09-30     751\n",
       "2  AACBU 2022-10-03 2025-09-30     751\n",
       "3   AACG 2022-10-03 2025-09-30     751\n",
       "4   AACI 2022-10-03 2025-09-30     751"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tickers: 5121\n"
     ]
    }
   ],
   "source": [
    "print(\"Rows:\", len(ae), \"| Columns:\", len(ae.columns))\n",
    "print(\"Columns:\", list(ae.columns))\n",
    "\n",
    "dups = ae.duplicated(['ticker','date']).sum()\n",
    "print(\"Duplicate (ticker,date) rows:\", dups)\n",
    "\n",
    "per_tkr = (\n",
    "    ae.groupby('ticker')\n",
    "      .agg(first_date=('date','min'), last_date=('date','max'), n_rows=('date','size'))\n",
    "      .reset_index()\n",
    ")\n",
    "display(per_tkr.head())\n",
    "print(\"Tickers:\", len(per_tkr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36376e40",
   "metadata": {},
   "source": [
    "3) Missingness & coverage snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "13b98f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>null_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>market_cap</th>\n",
       "      <td>0.954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sector</th>\n",
       "      <td>0.948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>industry</th>\n",
       "      <td>0.948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recent_filing_date</th>\n",
       "      <td>0.947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recent_form</th>\n",
       "      <td>0.947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cik</th>\n",
       "      <td>0.947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>float_shares</th>\n",
       "      <td>0.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free_float</th>\n",
       "      <td>0.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shares_outstanding</th>\n",
       "      <td>0.324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logret</th>\n",
       "      <td>0.252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    null_rate\n",
       "market_cap              0.954\n",
       "sector                  0.948\n",
       "industry                0.948\n",
       "recent_filing_date      0.947\n",
       "recent_form             0.947\n",
       "...                       ...\n",
       "cik                     0.947\n",
       "float_shares            0.350\n",
       "free_float              0.350\n",
       "shares_outstanding      0.324\n",
       "logret                  0.252\n",
       "\n",
       "[12 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tickers with any data by field:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tickers_with_any</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>open</th>\n",
       "      <td>5118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high</th>\n",
       "      <td>5118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low</th>\n",
       "      <td>5118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>close</th>\n",
       "      <td>5118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adj_close</th>\n",
       "      <td>5118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logret</th>\n",
       "      <td>4730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>float_shares</th>\n",
       "      <td>3966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>free_float</th>\n",
       "      <td>3966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shares_outstanding</th>\n",
       "      <td>3705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>market_cap</th>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    tickers_with_any\n",
       "open                            5118\n",
       "high                            5118\n",
       "low                             5118\n",
       "close                           5118\n",
       "adj_close                       5118\n",
       "...                              ...\n",
       "logret                          4730\n",
       "float_shares                    3966\n",
       "free_float                      3966\n",
       "shares_outstanding              3705\n",
       "market_cap                       235\n",
       "\n",
       "[12 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Overall nulls\n",
    "nulls = ae.isna().mean().sort_values(ascending=False).round(3)\n",
    "display(nulls.to_frame('null_rate').head(12))\n",
    "\n",
    "# Coverage by ticker for core fields\n",
    "core = ['open','high','low','close', adj_col, 'volume',\n",
    "        'ret','logret','market_cap','shares_outstanding','float_shares','free_float']\n",
    "core = [c for c in core if c in ae.columns]\n",
    "\n",
    "cov = (ae.groupby('ticker')[core].apply(lambda df: df.notna().any()).sum().sort_values(ascending=False))\n",
    "print(\"Tickers with any data by field:\")\n",
    "display(cov.to_frame('tickers_with_any'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce0a971",
   "metadata": {},
   "source": [
    "4) Price integrity rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "176180a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price integrity issues: {'high_below': np.int64(104), 'low_above': np.int64(111), 'neg_volume': np.int64(0), 'nonpos_prices': np.int64(0)}\n"
     ]
    }
   ],
   "source": [
    "issues = {}\n",
    "\n",
    "if all(c in ae.columns for c in ['high','open','close','low']):\n",
    "    issues['high_below'] = (ae['high'] < ae[['open','close','low']].max(axis=1)).sum()\n",
    "    issues['low_above']  = (ae['low']  > ae[['open','close','low']].min(axis=1)).sum()\n",
    "\n",
    "if 'volume' in ae.columns:\n",
    "    issues['neg_volume'] = (ae['volume'] < 0).sum()\n",
    "\n",
    "if 'close' in ae.columns and adj_col:\n",
    "    issues['nonpos_prices'] = ((ae['close'] <= 0) | (ae[adj_col] <= 0)).sum()\n",
    "\n",
    "print(\"Price integrity issues:\", issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb98d5b5",
   "metadata": {},
   "source": [
    "5) Return sanity: recompute and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8d7ecffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ret match: 1.000000\n",
      "logret match: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# Recompute exactly how the dataset was built, but keep index alignment\n",
    "tmp = ae.sort_values(['ticker','date'])[['ticker', adj_col, 'ret', 'logret']].copy()\n",
    "\n",
    "# Index-aligned recomputes (no .apply)\n",
    "ref_ret = tmp.groupby('ticker')[adj_col].pct_change(fill_method=None)\n",
    "ref_log = np.log(tmp[adj_col] / tmp.groupby('ticker')[adj_col].shift(1))\n",
    "\n",
    "# Compare only where both sides are present\n",
    "m_ret = tmp['ret'].notna() & ref_ret.notna()\n",
    "m_log = tmp['logret'].notna() & ref_log.notna()\n",
    "\n",
    "ok_ret = np.isclose(tmp.loc[m_ret, 'ret'].to_numpy(),\n",
    "                    ref_ret.loc[m_ret].to_numpy(), rtol=1e-10).mean()\n",
    "ok_log = np.isclose(tmp.loc[m_log, 'logret'].to_numpy(),\n",
    "                    ref_log.loc[m_log].to_numpy(), rtol=1e-10).mean()\n",
    "\n",
    "print(f\"ret match: {ok_ret:.6f}\")\n",
    "print(f\"logret match: {ok_log:.6f}\")\n",
    "\n",
    "# (optional) peek at any mismatches\n",
    "bad_ret = m_ret & ~np.isclose(tmp['ret'], ref_ret, rtol=1e-10)\n",
    "if bad_ret.any():\n",
    "    display(tmp.loc[bad_ret, ['ticker','date','ret']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cc9c1a",
   "metadata": {},
   "source": [
    "6) Cross-field coherence: market cap vs shares × price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "846ec61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCAP / (shares * adj_close) quantiles:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.01     0.000\n",
       "0.05     0.010\n",
       "0.50     1.005\n",
       "0.95     4.505\n",
       "0.99    43.962\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with very off ratios: 29297\n"
     ]
    }
   ],
   "source": [
    "if {'market_cap','shares_outstanding'}.issubset(ae.columns) and adj_col:\n",
    "    m = ae[['ticker','date','market_cap','shares_outstanding',adj_col]].dropna()\n",
    "    ratio = (m['market_cap'] / (m['shares_outstanding'] * m[adj_col])).replace([np.inf,-np.inf], np.nan)\n",
    "    q = ratio.quantile([0.01,0.05,0.5,0.95,0.99]).round(3)\n",
    "    print(\"MCAP / (shares * adj_close) quantiles:\")\n",
    "    display(q)\n",
    "    bad = m.loc[(ratio < 0.2) | (ratio > 5.0)]\n",
    "    print(\"Rows with very off ratios:\", len(bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2e9b9580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extreme mktcap ratios: 29677\n"
     ]
    }
   ],
   "source": [
    "core = ae[['ticker','date','market_cap','shares_outstanding', adj_col]].dropna()\n",
    "ratio = core['market_cap'] / (core['shares_outstanding'] * core[adj_col])\n",
    "bad   = ratio[(ratio < 0.2) | (ratio > 5)]  # adjust bands to taste\n",
    "print(\"Extreme mktcap ratios:\", len(bad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c89698",
   "metadata": {},
   "source": [
    "7) Float logic checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6f32200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ae['float_shares'].notna() & ae['shares_outstanding'].notna()\n",
    "over = m & (ae['float_shares'] > ae['shares_outstanding'] * 1.2)  # hard cap at 120%\n",
    "ae.loc[over, 'float_shares'] = ae.loc[over, 'shares_outstanding']\n",
    "ae['free_float'] = ae['float_shares']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "66838e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float_shares ≤ shares_outstanding fraction: 0.8244\n",
      "free_float equals float_shares fraction (after fillna sentinel): 1.0\n"
     ]
    }
   ],
   "source": [
    "if 'float_shares' in ae.columns and 'shares_outstanding' in ae.columns:\n",
    "    cond = (ae['float_shares'] <= ae['shares_outstanding']) | ae['shares_outstanding'].isna()\n",
    "    print(\"float_shares ≤ shares_outstanding fraction:\", round(cond.mean(),4))\n",
    "\n",
    "if 'free_float' in ae.columns and 'float_shares' in ae.columns:\n",
    "    eq = (ae['free_float'].fillna(-1) == ae['float_shares'].fillna(-1)).mean()\n",
    "    print(\"free_float equals float_shares fraction (after fillna sentinel):\", round(float(eq),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8914cf50",
   "metadata": {},
   "source": [
    "8) Split flags sanity (no both flags same day; big price moves around split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1179d8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with both split & reverse flags ON: 0\n",
      "Median prev/next price ratio on split days (expect >1): 0.999\n",
      "Median prev/next price ratio on reverse-split days (expect <1): 1.026\n"
     ]
    }
   ],
   "source": [
    "if {'performed_split','performed_reverse_split'}.issubset(ae.columns):\n",
    "    both = ae[(ae['performed_split'] == 1) & (ae['performed_reverse_split'] == 1)]\n",
    "    print(\"Rows with both split & reverse flags ON:\", len(both))\n",
    "\n",
    "    # crude price-ratio check around flagged days\n",
    "    if adj_col:\n",
    "        g = ae.sort_values(['ticker','date']).copy()\n",
    "        prev = g.groupby('ticker')[adj_col].shift(1)\n",
    "        ratio = prev / g[adj_col]\n",
    "        around_split = g.loc[g['performed_split']==1, ['ticker','date']].assign(ratio=ratio[g['performed_split']==1])\n",
    "        around_rev   = g.loc[g['performed_reverse_split']==1, ['ticker','date']].assign(ratio=ratio[g['performed_reverse_split']==1])\n",
    "        print(\"Median prev/next price ratio on split days (expect >1):\", round(around_split['ratio'].median(skipna=True),3))\n",
    "        print(\"Median prev/next price ratio on reverse-split days (expect <1):\", round(around_rev['ratio'].median(skipna=True),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca2a802",
   "metadata": {},
   "source": [
    "9) Ticker spot-check helper (quick look at any symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "76065282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spotcheck(tkr, n=8):\n",
    "    t = str(tkr).upper()\n",
    "    cols = ['date','open','high','low','close',adj_col,'volume','ret','logret',\n",
    "            'shares_outstanding','float_shares','free_float','performed_split','performed_reverse_split']\n",
    "    cols = [c for c in cols if c in ae.columns]\n",
    "    df = ae.loc[ae['ticker'].eq(t), cols].sort_values('date')\n",
    "    print(f\"{t}: rows={len(df)} | first={df['date'].min().date() if len(df) else None} | last={df['date'].max().date() if len(df) else None}\")\n",
    "    display(df.head(n))\n",
    "    display(df.tail(n))\n",
    "\n",
    "# Example:\n",
    "# spotcheck(\"AAPL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308f36d8",
   "metadata": {},
   "source": [
    "10) Save a tiny QC summary table (handy when you rerun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f4d4fc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rows</th>\n",
       "      <th>tickers</th>\n",
       "      <th>dupe_key_rows</th>\n",
       "      <th>pct_null_adj_close</th>\n",
       "      <th>tickers_with_any_shares_outstanding</th>\n",
       "      <th>tickers_with_any_float</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3845871</td>\n",
       "      <td>5121</td>\n",
       "      <td>0</td>\n",
       "      <td>0.251105</td>\n",
       "      <td>3705</td>\n",
       "      <td>3966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rows  tickers  dupe_key_rows  pct_null_adj_close  tickers_with_any_shares_outstanding  tickers_with_any_float\n",
       "0  3845871     5121              0            0.251105                                 3705                    3966"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qc = pd.DataFrame({\n",
    "    'rows':[len(ae)],\n",
    "    'tickers':[ae['ticker'].nunique()],\n",
    "    'dupe_key_rows':[dups],\n",
    "    'pct_null_adj_close':[ae[adj_col].isna().mean() if adj_col else np.nan],\n",
    "    'tickers_with_any_shares_outstanding':[ae.groupby('ticker')['shares_outstanding'].apply(lambda s: s.notna().any()).sum() if 'shares_outstanding' in ae else np.nan],\n",
    "    'tickers_with_any_float':[ae.groupby('ticker')['float_shares'].apply(lambda s: s.notna().any()).sum() if 'float_shares' in ae else np.nan],\n",
    "})\n",
    "display(qc)\n",
    "qc.to_csv(\"outputs/qc_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a726ae48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NaN, '6-K', '4', 'SCHEDULE 13G', 'S-3ASR', ..., '424B3', 'F-3', 'F-1', 'S-8', '20-F']\n",
       "Length: 30\n",
       "Categories (29, object): ['10-Q', '10-Q/A', '144', '20-F', ..., 'SCHEDULE 13D', 'SCHEDULE 13D/A', 'SCHEDULE 13G', 'SCHEDULE 13G/A']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_enriched[\"recent_form\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "86622c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yx/5t5dfjxj4pv8tjn2zvbfc2540000gn/T/ipykernel_4700/3701159746.py:56: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  analysis_enriched = pd.concat(parts, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows with filing_form: 2338594\n",
      "tickers with ≥1 filing: 3335\n"
     ]
    }
   ],
   "source": [
    "# ---- run-mode flag (near your other flags) ----\n",
    "REBUILD_FILINGS = False   # set True only when you want to refresh filings from sec_cache\n",
    "\n",
    "# --- SKIP GUARD: avoid re-annotating if already present ---\n",
    "if ('filing_form' in analysis_enriched.columns\n",
    "    and analysis_enriched['filing_form'].notna().any()\n",
    "    and not REBUILD_FILINGS):\n",
    "    print(\"[filings] Annotation already present; skipping (set REBUILD_FILINGS=True to refresh).\")\n",
    "else:\n",
    "    # === Annotate using prebuilt `filings` snapshot (fast, no re-parse) ===\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "\n",
    "    OUT = Path(\"outputs\")\n",
    "    P_FILINGS = OUT / \"filings_recent.parquet\"\n",
    "    TOL_DAYS = 365\n",
    "\n",
    "    # Read the snapshot that you’ve already built\n",
    "    filings = pd.read_parquet(P_FILINGS).rename(columns={'filing_date':'event_date', 'form':'filing_form'})\n",
    "    filings['ticker'] = filings['ticker'].astype(str).str.upper()\n",
    "    filings['event_date'] = pd.to_datetime(filings['event_date'], errors='coerce').dt.tz_localize(None).dt.normalize()\n",
    "    filings = filings[['ticker','event_date','filing_form']].sort_values(['ticker','event_date'])\n",
    "\n",
    "    # Prep left (prices panel)\n",
    "    ae = analysis_enriched.copy()\n",
    "    ae['ticker'] = ae['ticker'].astype(str).str.upper()\n",
    "    ae['date']   = pd.to_datetime(ae['date'], errors='coerce').dt.tz_localize(None).dt.normalize()\n",
    "    ae = ae.drop(columns=['filing_form','is_filing_day','last_filing_date','days_since_filing',\n",
    "                          'event_date','filing_form_x','filing_form_y'], errors='ignore')\n",
    "    ae = ae.dropna(subset=['ticker','date']).sort_values(['ticker','date'])\n",
    "\n",
    "    tol = pd.Timedelta(f'{TOL_DAYS}D')\n",
    "    parts = []\n",
    "    for tkr, left_t in ae.groupby('ticker', sort=False):\n",
    "        left_t = left_t.sort_values('date').copy()\n",
    "        right_t = filings.loc[filings['ticker'].eq(tkr), ['event_date','filing_form']].sort_values('event_date')\n",
    "\n",
    "        if right_t.empty:\n",
    "            left_t['filing_form']       = pd.NA\n",
    "            left_t['last_filing_date']  = pd.NaT\n",
    "            left_t['is_filing_day']     = False\n",
    "            left_t['days_since_filing'] = pd.NA\n",
    "        else:\n",
    "            j = pd.merge_asof(\n",
    "                left=left_t[['date']], right=right_t,\n",
    "                left_on='date', right_on='event_date',\n",
    "                direction='backward', allow_exact_matches=True, tolerance=tol\n",
    "            )\n",
    "            left_t['filing_form']       = j['filing_form'].values\n",
    "            left_t['last_filing_date']  = j['event_date'].values\n",
    "            left_t['is_filing_day']     = left_t['date'].eq(left_t['last_filing_date'])\n",
    "            left_t['days_since_filing'] = (left_t['date'] - left_t['last_filing_date']).dt.days\n",
    "\n",
    "        parts.append(left_t)\n",
    "\n",
    "    analysis_enriched = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "    # Quick checks\n",
    "    rows_with = int(analysis_enriched['filing_form'].notna().sum())\n",
    "    tickers_with = int(analysis_enriched.groupby('ticker')['filing_form'].apply(lambda s: s.notna().any()).sum())\n",
    "    print(\"rows with filing_form:\", rows_with)\n",
    "    print(\"tickers with ≥1 filing:\", tickers_with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5da8192a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filings columns present: True | rows: 2338594 | tickers: 3335\n"
     ]
    }
   ],
   "source": [
    "cols_ok = {'filing_form','is_filing_day'}.issubset(analysis_enriched.columns)\n",
    "non_null_rows = int(analysis_enriched['filing_form'].notna().sum())\n",
    "tickers_with  = int(analysis_enriched.groupby('ticker')['filing_form'].apply(lambda s: s.notna().any()).sum())\n",
    "print(\"filings columns present:\", cols_ok, \"| rows:\", non_null_rows, \"| tickers:\", tickers_with)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "20ea0733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "share of rows with filing_form: 0.608\n",
      "unique filing event rows: 334067\n",
      "tickers with ≥1 filing event: 3335\n",
      "median events/ticker: 97\n",
      "mean events/ticker: 100.17\n",
      "filing_form\n",
      "4           80436\n",
      "8-K         74139\n",
      "144         28363\n",
      "6-K         26642\n",
      "10-Q        12818\n",
      "SC 13G/A    12814\n",
      "UPLOAD       8916\n",
      "3            6561\n",
      "CORRESP      6344\n",
      "DEF 14A      5305\n",
      "Name: count, dtype: int64\n",
      "            date filing_form\n",
      "12017 2022-10-04           4\n",
      "12027 2022-10-18           4\n",
      "12034 2022-10-27         8-K\n",
      "12035 2022-10-28        10-K\n",
      "12037 2022-11-01           4\n",
      "12041 2022-11-07         8-K\n",
      "12043 2022-11-09      25-NSE\n",
      "12053 2022-11-23           4\n",
      "12086 2023-01-12     DEF 14A\n",
      "12092 2023-01-23     PX14A6G\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ae = analysis_enriched\n",
    "\n",
    "# 1) What fraction of all rows now have a filing form?\n",
    "coverage = ae['filing_form'].notna().mean()\n",
    "print(\"share of rows with filing_form:\", round(float(coverage), 3))\n",
    "\n",
    "# 2) How many *actual filing events* (not carry-forward)?\n",
    "events = ae.loc[ae['is_filing_day'] == True, ['ticker','date','filing_form']].drop_duplicates()\n",
    "print(\"unique filing event rows:\", len(events))\n",
    "print(\"tickers with ≥1 filing event:\", events['ticker'].nunique())\n",
    "\n",
    "# 3) Event count per ticker (median/mean) — should be far smaller than total daily rows\n",
    "per_tkr = events.groupby('ticker').size()\n",
    "print(\"median events/ticker:\", int(per_tkr.median()))\n",
    "print(\"mean events/ticker:\", round(float(per_tkr.mean()), 2))\n",
    "\n",
    "# 4) Top 10 filing forms by event-day count\n",
    "print(ae.loc[ae['is_filing_day'] == True, 'filing_form'].value_counts().head(10))\n",
    "\n",
    "# 5) Spot-check a ticker’s event days only (e.g., AAPL)\n",
    "print(ae.query(\"ticker == 'AAPL' and is_filing_day\")[['date','filing_form']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ecc33c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique filing events: 1286534\n",
      "tickers with ≥1 filing: 3357\n",
      "\n",
      "Top forms by event count:\n",
      "form\n",
      "4           327407\n",
      "8-K         280107\n",
      "10-Q         76999\n",
      "6-K          74339\n",
      "SC 13G/A     55677\n",
      "144          33719\n",
      "3            33514\n",
      "CORRESP      30493\n",
      "UPLOAD       30241\n",
      "SC 13G       25815\n",
      "Name: count, dtype: int64\n",
      "\n",
      "AAPL events (first 10):\n",
      "     filing_date     form\n",
      "2994  2014-11-13  CERTNYS\n",
      "2995  2014-11-21        4\n",
      "2996  2014-11-25        4\n",
      "2997  2014-12-04   NO ACT\n",
      "2998  2014-12-11   NO ACT\n",
      "2999  2014-12-23        4\n",
      "3000  2014-12-29   NO ACT\n",
      "3001  2014-12-30   NO ACT\n",
      "3002  2015-01-02        4\n",
      "3004  2015-01-22  DEF 14A\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "P_FILINGS = Path(\"outputs/filings_recent.parquet\")\n",
    "fil = pd.read_parquet(P_FILINGS)\n",
    "\n",
    "# Basic stats\n",
    "print(\"unique filing events:\", len(fil))\n",
    "print(\"tickers with ≥1 filing:\", fil['ticker'].nunique())\n",
    "print(\"\\nTop forms by event count:\")\n",
    "print(fil['form'].value_counts().head(10))\n",
    "\n",
    "# Example: AAPL’s events (if present)\n",
    "print(\"\\nAAPL events (first 10):\")\n",
    "print(fil.query(\"ticker=='AAPL'\")[['filing_date','form']].sort_values('filing_date').head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e11a5727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved with filings columns and mirrored to canonical names.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "OUT = Path(\"outputs\"); OUT.mkdir(exist_ok=True)\n",
    "\n",
    "# Save (compact, fast to reload)\n",
    "p_with_pq  = OUT / \"analysis_enriched_with_filings.parquet\"\n",
    "p_with_csv = OUT / \"analysis_enriched_with_filings.csv\"\n",
    "analysis_enriched.to_parquet(p_with_pq, index=False, compression=\"zstd\")\n",
    "analysis_enriched.to_csv(p_with_csv, index=False)\n",
    "\n",
    "# Tiny schema manifest\n",
    "schema = {c: str(t) for c, t in analysis_enriched.dtypes.items()}\n",
    "(OUT / \"analysis_enriched_with_filings.schema.json\").write_text(json.dumps(schema, indent=2))\n",
    "\n",
    "# Mirror to canonical filenames (byte-for-byte)\n",
    "p_can_pq  = OUT / \"analysis_enriched.parquet\"\n",
    "p_can_csv = OUT / \"analysis_enriched.csv\"\n",
    "p_can_pq.write_bytes(p_with_pq.read_bytes())\n",
    "p_can_csv.write_bytes(p_with_csv.read_bytes())\n",
    "\n",
    "print(\"Saved with filings columns and mirrored to canonical names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c5e70d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows with filing_form: 2338594\n",
      "tickers with ≥1 filing: 3335\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>filing_form</th>\n",
       "      <th>is_filing_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>AACB</td>\n",
       "      <td>2024-08-20</td>\n",
       "      <td>DRS</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>AACB</td>\n",
       "      <td>2024-08-21</td>\n",
       "      <td>DRS</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>AACB</td>\n",
       "      <td>2024-08-22</td>\n",
       "      <td>DRS</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>AACB</td>\n",
       "      <td>2024-08-23</td>\n",
       "      <td>DRS</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>AACB</td>\n",
       "      <td>2024-08-26</td>\n",
       "      <td>DRS</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>AACB</td>\n",
       "      <td>2024-09-11</td>\n",
       "      <td>DRS</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>AACB</td>\n",
       "      <td>2024-09-12</td>\n",
       "      <td>DRS</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>AACB</td>\n",
       "      <td>2024-09-13</td>\n",
       "      <td>DRS</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>AACB</td>\n",
       "      <td>2024-09-16</td>\n",
       "      <td>UPLOAD</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>AACB</td>\n",
       "      <td>2024-09-17</td>\n",
       "      <td>UPLOAD</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ticker       date filing_form  is_filing_day\n",
       "472   AACB 2024-08-20         DRS           True\n",
       "473   AACB 2024-08-21         DRS          False\n",
       "474   AACB 2024-08-22         DRS          False\n",
       "475   AACB 2024-08-23         DRS          False\n",
       "476   AACB 2024-08-26         DRS          False\n",
       "..     ...        ...         ...            ...\n",
       "487   AACB 2024-09-11         DRS          False\n",
       "488   AACB 2024-09-12         DRS          False\n",
       "489   AACB 2024-09-13         DRS          False\n",
       "490   AACB 2024-09-16      UPLOAD           True\n",
       "491   AACB 2024-09-17      UPLOAD          False\n",
       "\n",
       "[20 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>filing_form</th>\n",
       "      <th>is_filing_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12016</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12017</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12018</th>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12019</th>\n",
       "      <td>2022-10-06</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12020</th>\n",
       "      <td>2022-10-07</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12026</th>\n",
       "      <td>2022-10-17</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12027</th>\n",
       "      <td>2022-10-18</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12028</th>\n",
       "      <td>2022-10-19</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12029</th>\n",
       "      <td>2022-10-20</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12030</th>\n",
       "      <td>2022-10-21</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date filing_form  is_filing_day\n",
       "12016 2022-10-03           4          False\n",
       "12017 2022-10-04           4           True\n",
       "12018 2022-10-05           4          False\n",
       "12019 2022-10-06           4          False\n",
       "12020 2022-10-07           4          False\n",
       "...          ...         ...            ...\n",
       "12026 2022-10-17           4          False\n",
       "12027 2022-10-18           4           True\n",
       "12028 2022-10-19           4          False\n",
       "12029 2022-10-20           4          False\n",
       "12030 2022-10-21           4          False\n",
       "\n",
       "[15 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rows</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filing_form</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>529741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8-K</th>\n",
       "      <td>528086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6-K</th>\n",
       "      <td>265074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10-Q</th>\n",
       "      <td>131379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SC 13G/A</th>\n",
       "      <td>80908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SC 13G</th>\n",
       "      <td>36403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10-K</th>\n",
       "      <td>32993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CORRESP</th>\n",
       "      <td>26079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424B3</th>\n",
       "      <td>24179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SC 13D/A</th>\n",
       "      <td>23037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               rows\n",
       "filing_form        \n",
       "4            529741\n",
       "8-K          528086\n",
       "6-K          265074\n",
       "10-Q         131379\n",
       "SC 13G/A      80908\n",
       "...             ...\n",
       "SC 13G        36403\n",
       "10-K          32993\n",
       "CORRESP       26079\n",
       "424B3         24179\n",
       "SC 13D/A      23037\n",
       "\n",
       "[15 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "share of annotated rows that fall exactly on filing day: 0.143\n"
     ]
    }
   ],
   "source": [
    "# How many rows got a filing_form?\n",
    "total_rows_with_form = analysis_enriched['filing_form'].notna().sum()\n",
    "tickers_with_form = (analysis_enriched\n",
    "                     .groupby('ticker')['filing_form']\n",
    "                     .apply(lambda s: s.notna().any())\n",
    "                     .sum())\n",
    "print(\"rows with filing_form:\", int(total_rows_with_form))\n",
    "print(\"tickers with ≥1 filing:\", int(tickers_with_form))\n",
    "\n",
    "# Show a few examples anywhere filing_form is present\n",
    "display(analysis_enriched.loc[analysis_enriched['filing_form'].notna(),\n",
    "                              ['ticker','date','filing_form','is_filing_day']].head(20))\n",
    "\n",
    "# Check a specific ticker (AAPL as example)\n",
    "t = \"AAPL\"\n",
    "display(analysis_enriched.loc[analysis_enriched['ticker'].eq(t) &\n",
    "                              analysis_enriched['filing_form'].notna(),\n",
    "                              ['date','filing_form','is_filing_day']].head(15))\n",
    "\n",
    "# Count of forms (top 15)\n",
    "display(analysis_enriched.loc[analysis_enriched['filing_form'].notna(), 'filing_form']\n",
    "        .value_counts()\n",
    "        .head(15)\n",
    "        .to_frame('rows'))\n",
    "\n",
    "# What fraction of annotated rows are exact filing days?\n",
    "if total_rows_with_form:\n",
    "    exact = analysis_enriched.loc[analysis_enriched['filing_form'].notna(), 'is_filing_day'].mean()\n",
    "    print(\"share of annotated rows that fall exactly on filing day:\", round(float(exact), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "25322315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_enriched.filing_form.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bcef7dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ret</th>\n",
       "      <th>logret</th>\n",
       "      <th>...</th>\n",
       "      <th>recent_filing_date</th>\n",
       "      <th>performed_split</th>\n",
       "      <th>performed_reverse_split</th>\n",
       "      <th>shares_outstanding</th>\n",
       "      <th>float_shares</th>\n",
       "      <th>free_float</th>\n",
       "      <th>filing_form</th>\n",
       "      <th>last_filing_date</th>\n",
       "      <th>is_filing_day</th>\n",
       "      <th>days_since_filing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>287633</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>ASUR</td>\n",
       "      <td>5.63</td>\n",
       "      <td>5.71</td>\n",
       "      <td>5.21</td>\n",
       "      <td>5.37</td>\n",
       "      <td>5.37</td>\n",
       "      <td>50100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-09-17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20160000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-08-15</td>\n",
       "      <td>False</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287634</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>ASUR</td>\n",
       "      <td>5.31</td>\n",
       "      <td>5.49</td>\n",
       "      <td>5.10</td>\n",
       "      <td>5.20</td>\n",
       "      <td>5.20</td>\n",
       "      <td>58600</td>\n",
       "      <td>-0.031657</td>\n",
       "      <td>-0.032169</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-09-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-08-15</td>\n",
       "      <td>False</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287635</th>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>ASUR</td>\n",
       "      <td>5.20</td>\n",
       "      <td>5.31</td>\n",
       "      <td>5.14</td>\n",
       "      <td>5.25</td>\n",
       "      <td>5.25</td>\n",
       "      <td>16000</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.009569</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-09-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-08-15</td>\n",
       "      <td>False</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287636</th>\n",
       "      <td>2022-10-06</td>\n",
       "      <td>ASUR</td>\n",
       "      <td>5.34</td>\n",
       "      <td>5.52</td>\n",
       "      <td>5.16</td>\n",
       "      <td>5.51</td>\n",
       "      <td>5.51</td>\n",
       "      <td>22200</td>\n",
       "      <td>0.049524</td>\n",
       "      <td>0.048337</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-09-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-08-15</td>\n",
       "      <td>False</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287637</th>\n",
       "      <td>2022-10-07</td>\n",
       "      <td>ASUR</td>\n",
       "      <td>5.38</td>\n",
       "      <td>5.40</td>\n",
       "      <td>5.15</td>\n",
       "      <td>5.15</td>\n",
       "      <td>5.15</td>\n",
       "      <td>6300</td>\n",
       "      <td>-0.065336</td>\n",
       "      <td>-0.067568</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-09-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20160000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-08-15</td>\n",
       "      <td>False</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845866</th>\n",
       "      <td>2025-09-24</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.50</td>\n",
       "      <td>80600</td>\n",
       "      <td>-0.019608</td>\n",
       "      <td>-0.019803</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30297442.0</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845867</th>\n",
       "      <td>2025-09-25</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.47</td>\n",
       "      <td>109300</td>\n",
       "      <td>-0.020000</td>\n",
       "      <td>-0.020203</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30297442.0</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845868</th>\n",
       "      <td>2025-09-26</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.45</td>\n",
       "      <td>54700</td>\n",
       "      <td>-0.013605</td>\n",
       "      <td>-0.013699</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30297442.0</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>False</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845869</th>\n",
       "      <td>2025-09-29</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.42</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.47</td>\n",
       "      <td>115800</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30297442.0</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>False</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845870</th>\n",
       "      <td>2025-09-30</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>1.46</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.45</td>\n",
       "      <td>48200</td>\n",
       "      <td>-0.013605</td>\n",
       "      <td>-0.013699</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30297442.0</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>False</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98381 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date ticker  open  high   low  close  adj_close  volume       ret    logret  ...  recent_filing_date  \\\n",
       "287633  2022-10-03   ASUR  5.63  5.71  5.21   5.37       5.37   50100       NaN       NaN  ...          2025-09-17   \n",
       "287634  2022-10-04   ASUR  5.31  5.49  5.10   5.20       5.20   58600 -0.031657 -0.032169  ...          2025-09-17   \n",
       "287635  2022-10-05   ASUR  5.20  5.31  5.14   5.25       5.25   16000  0.009615  0.009569  ...          2025-09-17   \n",
       "287636  2022-10-06   ASUR  5.34  5.52  5.16   5.51       5.51   22200  0.049524  0.048337  ...          2025-09-17   \n",
       "287637  2022-10-07   ASUR  5.38  5.40  5.15   5.15       5.15    6300 -0.065336 -0.067568  ...          2025-09-17   \n",
       "...            ...    ...   ...   ...   ...    ...        ...     ...       ...       ...  ...                 ...   \n",
       "3845866 2025-09-24   ZYXI  1.55  1.59  1.50   1.50       1.50   80600 -0.019608 -0.019803  ...          2025-09-22   \n",
       "3845867 2025-09-25   ZYXI  1.51  1.54  1.45   1.47       1.47  109300 -0.020000 -0.020203  ...          2025-09-22   \n",
       "3845868 2025-09-26   ZYXI  1.47  1.50  1.44   1.45       1.45   54700 -0.013605 -0.013699  ...          2025-09-22   \n",
       "3845869 2025-09-29   ZYXI  1.45  1.49  1.42   1.47       1.47  115800  0.013793  0.013699  ...          2025-09-22   \n",
       "3845870 2025-09-30   ZYXI  1.46  1.48  1.45   1.45       1.45   48200 -0.013605 -0.013699  ...          2025-09-22   \n",
       "\n",
       "        performed_split performed_reverse_split shares_outstanding  float_shares    free_float filing_form  \\\n",
       "287633                0                       1         20160000.0           NaN           NaN           4   \n",
       "287634                0                       0         20160000.0           NaN           NaN           4   \n",
       "287635                0                       0         20160000.0           NaN           NaN           4   \n",
       "287636                0                       0         20160000.0           NaN           NaN           4   \n",
       "287637                0                       0         20160000.0           NaN           NaN           4   \n",
       "...                 ...                     ...                ...           ...           ...         ...   \n",
       "3845866               0                       0         30297442.0  2.139729e+07  2.139729e+07         8-K   \n",
       "3845867               0                       0         30297442.0  2.139729e+07  2.139729e+07         8-K   \n",
       "3845868               0                       0         30297442.0  2.139729e+07  2.139729e+07         8-K   \n",
       "3845869               0                       0         30297442.0  2.139729e+07  2.139729e+07         8-K   \n",
       "3845870               0                       0         30297442.0  2.139729e+07  2.139729e+07         8-K   \n",
       "\n",
       "        last_filing_date  is_filing_day  days_since_filing  \n",
       "287633        2022-08-15          False               49.0  \n",
       "287634        2022-08-15          False               50.0  \n",
       "287635        2022-08-15          False               51.0  \n",
       "287636        2022-08-15          False               52.0  \n",
       "287637        2022-08-15          False               53.0  \n",
       "...                  ...            ...                ...  \n",
       "3845866       2025-09-22          False                2.0  \n",
       "3845867       2025-09-22          False                3.0  \n",
       "3845868       2025-09-22          False                4.0  \n",
       "3845869       2025-09-22          False                7.0  \n",
       "3845870       2025-09-22          False                8.0  \n",
       "\n",
       "[98381 rows x 27 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_enriched.query(\"market_cap < 300000000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1611d3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- splits: skip-guard so we don't recompute on reruns ----\n",
    "SPLITS_REBUILD = False  # set True only if you need to rebuild/re-snap\n",
    "\n",
    "have_cols = {'split_ratio','is_split_day','is_reverse_split_day','split_cum_factor'}.issubset(analysis_enriched.columns)\n",
    "if have_cols and analysis_enriched['is_split_day'].notna().any() and not SPLITS_REBUILD:\n",
    "    print(\"[splits] already annotated; skipping (set SPLITS_REBUILD=True to rebuild).\")\n",
    "else:\n",
    "    # <-- keep your split-annotation block here -->\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e9ee8be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[splits] loaded from split_events.parquet -> 75614 rows, 2505 tickers\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>event_date</th>\n",
       "      <th>split_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAME</td>\n",
       "      <td>1985-11-04</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAME</td>\n",
       "      <td>1986-11-04</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAON</td>\n",
       "      <td>1993-09-16</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAON</td>\n",
       "      <td>1995-03-07</td>\n",
       "      <td>1.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAON</td>\n",
       "      <td>2001-10-01</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AAON</td>\n",
       "      <td>2002-06-05</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AAON</td>\n",
       "      <td>2007-08-22</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AAON</td>\n",
       "      <td>2011-06-14</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AAON</td>\n",
       "      <td>2013-07-03</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AAON</td>\n",
       "      <td>2014-07-17</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker event_date  split_ratio\n",
       "0   AAME 1985-11-04         2.00\n",
       "1   AAME 1986-11-04         1.25\n",
       "2   AAON 1993-09-16         0.25\n",
       "3   AAON 1995-03-07         1.10\n",
       "4   AAON 2001-10-01         1.50\n",
       "5   AAON 2002-06-05         1.50\n",
       "6   AAON 2007-08-22         1.50\n",
       "7   AAON 2011-06-14         1.50\n",
       "8   AAON 2013-07-03         1.50\n",
       "9   AAON 2014-07-17         1.50"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Split loader (cache-first, schema-tolerant) ===\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np\n",
    "import re\n",
    "\n",
    "OUT = Path(\"outputs\")\n",
    "P_SPLITS = OUT / \"split_events.parquet\"\n",
    "\n",
    "def _parse_ratio_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Accept numeric, or strings like '3:1', '1:10', returns shares-multiplier (3.0, 0.1, ...).\"\"\"\n",
    "    s = s.astype(str)\n",
    "    m = s.str.extract(r'^\\s*(\\d+(?:\\.\\d+)?)\\s*[:/]\\s*(\\d+(?:\\.\\d+)?)\\s*$')\n",
    "    ok = m.notna().all(axis=1)\n",
    "    out = pd.Series(np.nan, index=s.index, dtype='float64')\n",
    "    out.loc[ok] = m.loc[ok, 0].astype(float) / m.loc[ok, 1].astype(float)\n",
    "    # try plain numeric fallback where parse failed\n",
    "    out = out.where(ok, pd.to_numeric(s, errors='coerce'))\n",
    "    return out\n",
    "\n",
    "def _standardize_splits(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # --- choose/compute event_date ---\n",
    "    if 'event_date' in df.columns:\n",
    "        ev = df['event_date']\n",
    "    elif 'date' in df.columns:\n",
    "        ev = df['date']\n",
    "    else:\n",
    "        # last resort: common variants\n",
    "        for c in ['executionDate','effective_date','ex_date']:\n",
    "            if c in df.columns:\n",
    "                ev = df[c]\n",
    "                break\n",
    "        else:\n",
    "            raise AssertionError(\"No event_date/date column in split_events source.\")\n",
    "    ev = pd.to_datetime(ev, errors='coerce').dt.tz_localize(None).dt.normalize()\n",
    "\n",
    "    # --- compute split_ratio (shares multiplier; forward>1, reverse<1) ---\n",
    "    if 'split_ratio' in df.columns:\n",
    "        ratio = pd.to_numeric(df['split_ratio'], errors='coerce')\n",
    "    elif {'numerator','denominator'}.issubset(df.columns):\n",
    "        # e.g., 3-for-1 -> 3/1 = 3.0 ; 1-for-10 -> 1/10 = 0.1\n",
    "        ratio = pd.to_numeric(df['numerator'], errors='coerce') / pd.to_numeric(df['denominator'], errors='coerce')\n",
    "    elif 'ratio' in df.columns:\n",
    "        ratio = _parse_ratio_series(df['ratio'])\n",
    "    elif 'split' in df.columns:\n",
    "        ratio = _parse_ratio_series(df['split'])\n",
    "    else:\n",
    "        raise AssertionError(\"No split ratio columns found (expected split_ratio, ratio, numerator/denominator, or split).\")\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        'ticker': df.get('ticker'),\n",
    "        'event_date': ev,\n",
    "        'split_ratio': ratio\n",
    "    })\n",
    "    out['ticker'] = out['ticker'].astype(str).str.upper()\n",
    "    out = out.dropna(subset=['ticker','event_date','split_ratio'])\n",
    "    return (out\n",
    "            .sort_values(['ticker','event_date'])\n",
    "            .drop_duplicates(['ticker','event_date'], keep='last')\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "# --- load/create `splits` ---\n",
    "if 'splits' in globals() and isinstance(splits, pd.DataFrame) and not splits.empty:\n",
    "    splits = _standardize_splits(splits)\n",
    "    print(f\"[splits] using existing variable -> {len(splits)} rows, {splits['ticker'].nunique()} tickers\")\n",
    "elif P_SPLITS.exists():\n",
    "    raw = pd.read_parquet(P_SPLITS)\n",
    "    splits = _standardize_splits(raw)\n",
    "    print(f\"[splits] loaded from {P_SPLITS.name} -> {len(splits)} rows, {splits['ticker'].nunique()} tickers\")\n",
    "elif {'is_split_day','split_ratio'}.issubset(analysis_enriched.columns):\n",
    "    # derive from panel (only exact trading-day events)\n",
    "    tmp = (analysis_enriched.loc[analysis_enriched['is_split_day']]\n",
    "           .rename(columns={'date':'event_date'})[['ticker','event_date','split_ratio']])\n",
    "    splits = _standardize_splits(tmp)\n",
    "    print(f\"[splits] derived from analysis_enriched -> {len(splits)} rows, {splits['ticker'].nunique()} tickers\")\n",
    "else:\n",
    "    raise AssertionError(\"Run your split collection step (or provide outputs/split_events.parquet) before snapping.\")\n",
    "\n",
    "# quick peek\n",
    "display(splits.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a7743f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Split events: snap to nearest trading day (±3D) per-ticker & re-annotate (robust, vectorized) ===\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "# --- early skip for this cell ---\n",
    "SPLITS_REBUILD = globals().get(\"SPLITS_REBUILD\", False)\n",
    "\n",
    "already = (\n",
    "    {'split_ratio','is_split_day','is_reverse_split_day','split_cum_factor'}.issubset(analysis_enriched.columns)\n",
    "    and analysis_enriched['is_split_day'].notna().any()\n",
    ")\n",
    "\n",
    "if already and not SPLITS_REBUILD:\n",
    "    print(\"[splits] already annotated; skipping this cell (set SPLITS_REBUILD=True to rebuild).\")\n",
    "else:\n",
    "    assert 'analysis_enriched' in globals(), \"Run your finalize/backfill first.\"\n",
    "    assert 'splits' in globals() and not splits.empty, \"Run the split loader cell first.\"\n",
    "\n",
    "    # Normalize panel\n",
    "    ae = analysis_enriched.copy()\n",
    "    ae['ticker'] = ae['ticker'].astype(str).str.upper()\n",
    "    ae['date']   = pd.to_datetime(ae['date'], errors='coerce').dt.tz_localize(None).dt.normalize()\n",
    "    ae = ae.dropna(subset=['ticker','date']).sort_values(['ticker','date'])\n",
    "\n",
    "    # Trading calendar (unique trading dates per ticker)\n",
    "    cal = (ae[['ticker','date']]\n",
    "           .drop_duplicates()\n",
    "           .sort_values(['ticker','date']))\n",
    "\n",
    "    # Normalize split events\n",
    "    sp = (splits.assign(\n",
    "            ticker=lambda d: d['ticker'].astype(str).str.upper(),\n",
    "            event_date=lambda d: pd.to_datetime(d['event_date'], errors='coerce').dt.tz_localize(None).dt.normalize(),\n",
    "            split_ratio=lambda d: pd.to_numeric(d['split_ratio'], errors='coerce')\n",
    "         )\n",
    "         .dropna(subset=['ticker','event_date','split_ratio'])\n",
    "         .sort_values(['ticker','event_date'])\n",
    "         .copy())\n",
    "\n",
    "    tol = pd.Timedelta('3D')\n",
    "\n",
    "# ---------- SNAP USING SEARCHSORTED (no merge_asof) ----------\n",
    "snapped = []  # holds per-ticker frames with event_trading_date\n",
    "tol_ns = np.timedelta64(3, 'D')\n",
    "\n",
    "for tkr, sp_t in sp.groupby('ticker', sort=False):\n",
    "    # numpy datetime arrays (ns)\n",
    "    r = cal.loc[cal['ticker'].eq(tkr), 'date'].values.astype('datetime64[ns]')\n",
    "    if r.size == 0:\n",
    "        continue\n",
    "\n",
    "    ev = sp_t['event_date'].values.astype('datetime64[ns]')\n",
    "\n",
    "    # forward index (first trading date >= event_date)\n",
    "    idx_f = np.searchsorted(r, ev, side='left')\n",
    "    # backward index (last trading date <= event_date), clipped to [0, size-1]\n",
    "    idx_b = np.clip(idx_f - 1, 0, r.size - 1)\n",
    "\n",
    "    # candidate forward dates (safe construction: fill with NaT, then set where in-bounds)\n",
    "    d_f = np.full(ev.shape, np.datetime64('NaT', 'ns'), dtype='datetime64[ns]')\n",
    "    mask_f = idx_f < r.size\n",
    "    d_f[mask_f] = r[idx_f[mask_f]]\n",
    "\n",
    "    # candidate backward dates (always valid after clipping)\n",
    "    d_b = r[idx_b]\n",
    "\n",
    "    # distances (timedelta64[ns])\n",
    "    df = d_f - ev  # forward distance (>=0 if valid)\n",
    "    db = ev - d_b  # backward distance (>=0)\n",
    "\n",
    "    # within tolerance\n",
    "    ok_f = mask_f & (df >= np.timedelta64(0, 'ns')) & (df <= tol_ns)\n",
    "    ok_b = (db >= np.timedelta64(0, 'ns')) & (db <= tol_ns)\n",
    "\n",
    "    # choose closer; prefer forward if tie / both valid\n",
    "    choose_f = ok_f & (~ok_b | (df <= db))\n",
    "    chosen = np.where(choose_f, d_f, np.where(ok_b, d_b, np.datetime64('NaT', 'ns')))\n",
    "\n",
    "    out = sp_t.copy()\n",
    "    # chosen is numpy datetime64; convert to pandas (naive date)\n",
    "    out['event_trading_date'] = pd.to_datetime(chosen).tz_localize(None).normalize()\n",
    "    snapped.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "489bb409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis_enriched with split columns.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "OUT = Path(\"outputs\"); OUT.mkdir(exist_ok=True)\n",
    "analysis_enriched.to_parquet(OUT / \"analysis_enriched_with_splits.parquet\", index=False, compression=\"zstd\")\n",
    "analysis_enriched.to_csv(OUT / \"analysis_enriched_with_splits.csv\", index=False)\n",
    "# optional: mirror to canonical if you want downstream code to keep using the same filename\n",
    "(OUT / \"analysis_enriched.parquet\").write_bytes((OUT / \"analysis_enriched_with_splits.parquet\").read_bytes())\n",
    "(OUT / \"analysis_enriched.csv\").write_bytes((OUT / \"analysis_enriched_with_splits.csv\").read_bytes())\n",
    "print(\"Saved analysis_enriched with split columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a4024151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'split_ratio','is_split_day','is_reverse_split_day','split_cum_factor'}.issubset(analysis_enriched.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "998e44f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split cols present: True\n"
     ]
    }
   ],
   "source": [
    "# --- Ensure split annotations exist on `analysis_enriched` (idempotent) ---\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "NEEDED = {'split_ratio','is_split_day','is_reverse_split_day','split_cum_factor'}\n",
    "have_cols = NEEDED.issubset(analysis_enriched.columns)\n",
    "\n",
    "if not have_cols:\n",
    "    # 1) Load/normalize `splits` if missing (try a few on-disk candidates)\n",
    "    if 'splits' not in globals() or splits is None or getattr(splits, 'empty', True):\n",
    "        OUT = Path(\"outputs\")\n",
    "        cand = [\n",
    "            OUT / \"split_events.parquet\",\n",
    "            OUT / \"splits_events.parquet\",\n",
    "            OUT / \"shares_events.parquet\",   # sometimes contains split info\n",
    "        ]\n",
    "        splits = pd.DataFrame()\n",
    "        for p in cand:\n",
    "            if p.exists():\n",
    "                try:\n",
    "                    tmp = pd.read_parquet(p)\n",
    "                    # normalize to ['ticker','event_date','split_ratio']\n",
    "                    if {'ticker','event_date','split_ratio'} <= set(tmp.columns):\n",
    "                        splits = tmp[['ticker','event_date','split_ratio']].copy()\n",
    "                        break\n",
    "                    elif {'ticker','date','split_ratio'} <= set(tmp.columns):\n",
    "                        tmp = tmp.rename(columns={'date':'event_date'})\n",
    "                        splits = tmp[['ticker','event_date','split_ratio']].copy()\n",
    "                        break\n",
    "                    elif {'ticker','date','splitTo','splitFrom'} <= set(tmp.columns):\n",
    "                        tmp = tmp.rename(columns={'date':'event_date'})\n",
    "                        tmp['split_ratio'] = pd.to_numeric(tmp['splitTo'], errors='coerce') / pd.to_numeric(tmp['splitFrom'], errors='coerce')\n",
    "                        splits = tmp[['ticker','event_date','split_ratio']].copy()\n",
    "                        break\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    # If still nothing, add empty columns so downstream cells don’t crash\n",
    "    if splits is None or splits.empty:\n",
    "        print(\"[splits] No split events found on disk; adding empty columns.\")\n",
    "        analysis_enriched = analysis_enriched.copy()\n",
    "        if 'split_ratio' not in analysis_enriched.columns:\n",
    "            analysis_enriched['split_ratio'] = np.nan\n",
    "        if 'is_split_day' not in analysis_enriched.columns:\n",
    "            analysis_enriched['is_split_day'] = False\n",
    "        if 'is_reverse_split_day' not in analysis_enriched.columns:\n",
    "            analysis_enriched['is_reverse_split_day'] = False\n",
    "        if 'split_cum_factor' not in analysis_enriched.columns:\n",
    "            # if no splits, cum factor is 1.0 by default\n",
    "            analysis_enriched['split_cum_factor'] = 1.0\n",
    "    else:\n",
    "        # 2) Snap events to nearest trading day (±3D) and annotate panel\n",
    "        ae = analysis_enriched.copy()\n",
    "        ae['ticker'] = ae['ticker'].astype(str).str.upper()\n",
    "        ae['date']   = pd.to_datetime(ae['date'], errors='coerce').dt.tz_localize(None).dt.normalize()\n",
    "        ae = ae.dropna(subset=['ticker','date']).sort_values(['ticker','date'])\n",
    "\n",
    "        # trading calendar\n",
    "        cal = (ae[['ticker','date']].drop_duplicates().sort_values(['ticker','date']))\n",
    "\n",
    "        # normalize splits table\n",
    "        sp = (splits.assign(\n",
    "                ticker=lambda d: d['ticker'].astype(str).str.upper(),\n",
    "                event_date=lambda d: pd.to_datetime(d['event_date'], errors='coerce').dt.tz_localize(None).dt.normalize(),\n",
    "                split_ratio=lambda d: pd.to_numeric(d['split_ratio'], errors='coerce')\n",
    "             )\n",
    "             .dropna(subset=['ticker','event_date','split_ratio'])\n",
    "             .sort_values(['ticker','event_date'])\n",
    "             .copy())\n",
    "\n",
    "        tol = np.timedelta64(3, 'D')\n",
    "        snapped = []\n",
    "\n",
    "        for tkr, sp_t in sp.groupby('ticker', sort=False):\n",
    "            r = cal.loc[cal['ticker'].eq(tkr), 'date'].to_numpy('datetime64[ns]')\n",
    "            if r.size == 0:\n",
    "                continue\n",
    "            ev = sp_t['event_date'].to_numpy('datetime64[ns]')\n",
    "\n",
    "            idx_f = np.searchsorted(r, ev, side='left')\n",
    "            idx_b = np.clip(idx_f - 1, 0, r.size - 1)\n",
    "\n",
    "            mask_f = idx_f < r.size\n",
    "            # safe forward candidate (avoid out-of-bounds)\n",
    "            d_f = np.where(mask_f, r[np.where(mask_f, idx_f, 0)], np.datetime64('NaT'))\n",
    "            d_b = r[idx_b]\n",
    "\n",
    "            df = d_f - ev\n",
    "            db = ev - d_b\n",
    "\n",
    "            ok_f = mask_f & (df >= np.timedelta64(0,'ns')) & (df <= tol)\n",
    "            ok_b = (db >= np.timedelta64(0,'ns')) & (db <= tol)\n",
    "\n",
    "            choose_f = ok_f & (~ok_b | (df <= db))\n",
    "            chosen = np.where(choose_f, d_f, np.where(ok_b, d_b, np.datetime64('NaT')))\n",
    "\n",
    "            out = sp_t.copy()\n",
    "            out['event_trading_date'] = pd.to_datetime(chosen).normalize()\n",
    "            snapped.append(out)\n",
    "\n",
    "        if snapped:\n",
    "            sp2 = (pd.concat(snapped, ignore_index=True)\n",
    "                     .dropna(subset=['event_trading_date'])\n",
    "                     .loc[:, ['ticker','event_trading_date','split_ratio']]\n",
    "                     .drop_duplicates(['ticker','event_trading_date'], keep='last')\n",
    "                     .rename(columns={'event_trading_date':'date'}))\n",
    "        else:\n",
    "            sp2 = pd.DataFrame(columns=['ticker','date','split_ratio'])\n",
    "\n",
    "        # merge onto panel + flags + cum factor\n",
    "        ae = ae.drop(columns=['split_ratio','is_split_day','is_reverse_split_day','split_cum_factor'], errors='ignore')\n",
    "        merged = ae.merge(sp2, on=['ticker','date'], how='left')\n",
    "\n",
    "        merged['is_split_day'] = merged['split_ratio'].notna() & (merged['split_ratio'] != 1.0)\n",
    "        merged['is_reverse_split_day'] = merged['is_split_day'] & (merged['split_ratio'] < 1.0)\n",
    "\n",
    "        base = merged['split_ratio'].astype(float).fillna(1.0)\n",
    "        merged['split_cum_factor'] = base.groupby(merged['ticker']).cumprod()\n",
    "\n",
    "        analysis_enriched = merged\n",
    "\n",
    "# Quick confirmation for your next cell\n",
    "print(\"split cols present:\", NEEDED.issubset(analysis_enriched.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a651cd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split cols present: True\n",
      "is_split_day rows: 17695\n",
      "reverse_split rows: 17588\n",
      "tickers with ≥1 split day: 2040\n",
      "cumprod mismatches: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>split_ratio</th>\n",
       "      <th>is_reverse_split_day</th>\n",
       "      <th>split_cum_factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9231</th>\n",
       "      <td>AAON</td>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>False</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16757</th>\n",
       "      <td>ABAT</td>\n",
       "      <td>2023-09-11</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>True</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27813</th>\n",
       "      <td>ABTC</td>\n",
       "      <td>2022-11-08</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28127</th>\n",
       "      <td>ABTC</td>\n",
       "      <td>2024-02-09</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28518</th>\n",
       "      <td>ABTC</td>\n",
       "      <td>2025-09-03</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73818</th>\n",
       "      <td>ADTX</td>\n",
       "      <td>2023-08-18</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74100</th>\n",
       "      <td>ADTX</td>\n",
       "      <td>2024-10-02</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74212</th>\n",
       "      <td>ADTX</td>\n",
       "      <td>2025-03-17</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77721</th>\n",
       "      <td>ADVM</td>\n",
       "      <td>2024-03-21</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79120</th>\n",
       "      <td>ADXN</td>\n",
       "      <td>2023-10-23</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>True</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker       date  split_ratio  is_reverse_split_day  split_cum_factor\n",
       "9231    AAON 2023-08-17     1.500000                 False          1.500000\n",
       "16757   ABAT 2023-09-11     0.066667                  True          0.066667\n",
       "27813   ABTC 2022-11-08     0.050000                  True          0.050000\n",
       "28127   ABTC 2024-02-09     0.050000                  True          0.002500\n",
       "28518   ABTC 2025-09-03     0.200000                  True          0.000500\n",
       "...      ...        ...          ...                   ...               ...\n",
       "73818   ADTX 2023-08-18     0.025000                  True          0.025000\n",
       "74100   ADTX 2024-10-02     0.025000                  True          0.000625\n",
       "74212   ADTX 2025-03-17     0.004000                  True          0.000003\n",
       "77721   ADVM 2024-03-21     0.100000                  True          0.100000\n",
       "79120   ADXN 2023-10-23     0.050000                  True          0.050000\n",
       "\n",
       "[20 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1) Do we have the columns?\n",
    "cols_ok = {'split_ratio','is_split_day','is_reverse_split_day','split_cum_factor'} <= set(analysis_enriched.columns)\n",
    "print(\"split cols present:\", cols_ok)\n",
    "\n",
    "# 2) Count split & reverse-split event rows (panel days, not unique events)\n",
    "print(\"is_split_day rows:\", int((analysis_enriched['is_split_day'] == True).sum()))\n",
    "print(\"reverse_split rows:\", int((analysis_enriched['is_reverse_split_day'] == True).sum()))\n",
    "print(\"tickers with ≥1 split day:\", analysis_enriched.loc[analysis_enriched['is_split_day'], 'ticker'].nunique())\n",
    "\n",
    "# 3) Verify split_cum_factor is the groupwise cumprod of split_ratio (NaNs→1.0)\n",
    "_base = analysis_enriched['split_ratio'].astype(float).fillna(1.0)\n",
    "check = _base.groupby(analysis_enriched['ticker']).cumprod()\n",
    "mismatch = (~(analysis_enriched['split_cum_factor'].astype(float).round(12)\n",
    "              .eq(check.round(12)))).sum()\n",
    "print(\"cumprod mismatches:\", int(mismatch))\n",
    "\n",
    "# 4) Peek a few split days\n",
    "display(analysis_enriched.loc[analysis_enriched['is_split_day'],\n",
    "                              ['ticker','date','split_ratio','is_reverse_split_day','split_cum_factor']]\n",
    "        .sort_values(['ticker','date']).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a4238be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [date, split_ratio, is_split_day]\n",
      "Index: []\n",
      "median |logret| on split-days: 0.006226211191315398\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Always bind to the master panel that has the split columns\n",
    "panel = analysis_enriched.copy()\n",
    "\n",
    "# Sanity: make sure the columns exist\n",
    "need = {'split_ratio', 'is_split_day', 'date', 'ticker'}\n",
    "missing = need - set(panel.columns)\n",
    "assert not missing, f\"Missing columns: {missing}. Re-run the split-annotation cell.\"\n",
    "\n",
    "# Normalize date just in case\n",
    "panel['date'] = pd.to_datetime(panel['date'], errors='coerce').dt.tz_localize(None).dt.normalize()\n",
    "\n",
    "# 1) Known forward split: TSLA 3-for-1 on 2022-08-25 -> expect split_ratio ~ 3.0\n",
    "print(\n",
    "    panel.query(\"ticker == 'TSLA' and date >= '2022-08-20' and date <= '2022-08-30'\")\n",
    "         [['date','split_ratio','is_split_day']]\n",
    "         .sort_values('date')\n",
    ")\n",
    "\n",
    "# 2) Event-day return sanity (adj prices are already split-adjusted → small |logret|)\n",
    "evt = panel.loc[panel['is_split_day'] == True, ['ticker','date','logret']]\n",
    "print(\"median |logret| on split-days:\", float(evt['logret'].abs().median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "76e6f3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median |logret| by type:\n",
      "is_reverse_split_day\n",
      "False    0.011819\n",
      "True     0.006179\n",
      "Name: abs_lr, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "panel = analysis_enriched.copy()\n",
    "panel['date'] = pd.to_datetime(panel['date']).dt.tz_localize(None).dt.normalize()\n",
    "\n",
    "# Split-day returns split by direction\n",
    "evt = panel.loc[panel['is_split_day'] == True,\n",
    "                ['ticker','date','logret','is_reverse_split_day','split_ratio']].copy()\n",
    "evt['abs_lr'] = evt['logret'].abs()\n",
    "print(\"median |logret| by type:\")\n",
    "print(evt.groupby('is_reverse_split_day')['abs_lr'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "26da9bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward splits — median rel. error: 0.48480164863472436\n",
      "Reverse splits — median rel. error: 0.9370940162858776\n",
      "Forward splits — share within 10%: 0.22429906542056074\n",
      "Reverse splits — share within 10%: 0.00011371389583807141\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ae = analysis_enriched.copy()\n",
    "ae['date'] = pd.to_datetime(ae['date']).dt.tz_localize(None).dt.normalize()\n",
    "\n",
    "# Get previous day's close per ticker\n",
    "ae = ae.sort_values(['ticker','date'])\n",
    "ae['prev_close'] = ae.groupby('ticker')['close'].shift(1)\n",
    "\n",
    "spl = ae.loc[ae['is_split_day']].copy()\n",
    "\n",
    "# For forward splits (ratio > 1): prev_close / close ≈ split_ratio\n",
    "fwd = spl.loc[~spl['is_reverse_split_day'] & spl['split_ratio'].notna()].copy()\n",
    "fwd['est_ratio'] = fwd['prev_close'] / fwd['close']\n",
    "fwd['rel_err']   = (fwd['est_ratio'] - fwd['split_ratio']).abs() / fwd['split_ratio']\n",
    "\n",
    "# For reverse splits (ratio < 1): close / prev_close ≈ 1/ratio\n",
    "rev = spl.loc[spl['is_reverse_split_day'] & spl['split_ratio'].notna()].copy()\n",
    "rev['est_ratio_inv'] = rev['close'] / rev['prev_close']\n",
    "rev['rel_err']       = (rev['est_ratio_inv'] - (1.0 / rev['split_ratio'])).abs() / (1.0 / rev['split_ratio'])\n",
    "\n",
    "print(\"Forward splits — median rel. error:\", float(fwd['rel_err'].median()))\n",
    "print(\"Reverse splits — median rel. error:\", float(rev['rel_err'].median()))\n",
    "print(\"Forward splits — share within 10%:\", float((fwd['rel_err'] <= 0.10).mean()))\n",
    "print(\"Reverse splits — share within 10%:\", float((rev['rel_err'] <= 0.10).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c142fa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "factor step matches split_ratio (excluding first splits per ticker): 0.1296485260770975\n",
      "first-split rows (no prev_cum, expected NaN): 55\n",
      "mismatch examples among valid rows (if any):\n",
      "       ticker       date  split_ratio  factor_step\n",
      "593643     CA 2024-02-29          0.0          NaN\n",
      "593664     CA 2024-04-01          0.0          NaN\n",
      "593685     CA 2024-04-30          0.0          NaN\n",
      "593708     CA 2024-06-03          0.0          NaN\n",
      "593727     CA 2024-07-01          0.0          NaN\n",
      "593748     CA 2024-07-31          0.0          NaN\n",
      "593771     CA 2024-09-03          0.0          NaN\n",
      "593790     CA 2024-09-30          0.0          NaN\n",
      "593813     CA 2024-10-31          0.0          NaN\n",
      "593834     CA 2024-12-02          0.0          NaN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "panel = analysis_enriched.copy()\n",
    "panel['date'] = pd.to_datetime(panel['date']).dt.tz_localize(None).dt.normalize()\n",
    "panel = panel.sort_values(['ticker','date'])\n",
    "\n",
    "# previous day's cumulative factor per ticker (from the full panel)\n",
    "panel['prev_cum'] = panel.groupby('ticker')['split_cum_factor'].shift(1)\n",
    "\n",
    "# restrict to actual split days\n",
    "evt = panel.loc[panel['is_split_day']].copy()\n",
    "\n",
    "# factor step should equal the announced split_ratio on the event trading day\n",
    "evt['factor_step'] = evt['split_cum_factor'] / evt['prev_cum']\n",
    "\n",
    "# evaluate only where we have a previous trading day cum factor\n",
    "valid = evt['prev_cum'].notna() & evt['split_ratio'].notna()\n",
    "ok = np.isclose(evt.loc[valid, 'factor_step'],\n",
    "                evt.loc[valid, 'split_ratio'],\n",
    "                rtol=1e-6)\n",
    "\n",
    "print(\"factor step matches split_ratio (excluding first splits per ticker):\",\n",
    "      float(ok.mean()))\n",
    "\n",
    "print(\"first-split rows (no prev_cum, expected NaN):\",\n",
    "      int((evt['prev_cum'].isna()).sum()))\n",
    "\n",
    "print(\"mismatch examples among valid rows (if any):\")\n",
    "bad = evt.loc[valid].loc[~ok, ['ticker','date','split_ratio','factor_step']].head(10)\n",
    "print(bad if not bad.empty else \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c2d080e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ret</th>\n",
       "      <th>logret</th>\n",
       "      <th>...</th>\n",
       "      <th>float_shares</th>\n",
       "      <th>free_float</th>\n",
       "      <th>filing_form</th>\n",
       "      <th>last_filing_date</th>\n",
       "      <th>is_filing_day</th>\n",
       "      <th>days_since_filing</th>\n",
       "      <th>split_ratio</th>\n",
       "      <th>is_split_day</th>\n",
       "      <th>is_reverse_split_day</th>\n",
       "      <th>split_cum_factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>AACB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.214790e+07</td>\n",
       "      <td>2.214790e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>AACB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.214790e+07</td>\n",
       "      <td>2.214790e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>AACB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.214790e+07</td>\n",
       "      <td>2.214790e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-06</td>\n",
       "      <td>AACB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.214790e+07</td>\n",
       "      <td>2.214790e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-07</td>\n",
       "      <td>AACB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.214790e+07</td>\n",
       "      <td>2.214790e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845866</th>\n",
       "      <td>2025-09-24</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.50</td>\n",
       "      <td>80600</td>\n",
       "      <td>-0.019608</td>\n",
       "      <td>-0.019803</td>\n",
       "      <td>...</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845867</th>\n",
       "      <td>2025-09-25</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.47</td>\n",
       "      <td>109300</td>\n",
       "      <td>-0.020000</td>\n",
       "      <td>-0.020203</td>\n",
       "      <td>...</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845868</th>\n",
       "      <td>2025-09-26</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.45</td>\n",
       "      <td>54700</td>\n",
       "      <td>-0.013605</td>\n",
       "      <td>-0.013699</td>\n",
       "      <td>...</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>False</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845869</th>\n",
       "      <td>2025-09-29</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.42</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.47</td>\n",
       "      <td>115800</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>...</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>False</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845870</th>\n",
       "      <td>2025-09-30</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>1.46</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.45</td>\n",
       "      <td>48200</td>\n",
       "      <td>-0.013605</td>\n",
       "      <td>-0.013699</td>\n",
       "      <td>...</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>False</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3845871 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date ticker  open  high   low  close  adj_close  volume       ret    logret  ...  float_shares  \\\n",
       "0       2022-10-03   AACB   NaN   NaN   NaN    NaN        NaN    <NA>       NaN       NaN  ...  2.214790e+07   \n",
       "1       2022-10-04   AACB   NaN   NaN   NaN    NaN        NaN    <NA>       NaN       NaN  ...  2.214790e+07   \n",
       "2       2022-10-05   AACB   NaN   NaN   NaN    NaN        NaN    <NA>       NaN       NaN  ...  2.214790e+07   \n",
       "3       2022-10-06   AACB   NaN   NaN   NaN    NaN        NaN    <NA>       NaN       NaN  ...  2.214790e+07   \n",
       "4       2022-10-07   AACB   NaN   NaN   NaN    NaN        NaN    <NA>       NaN       NaN  ...  2.214790e+07   \n",
       "...            ...    ...   ...   ...   ...    ...        ...     ...       ...       ...  ...           ...   \n",
       "3845866 2025-09-24   ZYXI  1.55  1.59  1.50   1.50       1.50   80600 -0.019608 -0.019803  ...  2.139729e+07   \n",
       "3845867 2025-09-25   ZYXI  1.51  1.54  1.45   1.47       1.47  109300 -0.020000 -0.020203  ...  2.139729e+07   \n",
       "3845868 2025-09-26   ZYXI  1.47  1.50  1.44   1.45       1.45   54700 -0.013605 -0.013699  ...  2.139729e+07   \n",
       "3845869 2025-09-29   ZYXI  1.45  1.49  1.42   1.47       1.47  115800  0.013793  0.013699  ...  2.139729e+07   \n",
       "3845870 2025-09-30   ZYXI  1.46  1.48  1.45   1.45       1.45   48200 -0.013605 -0.013699  ...  2.139729e+07   \n",
       "\n",
       "           free_float filing_form last_filing_date is_filing_day days_since_filing split_ratio is_split_day  \\\n",
       "0        2.214790e+07         NaN              NaT         False               NaN         NaN        False   \n",
       "1        2.214790e+07         NaN              NaT         False               NaN         NaN        False   \n",
       "2        2.214790e+07         NaN              NaT         False               NaN         NaN        False   \n",
       "3        2.214790e+07         NaN              NaT         False               NaN         NaN        False   \n",
       "4        2.214790e+07         NaN              NaT         False               NaN         NaN        False   \n",
       "...               ...         ...              ...           ...               ...         ...          ...   \n",
       "3845866  2.139729e+07         8-K       2025-09-22         False               2.0         NaN        False   \n",
       "3845867  2.139729e+07         8-K       2025-09-22         False               3.0         NaN        False   \n",
       "3845868  2.139729e+07         8-K       2025-09-22         False               4.0         NaN        False   \n",
       "3845869  2.139729e+07         8-K       2025-09-22         False               7.0         NaN        False   \n",
       "3845870  2.139729e+07         8-K       2025-09-22         False               8.0         NaN        False   \n",
       "\n",
       "         is_reverse_split_day  split_cum_factor  \n",
       "0                       False               1.0  \n",
       "1                       False               1.0  \n",
       "2                       False               1.0  \n",
       "3                       False               1.0  \n",
       "4                       False               1.0  \n",
       "...                       ...               ...  \n",
       "3845866                 False               1.0  \n",
       "3845867                 False               1.0  \n",
       "3845868                 False               1.0  \n",
       "3845869                 False               1.0  \n",
       "3845870                 False               1.0  \n",
       "\n",
       "[3845871 rows x 31 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5cd5f7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols present: True\n",
      "shape: (3845871, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>split_ratio</th>\n",
       "      <th>is_reverse_split_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9231</th>\n",
       "      <td>AAON</td>\n",
       "      <td>2023-08-17</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16757</th>\n",
       "      <td>ABAT</td>\n",
       "      <td>2023-09-11</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27813</th>\n",
       "      <td>ABTC</td>\n",
       "      <td>2022-11-08</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28127</th>\n",
       "      <td>ABTC</td>\n",
       "      <td>2024-02-09</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28518</th>\n",
       "      <td>ABTC</td>\n",
       "      <td>2025-09-03</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker       date  split_ratio  is_reverse_split_day\n",
       "9231    AAON 2023-08-17     1.500000                 False\n",
       "16757   ABAT 2023-09-11     0.066667                  True\n",
       "27813   ABTC 2022-11-08     0.050000                  True\n",
       "28127   ABTC 2024-02-09     0.050000                  True\n",
       "28518   ABTC 2025-09-03     0.200000                  True"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEEDED = {'split_ratio','is_split_day','is_reverse_split_day','split_cum_factor'}\n",
    "print(\"cols present:\", NEEDED.issubset(analysis_enriched.columns))\n",
    "print(\"shape:\", analysis_enriched.shape)\n",
    "analysis_enriched.loc[analysis_enriched['is_split_day'], \n",
    "                      ['ticker','date','split_ratio','is_reverse_split_day']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a56cbe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "OUT = Path(\"outputs\"); OUT.mkdir(exist_ok=True)\n",
    "analysis_enriched.to_parquet(OUT / \"analysis_enriched.parquet\", index=False, compression=\"zstd\")\n",
    "analysis_enriched.to_csv(OUT / \"analysis_enriched.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b67bb8",
   "metadata": {},
   "source": [
    "Below two files are now your latest, “master” snapshots:\n",
    "* outputs/analysis_enriched.parquet (preferred for speed & types)\n",
    "* outputs/analysis_enriched.csv (useful for Excel/sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "218c1ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yx/5t5dfjxj4pv8tjn2zvbfc2540000gn/T/ipykernel_4700/2180517585.py:6: DtypeWarning: Columns (11,12,15,16,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  csv = pd.read_csv(P / \"analysis_enriched.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same columns (parquet)? True\n",
      "same row count (parquet)? True\n",
      "same columns (csv)? True\n",
      "same row count (csv)? True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, pathlib as pl\n",
    "\n",
    "P = pl.Path(\"outputs\")\n",
    "mem = analysis_enriched\n",
    "pq  = pd.read_parquet(P / \"analysis_enriched.parquet\")\n",
    "csv = pd.read_csv(P / \"analysis_enriched.csv\")\n",
    "\n",
    "print(\"same columns (parquet)?\", set(mem.columns) == set(pq.columns))\n",
    "print(\"same row count (parquet)?\", len(mem) == len(pq))\n",
    "print(\"same columns (csv)?\", set(mem.columns) == set(csv.columns))\n",
    "print(\"same row count (csv)?\", len(mem) == len(csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "309ada85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "mem.to_parquet(P / f\"analysis_enriched_{ts}.parquet\", index=False, compression=\"zstd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3fcb5f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ret</th>\n",
       "      <th>logret</th>\n",
       "      <th>...</th>\n",
       "      <th>float_shares</th>\n",
       "      <th>free_float</th>\n",
       "      <th>filing_form</th>\n",
       "      <th>last_filing_date</th>\n",
       "      <th>is_filing_day</th>\n",
       "      <th>days_since_filing</th>\n",
       "      <th>split_ratio</th>\n",
       "      <th>is_split_day</th>\n",
       "      <th>is_reverse_split_day</th>\n",
       "      <th>split_cum_factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-03</td>\n",
       "      <td>AACB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.214790e+07</td>\n",
       "      <td>2.214790e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>AACB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.214790e+07</td>\n",
       "      <td>2.214790e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>AACB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.214790e+07</td>\n",
       "      <td>2.214790e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-06</td>\n",
       "      <td>AACB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.214790e+07</td>\n",
       "      <td>2.214790e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-07</td>\n",
       "      <td>AACB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.214790e+07</td>\n",
       "      <td>2.214790e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845866</th>\n",
       "      <td>2025-09-24</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.59</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.50</td>\n",
       "      <td>80600</td>\n",
       "      <td>-0.019608</td>\n",
       "      <td>-0.019803</td>\n",
       "      <td>...</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845867</th>\n",
       "      <td>2025-09-25</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.47</td>\n",
       "      <td>109300</td>\n",
       "      <td>-0.020000</td>\n",
       "      <td>-0.020203</td>\n",
       "      <td>...</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845868</th>\n",
       "      <td>2025-09-26</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.45</td>\n",
       "      <td>54700</td>\n",
       "      <td>-0.013605</td>\n",
       "      <td>-0.013699</td>\n",
       "      <td>...</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>False</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845869</th>\n",
       "      <td>2025-09-29</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.42</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.47</td>\n",
       "      <td>115800</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>...</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>False</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845870</th>\n",
       "      <td>2025-09-30</td>\n",
       "      <td>ZYXI</td>\n",
       "      <td>1.46</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.45</td>\n",
       "      <td>48200</td>\n",
       "      <td>-0.013605</td>\n",
       "      <td>-0.013699</td>\n",
       "      <td>...</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>2.139729e+07</td>\n",
       "      <td>8-K</td>\n",
       "      <td>2025-09-22</td>\n",
       "      <td>False</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3845871 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date ticker  open  high   low  close  adj_close  volume       ret    logret  ...  float_shares  \\\n",
       "0       2022-10-03   AACB   NaN   NaN   NaN    NaN        NaN    <NA>       NaN       NaN  ...  2.214790e+07   \n",
       "1       2022-10-04   AACB   NaN   NaN   NaN    NaN        NaN    <NA>       NaN       NaN  ...  2.214790e+07   \n",
       "2       2022-10-05   AACB   NaN   NaN   NaN    NaN        NaN    <NA>       NaN       NaN  ...  2.214790e+07   \n",
       "3       2022-10-06   AACB   NaN   NaN   NaN    NaN        NaN    <NA>       NaN       NaN  ...  2.214790e+07   \n",
       "4       2022-10-07   AACB   NaN   NaN   NaN    NaN        NaN    <NA>       NaN       NaN  ...  2.214790e+07   \n",
       "...            ...    ...   ...   ...   ...    ...        ...     ...       ...       ...  ...           ...   \n",
       "3845866 2025-09-24   ZYXI  1.55  1.59  1.50   1.50       1.50   80600 -0.019608 -0.019803  ...  2.139729e+07   \n",
       "3845867 2025-09-25   ZYXI  1.51  1.54  1.45   1.47       1.47  109300 -0.020000 -0.020203  ...  2.139729e+07   \n",
       "3845868 2025-09-26   ZYXI  1.47  1.50  1.44   1.45       1.45   54700 -0.013605 -0.013699  ...  2.139729e+07   \n",
       "3845869 2025-09-29   ZYXI  1.45  1.49  1.42   1.47       1.47  115800  0.013793  0.013699  ...  2.139729e+07   \n",
       "3845870 2025-09-30   ZYXI  1.46  1.48  1.45   1.45       1.45   48200 -0.013605 -0.013699  ...  2.139729e+07   \n",
       "\n",
       "           free_float filing_form last_filing_date is_filing_day days_since_filing split_ratio is_split_day  \\\n",
       "0        2.214790e+07         NaN              NaT         False               NaN         NaN        False   \n",
       "1        2.214790e+07         NaN              NaT         False               NaN         NaN        False   \n",
       "2        2.214790e+07         NaN              NaT         False               NaN         NaN        False   \n",
       "3        2.214790e+07         NaN              NaT         False               NaN         NaN        False   \n",
       "4        2.214790e+07         NaN              NaT         False               NaN         NaN        False   \n",
       "...               ...         ...              ...           ...               ...         ...          ...   \n",
       "3845866  2.139729e+07         8-K       2025-09-22         False               2.0         NaN        False   \n",
       "3845867  2.139729e+07         8-K       2025-09-22         False               3.0         NaN        False   \n",
       "3845868  2.139729e+07         8-K       2025-09-22         False               4.0         NaN        False   \n",
       "3845869  2.139729e+07         8-K       2025-09-22         False               7.0         NaN        False   \n",
       "3845870  2.139729e+07         8-K       2025-09-22         False               8.0         NaN        False   \n",
       "\n",
       "         is_reverse_split_day  split_cum_factor  \n",
       "0                       False               1.0  \n",
       "1                       False               1.0  \n",
       "2                       False               1.0  \n",
       "3                       False               1.0  \n",
       "4                       False               1.0  \n",
       "...                       ...               ...  \n",
       "3845866                 False               1.0  \n",
       "3845867                 False               1.0  \n",
       "3845868                 False               1.0  \n",
       "3845869                 False               1.0  \n",
       "3845870                 False               1.0  \n",
       "\n",
       "[3845871 rows x 31 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_enriched"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
